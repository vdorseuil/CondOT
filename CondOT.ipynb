{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icnnet import ICNNet\n",
    "from mydataset import MyDataset, get_gaussian_dataset, get_gaussian_transport_dataset\n",
    "from toy_data_dataloader_gaussian import generate_gaussian_dataset, get_dataset, generate_dataset\n",
    "from train_picnn import PICNNtrain\n",
    "from train_wasserstein import train_wasserstein\n",
    "from train_makkuva import train_makkuva, train_makkuva_epoch\n",
    "from visualization import plot_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Generate dataset__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = get_dataset(d=2, r=100, N=500) #valou\n",
    "#dataset = generate_gaussian_dataset(d=2, r=400, N=10000) #thomas\n",
    "dataset = generate_dataset(d=2, r=200, N=200)\n",
    "gaussian_dataset = get_gaussian_dataset(dataset)\n",
    "gaussian_transport_dataset = get_gaussian_transport_dataset(gaussian_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Initialization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __PICNN training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "layer_sizes = [input_size,64,64,64, 1]\n",
    "n_layers = len(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_embedding(C, c):\n",
    "    scalar_product = torch.matmul(c.float(), C.t().float())\n",
    "    embedding = F.softmax(scalar_product, dim=1)\n",
    "    return(embedding)\n",
    "\n",
    "context_layer_sizes = [2] * n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init_f = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "model_init_g = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training f\n",
      "Epoch 1/150 Loss: 14.007146835327148\n",
      "Epoch 2/150 Loss: 10.978131294250488\n",
      "Epoch 3/150 Loss: 11.847502708435059\n",
      "Epoch 4/150 Loss: 10.56534194946289\n",
      "Epoch 5/150 Loss: 9.083248138427734\n",
      "Epoch 6/150 Loss: 8.584258079528809\n",
      "Epoch 7/150 Loss: 8.428940773010254\n",
      "Epoch 8/150 Loss: 7.916776657104492\n",
      "Epoch 9/150 Loss: 7.059391975402832\n",
      "Epoch 10/150 Loss: 6.232521057128906\n",
      "Epoch 11/150 Loss: 5.73097562789917\n",
      "Epoch 12/150 Loss: 5.496456623077393\n",
      "Epoch 13/150 Loss: 5.206870079040527\n",
      "Epoch 14/150 Loss: 4.701727867126465\n",
      "Epoch 15/150 Loss: 4.13549280166626\n",
      "Epoch 16/150 Loss: 3.7138140201568604\n",
      "Epoch 17/150 Loss: 3.4637210369110107\n",
      "Epoch 18/150 Loss: 3.2625577449798584\n",
      "Epoch 19/150 Loss: 2.9978554248809814\n",
      "Epoch 20/150 Loss: 2.670083522796631\n",
      "Epoch 21/150 Loss: 2.364135980606079\n",
      "Epoch 22/150 Loss: 2.154249906539917\n",
      "Epoch 23/150 Loss: 2.030212879180908\n",
      "Epoch 24/150 Loss: 1.9112662076950073\n",
      "Epoch 25/150 Loss: 1.7414461374282837\n",
      "Epoch 26/150 Loss: 1.5500085353851318\n",
      "Epoch 27/150 Loss: 1.3984699249267578\n",
      "Epoch 28/150 Loss: 1.3077309131622314\n",
      "Epoch 29/150 Loss: 1.2446422576904297\n",
      "Epoch 30/150 Loss: 1.166838526725769\n",
      "Epoch 31/150 Loss: 1.068126916885376\n",
      "Epoch 32/150 Loss: 0.9775297045707703\n",
      "Epoch 33/150 Loss: 0.9198925495147705\n",
      "Epoch 34/150 Loss: 0.8895227313041687\n",
      "Epoch 35/150 Loss: 0.8595501780509949\n",
      "Epoch 36/150 Loss: 0.8106222748756409\n",
      "Epoch 37/150 Loss: 0.7546217441558838\n",
      "Epoch 38/150 Loss: 0.7136722803115845\n",
      "Epoch 39/150 Loss: 0.6929722428321838\n",
      "Epoch 40/150 Loss: 0.6784881949424744\n",
      "Epoch 41/150 Loss: 0.6572192907333374\n",
      "Epoch 42/150 Loss: 0.629929780960083\n",
      "Epoch 43/150 Loss: 0.6077726483345032\n",
      "Epoch 44/150 Loss: 0.5965011715888977\n",
      "Epoch 45/150 Loss: 0.5900662541389465\n",
      "Epoch 46/150 Loss: 0.5790255069732666\n",
      "Epoch 47/150 Loss: 0.5623868703842163\n",
      "Epoch 48/150 Loss: 0.547377347946167\n",
      "Epoch 49/150 Loss: 0.5391798615455627\n",
      "Epoch 50/150 Loss: 0.5348557233810425\n",
      "Epoch 51/150 Loss: 0.5286681056022644\n",
      "Epoch 52/150 Loss: 0.5193230509757996\n",
      "Epoch 53/150 Loss: 0.5105905532836914\n",
      "Epoch 54/150 Loss: 0.5057411193847656\n",
      "Epoch 55/150 Loss: 0.5033532977104187\n",
      "Epoch 56/150 Loss: 0.4997347295284271\n",
      "Epoch 57/150 Loss: 0.49392417073249817\n",
      "Epoch 58/150 Loss: 0.48841801285743713\n",
      "Epoch 59/150 Loss: 0.48533114790916443\n",
      "Epoch 60/150 Loss: 0.4837827682495117\n",
      "Epoch 61/150 Loss: 0.48150789737701416\n",
      "Epoch 62/150 Loss: 0.47798046469688416\n",
      "Epoch 63/150 Loss: 0.4747491776943207\n",
      "Epoch 64/150 Loss: 0.47304561734199524\n",
      "Epoch 65/150 Loss: 0.472170889377594\n",
      "Epoch 66/150 Loss: 0.4706651270389557\n",
      "Epoch 67/150 Loss: 0.46841195225715637\n",
      "Epoch 68/150 Loss: 0.4665083885192871\n",
      "Epoch 69/150 Loss: 0.46558326482772827\n",
      "Epoch 70/150 Loss: 0.4650055766105652\n",
      "Epoch 71/150 Loss: 0.4639451205730438\n",
      "Epoch 72/150 Loss: 0.46252626180648804\n",
      "Epoch 73/150 Loss: 0.4614790081977844\n",
      "Epoch 74/150 Loss: 0.4610126316547394\n",
      "Epoch 75/150 Loss: 0.4605930745601654\n",
      "Epoch 76/150 Loss: 0.459805965423584\n",
      "Epoch 77/150 Loss: 0.4589180648326874\n",
      "Epoch 78/150 Loss: 0.4583703577518463\n",
      "Epoch 79/150 Loss: 0.4581092298030853\n",
      "Epoch 80/150 Loss: 0.45774081349372864\n",
      "Epoch 81/150 Loss: 0.4571540653705597\n",
      "Epoch 82/150 Loss: 0.45662710070610046\n",
      "Epoch 83/150 Loss: 0.4563474655151367\n",
      "Epoch 84/150 Loss: 0.4561454951763153\n",
      "Epoch 85/150 Loss: 0.45579907298088074\n",
      "Epoch 86/150 Loss: 0.45536890625953674\n",
      "Epoch 87/150 Loss: 0.45505470037460327\n",
      "Epoch 88/150 Loss: 0.4548698365688324\n",
      "Epoch 89/150 Loss: 0.4546491801738739\n",
      "Epoch 90/150 Loss: 0.45432841777801514\n",
      "Epoch 91/150 Loss: 0.4540210962295532\n",
      "Epoch 92/150 Loss: 0.4538126587867737\n",
      "Epoch 93/150 Loss: 0.45363369584083557\n",
      "Epoch 94/150 Loss: 0.4533877968788147\n",
      "Epoch 95/150 Loss: 0.45310717821121216\n",
      "Epoch 96/150 Loss: 0.4528772830963135\n",
      "Epoch 97/150 Loss: 0.45269301533699036\n",
      "Epoch 98/150 Loss: 0.45248085260391235\n",
      "Epoch 99/150 Loss: 0.4522259831428528\n",
      "Epoch 100/150 Loss: 0.4519861340522766\n",
      "Epoch 101/150 Loss: 0.451785147190094\n",
      "Epoch 102/150 Loss: 0.4515796899795532\n",
      "Epoch 103/150 Loss: 0.4513408839702606\n",
      "Epoch 104/150 Loss: 0.4510987401008606\n",
      "Epoch 105/150 Loss: 0.45088285207748413\n",
      "Epoch 106/150 Loss: 0.45067286491394043\n",
      "Epoch 107/150 Loss: 0.45044130086898804\n",
      "Epoch 108/150 Loss: 0.4502001404762268\n",
      "Epoch 109/150 Loss: 0.4499739706516266\n",
      "Epoch 110/150 Loss: 0.4497565031051636\n",
      "Epoch 111/150 Loss: 0.4495265483856201\n",
      "Epoch 112/150 Loss: 0.44928666949272156\n",
      "Epoch 113/150 Loss: 0.4490545988082886\n",
      "Epoch 114/150 Loss: 0.44883042573928833\n",
      "Epoch 115/150 Loss: 0.44859927892684937\n",
      "Epoch 116/150 Loss: 0.44836005568504333\n",
      "Epoch 117/150 Loss: 0.4481249153614044\n",
      "Epoch 118/150 Loss: 0.44789570569992065\n",
      "Epoch 119/150 Loss: 0.44766274094581604\n",
      "Epoch 120/150 Loss: 0.44742369651794434\n",
      "Epoch 121/150 Loss: 0.44718652963638306\n",
      "Epoch 122/150 Loss: 0.4469549357891083\n",
      "Epoch 123/150 Loss: 0.44672128558158875\n",
      "Epoch 124/150 Loss: 0.4464837312698364\n",
      "Epoch 125/150 Loss: 0.4462471604347229\n",
      "Epoch 126/150 Loss: 0.44601327180862427\n",
      "Epoch 127/150 Loss: 0.4457778334617615\n",
      "Epoch 128/150 Loss: 0.445539653301239\n",
      "Epoch 129/150 Loss: 0.44530200958251953\n",
      "Epoch 130/150 Loss: 0.4450662136077881\n",
      "Epoch 131/150 Loss: 0.44482943415641785\n",
      "Epoch 132/150 Loss: 0.44459056854248047\n",
      "Epoch 133/150 Loss: 0.44435223937034607\n",
      "Epoch 134/150 Loss: 0.4441148340702057\n",
      "Epoch 135/150 Loss: 0.4438764154911041\n",
      "Epoch 136/150 Loss: 0.44363653659820557\n",
      "Epoch 137/150 Loss: 0.4433964788913727\n",
      "Epoch 138/150 Loss: 0.4431568384170532\n",
      "Epoch 139/150 Loss: 0.44291621446609497\n",
      "Epoch 140/150 Loss: 0.44267451763153076\n",
      "Epoch 141/150 Loss: 0.44243258237838745\n",
      "Epoch 142/150 Loss: 0.44219058752059937\n",
      "Epoch 143/150 Loss: 0.44194766879081726\n",
      "Epoch 144/150 Loss: 0.44170379638671875\n",
      "Epoch 145/150 Loss: 0.4414595067501068\n",
      "Epoch 146/150 Loss: 0.4412150979042053\n",
      "Epoch 147/150 Loss: 0.4409697651863098\n",
      "Epoch 148/150 Loss: 0.44072386622428894\n",
      "Epoch 149/150 Loss: 0.44047781825065613\n",
      "Epoch 150/150 Loss: 0.4402312934398651\n",
      "training g\n",
      "Epoch 1/150 Loss: 4625.18359375\n",
      "Epoch 2/150 Loss: 2953.44921875\n",
      "Epoch 3/150 Loss: 1858.4317626953125\n",
      "Epoch 4/150 Loss: 1160.603271484375\n",
      "Epoch 5/150 Loss: 724.123779296875\n",
      "Epoch 6/150 Loss: 453.8384094238281\n",
      "Epoch 7/150 Loss: 286.927734375\n",
      "Epoch 8/150 Loss: 183.56124877929688\n",
      "Epoch 9/150 Loss: 119.1015853881836\n",
      "Epoch 10/150 Loss: 78.53254699707031\n",
      "Epoch 11/150 Loss: 52.744937896728516\n",
      "Epoch 12/150 Loss: 36.207733154296875\n",
      "Epoch 13/150 Loss: 25.550508499145508\n",
      "Epoch 14/150 Loss: 18.643264770507812\n",
      "Epoch 15/150 Loss: 14.085800170898438\n",
      "Epoch 16/150 Loss: 10.99577808380127\n",
      "Epoch 17/150 Loss: 8.848715782165527\n",
      "Epoch 18/150 Loss: 7.3332390785217285\n",
      "Epoch 19/150 Loss: 6.266278266906738\n",
      "Epoch 20/150 Loss: 5.5019001960754395\n",
      "Epoch 21/150 Loss: 4.931966304779053\n",
      "Epoch 22/150 Loss: 4.494383811950684\n",
      "Epoch 23/150 Loss: 4.150650501251221\n",
      "Epoch 24/150 Loss: 3.8752949237823486\n",
      "Epoch 25/150 Loss: 3.650904655456543\n",
      "Epoch 26/150 Loss: 3.464832305908203\n",
      "Epoch 27/150 Loss: 3.3087680339813232\n",
      "Epoch 28/150 Loss: 3.176304340362549\n",
      "Epoch 29/150 Loss: 3.062809467315674\n",
      "Epoch 30/150 Loss: 2.9647789001464844\n",
      "Epoch 31/150 Loss: 2.879485845565796\n",
      "Epoch 32/150 Loss: 2.804792642593384\n",
      "Epoch 33/150 Loss: 2.7389931678771973\n",
      "Epoch 34/150 Loss: 2.6807076930999756\n",
      "Epoch 35/150 Loss: 2.62882137298584\n",
      "Epoch 36/150 Loss: 2.5824148654937744\n",
      "Epoch 37/150 Loss: 2.5407257080078125\n",
      "Epoch 38/150 Loss: 2.503119945526123\n",
      "Epoch 39/150 Loss: 2.4690635204315186\n",
      "Epoch 40/150 Loss: 2.4381048679351807\n",
      "Epoch 41/150 Loss: 2.409855365753174\n",
      "Epoch 42/150 Loss: 2.383988618850708\n",
      "Epoch 43/150 Loss: 2.3602192401885986\n",
      "Epoch 44/150 Loss: 2.338303565979004\n",
      "Epoch 45/150 Loss: 2.3180274963378906\n",
      "Epoch 46/150 Loss: 2.299208879470825\n",
      "Epoch 47/150 Loss: 2.2816848754882812\n",
      "Epoch 48/150 Loss: 2.265315055847168\n",
      "Epoch 49/150 Loss: 2.2499701976776123\n",
      "Epoch 50/150 Loss: 2.2355449199676514\n",
      "Epoch 51/150 Loss: 2.2219419479370117\n",
      "Epoch 52/150 Loss: 2.209073305130005\n",
      "Epoch 53/150 Loss: 2.1968636512756348\n",
      "Epoch 54/150 Loss: 2.1852426528930664\n",
      "Epoch 55/150 Loss: 2.1741528511047363\n",
      "Epoch 56/150 Loss: 2.16353702545166\n",
      "Epoch 57/150 Loss: 2.153348922729492\n",
      "Epoch 58/150 Loss: 2.1435441970825195\n",
      "Epoch 59/150 Loss: 2.134084939956665\n",
      "Epoch 60/150 Loss: 2.124934434890747\n",
      "Epoch 61/150 Loss: 2.1160635948181152\n",
      "Epoch 62/150 Loss: 2.107440710067749\n",
      "Epoch 63/150 Loss: 2.099043846130371\n",
      "Epoch 64/150 Loss: 2.0908501148223877\n",
      "Epoch 65/150 Loss: 2.0828375816345215\n",
      "Epoch 66/150 Loss: 2.07498836517334\n",
      "Epoch 67/150 Loss: 2.067286252975464\n",
      "Epoch 68/150 Loss: 2.059715747833252\n",
      "Epoch 69/150 Loss: 2.052262783050537\n",
      "Epoch 70/150 Loss: 2.0449180603027344\n",
      "Epoch 71/150 Loss: 2.037667989730835\n",
      "Epoch 72/150 Loss: 2.0305027961730957\n",
      "Epoch 73/150 Loss: 2.023416519165039\n",
      "Epoch 74/150 Loss: 2.0163967609405518\n",
      "Epoch 75/150 Loss: 2.0094408988952637\n",
      "Epoch 76/150 Loss: 2.0025382041931152\n",
      "Epoch 77/150 Loss: 1.9956861734390259\n",
      "Epoch 78/150 Loss: 1.9888790845870972\n",
      "Epoch 79/150 Loss: 1.9821128845214844\n",
      "Epoch 80/150 Loss: 1.9753824472427368\n",
      "Epoch 81/150 Loss: 1.9686826467514038\n",
      "Epoch 82/150 Loss: 1.9620128870010376\n",
      "Epoch 83/150 Loss: 1.9553691148757935\n",
      "Epoch 84/150 Loss: 1.9487485885620117\n",
      "Epoch 85/150 Loss: 1.9421484470367432\n",
      "Epoch 86/150 Loss: 1.9355677366256714\n",
      "Epoch 87/150 Loss: 1.9290039539337158\n",
      "Epoch 88/150 Loss: 1.922455072402954\n",
      "Epoch 89/150 Loss: 1.9159202575683594\n",
      "Epoch 90/150 Loss: 1.909398078918457\n",
      "Epoch 91/150 Loss: 1.9028886556625366\n",
      "Epoch 92/150 Loss: 1.8963894844055176\n",
      "Epoch 93/150 Loss: 1.8899009227752686\n",
      "Epoch 94/150 Loss: 1.8834216594696045\n",
      "Epoch 95/150 Loss: 1.876949667930603\n",
      "Epoch 96/150 Loss: 1.8704888820648193\n",
      "Epoch 97/150 Loss: 1.8640344142913818\n",
      "Epoch 98/150 Loss: 1.85758638381958\n",
      "Epoch 99/150 Loss: 1.8511476516723633\n",
      "Epoch 100/150 Loss: 1.8447140455245972\n",
      "Epoch 101/150 Loss: 1.8382880687713623\n",
      "Epoch 102/150 Loss: 1.8318697214126587\n",
      "Epoch 103/150 Loss: 1.8254575729370117\n",
      "Epoch 104/150 Loss: 1.81905198097229\n",
      "Epoch 105/150 Loss: 1.812654733657837\n",
      "Epoch 106/150 Loss: 1.8062633275985718\n",
      "Epoch 107/150 Loss: 1.7998783588409424\n",
      "Epoch 108/150 Loss: 1.7934998273849487\n",
      "Epoch 109/150 Loss: 1.7871289253234863\n",
      "Epoch 110/150 Loss: 1.7807657718658447\n",
      "Epoch 111/150 Loss: 1.7746508121490479\n",
      "Epoch 112/150 Loss: 1.7690129280090332\n",
      "Epoch 113/150 Loss: 1.763381004333496\n",
      "Epoch 114/150 Loss: 1.7577537298202515\n",
      "Epoch 115/150 Loss: 1.7521321773529053\n",
      "Epoch 116/150 Loss: 1.7465158700942993\n",
      "Epoch 117/150 Loss: 1.7409058809280396\n",
      "Epoch 118/150 Loss: 1.7353017330169678\n",
      "Epoch 119/150 Loss: 1.7297027111053467\n",
      "Epoch 120/150 Loss: 1.7241089344024658\n",
      "Epoch 121/150 Loss: 1.718523621559143\n",
      "Epoch 122/150 Loss: 1.7129429578781128\n",
      "Epoch 123/150 Loss: 1.7073695659637451\n",
      "Epoch 124/150 Loss: 1.7018014192581177\n",
      "Epoch 125/150 Loss: 1.696241021156311\n",
      "Epoch 126/150 Loss: 1.6906869411468506\n",
      "Epoch 127/150 Loss: 1.6851396560668945\n",
      "Epoch 128/150 Loss: 1.6796005964279175\n",
      "Epoch 129/150 Loss: 1.674068570137024\n",
      "Epoch 130/150 Loss: 1.6685453653335571\n",
      "Epoch 131/150 Loss: 1.6630278825759888\n",
      "Epoch 132/150 Loss: 1.6575195789337158\n",
      "Epoch 133/150 Loss: 1.6520195007324219\n",
      "Epoch 134/150 Loss: 1.6465262174606323\n",
      "Epoch 135/150 Loss: 1.641343355178833\n",
      "Epoch 136/150 Loss: 1.636364459991455\n",
      "Epoch 137/150 Loss: 1.6313925981521606\n",
      "Epoch 138/150 Loss: 1.6264276504516602\n",
      "Epoch 139/150 Loss: 1.6214689016342163\n",
      "Epoch 140/150 Loss: 1.6170977354049683\n",
      "Epoch 141/150 Loss: 1.6128534078598022\n",
      "Epoch 142/150 Loss: 1.60861337184906\n",
      "Epoch 143/150 Loss: 1.6043784618377686\n",
      "Epoch 144/150 Loss: 1.6001487970352173\n",
      "Epoch 145/150 Loss: 1.5959242582321167\n",
      "Epoch 146/150 Loss: 1.5917032957077026\n",
      "Epoch 147/150 Loss: 1.5874888896942139\n",
      "Epoch 148/150 Loss: 1.5832791328430176\n",
      "Epoch 149/150 Loss: 1.5790743827819824\n",
      "Epoch 150/150 Loss: 1.5748741626739502\n"
     ]
    }
   ],
   "source": [
    "print('training f')\n",
    "gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "PICNNtrain(model_init_f, gaussian_transport_dataloader, lr=0.001, epochs=150, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)\n",
    "#PICNNtrain(model_init_f, gaussian_transport_dataloader, lr=0.0001, epochs=1, init_z = lambda x: x)\n",
    "\n",
    "\n",
    "print('training g')\n",
    "reversed_gaussian_dataset = MyDataset(gaussian_dataset.Y, gaussian_dataset.C, gaussian_dataset.X)\n",
    "gaussian_transport_dataset_reversed = get_gaussian_transport_dataset(reversed_gaussian_dataset)\n",
    "gaussian_transport_dataloader_reversed = DataLoader(gaussian_transport_dataset_reversed, batch_size=250, shuffle=True)\n",
    "#PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, lr=0.0001, epochs=25, init_z = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2)\n",
    "PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, lr=0.001, epochs=150, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_init_f = model_init_f.state_dict()\n",
    "state_dict_init_g = model_init_g.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=85\n",
    "# n_points = 1000\n",
    "# plot_transport(dataset, test, model_init_f, model_init_g, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('training f')\n",
    "# gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "# train_wasserstein(model_init_f, gaussian_transport_dataloader, lr=0.1, epochs=10, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y, C = gaussian_dataset.X, gaussian_dataset.Y, gaussian_dataset.C\n",
    "# #Calcul de la dérivée du PICNN\n",
    "\n",
    "# for test in range(20):\n",
    "#     x_i = X[test, :, :]\n",
    "#     y_i = Y[test, :, :]\n",
    "#     c_i = C[test, :, :]\n",
    "\n",
    "#     locs = c_i[:,0]\n",
    "#     #print(locs)\n",
    "\n",
    "#     scales = c_i[:,1]\n",
    "#     #print(scales)  \n",
    "\n",
    "\n",
    "#     y_i.requires_grad_(True)\n",
    "#     x_i.requires_grad_(True)\n",
    "#     #c_i.requires_grad_(True)    \n",
    "\n",
    "#     output_model_f = model_init_f(x_i, c_i)\n",
    "#     grad_model_f = torch.autograd.grad(outputs=output_model_f, inputs=x_i, grad_outputs=torch.ones_like(output_model_f), create_graph=True)[0].detach().numpy()\n",
    "\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True)\n",
    "#     plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True)\n",
    "#     plt.hist(grad_model_f[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     output_model_g = model_init_g(y_i, c_i)\n",
    "#     grad_model_g = torch.autograd.grad(outputs=output_model_g, inputs=y_i, grad_outputs=torch.ones_like(output_model_g), create_graph=True)[0].detach().numpy()\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True, color = 'red')\n",
    "#     #plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True, color = 'blue')\n",
    "#     plt.hist(grad_model_g[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     #plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Makkuva__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict_init_f = torch.load('trained_models/training6/models/model_f_0.pth')\n",
    "# state_dict_init_g = torch.load('trained_models/training6/models/model_g_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ICNNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ICNNf \u001b[38;5;241m=\u001b[39m \u001b[43mICNNet\u001b[49m(layer_sizes \u001b[38;5;241m=\u001b[39m layer_sizes, context_layer_sizes\u001b[38;5;241m=\u001b[39mcontext_layer_sizes, init_bunne \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m ICNNg \u001b[38;5;241m=\u001b[39m ICNNet(layer_sizes \u001b[38;5;241m=\u001b[39m layer_sizes, context_layer_sizes\u001b[38;5;241m=\u001b[39mcontext_layer_sizes, init_bunne \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m old_ICNNf \u001b[38;5;241m=\u001b[39m ICNNet(layer_sizes \u001b[38;5;241m=\u001b[39m layer_sizes, context_layer_sizes\u001b[38;5;241m=\u001b[39mcontext_layer_sizes, init_bunne \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ICNNet' is not defined"
     ]
    }
   ],
   "source": [
    "ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "old_ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "old_ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "# Load the state dictionary into ICNNf and ICNNg\n",
    "ICNNf.load_state_dict(state_dict_init_f)\n",
    "ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "old_ICNNf.load_state_dict(state_dict_init_f)\n",
    "old_ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "l_ICNNf = [ICNNf, old_ICNNf]\n",
    "l_ICNNg = [ICNNg, old_ICNNg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 1000\n",
    "test = 0\n",
    "\n",
    "filepath_pth_f = 'trained_models/training6/models/model_f_'\n",
    "filepath_pth_g = 'trained_models/training6/models/model_g_'\n",
    "\n",
    "filepath_plt_f = 'trained_models/training6/plots/model_f_'\n",
    "filepath_plt_g = 'trained_models/training6/plots/model_g_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pth_f = filepath_pth_f + str(0) + '.pth'\n",
    "filename_pth_g = filepath_pth_g + str(0) + '.pth'\n",
    "torch.save(ICNNf.state_dict(), filename_pth_f)\n",
    "torch.save(ICNNg.state_dict(), filename_pth_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename_plt_f = filepath_plt_f + str(0) + '.png'\n",
    "filename_plt_g = filepath_plt_g + str(0) + '.png'\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, filename_f = filename_plt_f, filename_g = filename_plt_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_freq_g 1\n",
      "train_freq_f 1\n",
      "loss_g: 5.22470760345459, loss_f: 3.7670202255249023\n",
      "train_freq_g 1\n",
      "train_freq_f 1\n",
      "loss_g: 5.22470760345459, loss_f: 3.7670202255249023\n",
      "train_freq_g 1\n",
      "train_freq_f 1\n",
      "loss_g: 4.409957408905029, loss_f: 2.8092312812805176\n",
      "train_freq_g 1\n",
      "train_freq_f 1\n",
      "loss_g: 4.409956932067871, loss_f: 2.8092312812805176\n",
      "train_freq_g 1\n",
      "train_freq_f 1\n",
      "loss_g: 3.376021146774292, loss_f: 1.7686762809753418\n",
      "train_freq_g 1\n",
      "train_freq_f 1\n",
      "loss_g: 3.376020908355713, loss_f: 1.7686761617660522\n",
      "train_freq_g 1\n",
      "train_freq_f 1\n",
      "loss_g: 1.8238725662231445, loss_f: 0.6411782503128052\n",
      "train_freq_g 1\n",
      "train_freq_f 1\n",
      "loss_g: 1.8238725662231445, loss_f: 0.6411781907081604\n",
      "train_freq_g 1\n",
      "train_freq_f 1\n",
      "loss_g: -0.4731605648994446, loss_f: -0.33082646131515503\n",
      "train_freq_g 1\n",
      "train_freq_f 1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=500, shuffle=True)\n",
    "\n",
    "loss_f = list()\n",
    "loss_g = list()\n",
    "\n",
    "for epoch in range(1, 31) :\n",
    "    print('epoch :', epoch, end=('\\r'))\n",
    "    mean_loss_f, mean_loss_g = train_makkuva_epoch(l_ICNNf[epoch%2], l_ICNNg[epoch%2], l_ICNNf[1 - epoch%2], l_ICNNg[1 - epoch%2], dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, lr=0.001, train_freq_g=10, train_freq_f=1, gaussian_transport=False, regularize_f = True, regularize_g = True)\n",
    "    #mean_loss_f, mean_loss_g = train_makkuva_epoch(ICNNf, ICNNg, None, None, dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2, lr=0.0001, train_freq_g=10, train_freq_f=1, gaussian_transport=False)\n",
    "\n",
    "    loss_f.append(mean_loss_f)\n",
    "    loss_g.append(mean_loss_g)\n",
    "\n",
    "    filename_pth_f = filepath_pth_f + str(epoch) + '.pth'\n",
    "    filename_pth_g = filepath_pth_g + str(epoch) + '.pth'\n",
    "    torch.save(l_ICNNf[epoch%2].state_dict(), filename_pth_f)\n",
    "    torch.save(l_ICNNg[epoch%2].state_dict(), filename_pth_g)\n",
    "\n",
    "    filename_plt_f = filepath_plt_f + str(epoch) + '.png'\n",
    "    filename_plt_g = filepath_plt_g + str(epoch) + '.png'\n",
    "    plot_transport(dataset, test, l_ICNNf[epoch%2], l_ICNNg[epoch%2], filename_f = filename_plt_f, filename_g = filename_plt_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(l_ICNNf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7670202255249023\n",
      "3.7670202255249023\n",
      "2.8092315196990967\n",
      "2.8092312812805176\n",
      "1.7686762809753418\n",
      "1.7686762809753418\n",
      "0.6411781907081604\n",
      "0.6411782503128052\n",
      "-0.33082661032676697\n",
      "-0.3308264911174774\n",
      "-2.9172003269195557\n",
      "-2.9172000885009766\n",
      "-11.605838775634766\n",
      "-11.605839729309082\n",
      "-6.383660316467285\n",
      "-6.383660316467285\n",
      "-1.5622224807739258\n",
      "-1.5622214078903198\n",
      "2.2154698371887207\n",
      "2.2155182361602783\n",
      "-1.558901071548462\n",
      "-1.5589052438735962\n",
      "2.2143967151641846\n",
      "2.215229034423828\n",
      "-1.5539886951446533\n",
      "-1.5544174909591675\n",
      "2.211855888366699\n",
      "2.21500563621521\n",
      "-1.5496015548706055\n",
      "-1.5500071048736572\n",
      "stop\n",
      "5.224708080291748\n",
      "5.22470760345459\n",
      "4.409956932067871\n",
      "4.409957408905029\n",
      "3.376020908355713\n",
      "3.376021146774292\n",
      "1.8238725662231445\n",
      "1.823872685432434\n",
      "-0.47316065430641174\n",
      "-0.47316059470176697\n",
      "-1.9266314506530762\n",
      "-1.926631212234497\n",
      "-1.5410505533218384\n",
      "-1.5410505533218384\n",
      "5.427972793579102\n",
      "5.427972316741943\n",
      "5.0130720138549805\n",
      "5.0130720138549805\n",
      "4.734468460083008\n",
      "4.734989166259766\n",
      "2.835937261581421\n",
      "2.835942029953003\n",
      "4.721141338348389\n",
      "4.723164081573486\n",
      "2.8540961742401123\n",
      "2.853990077972412\n",
      "4.690329551696777\n",
      "4.731281757354736\n",
      "2.8263823986053467\n",
      "2.8659889698028564\n"
     ]
    }
   ],
   "source": [
    "for ele in loss_f:\n",
    "    print(ele)\n",
    "\n",
    "print('stop')\n",
    "\n",
    "for ele in loss_g:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ICNNf.load_state_dict(torch.load(filepath_pth_f + '0.pth'))\n",
    "ICNNg.load_state_dict(torch.load(filepath_pth_g + '0.pth'))\n",
    "\n",
    "filename_plt_f = 'trained_models/training6/plots/model_f_test'\n",
    "filename_plt_g = 'trained_models/training6/plots/model_g_test'\n",
    "\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, filename_f = filename_plt_f, filename_g = filename_plt_g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
