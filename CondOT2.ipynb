{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icnnet import ICNNet\n",
    "from mydataset import MyDataset, get_gaussian_dataset, get_gaussian_transport_dataset\n",
    "from toy_data_dataloader_gaussian import generate_gaussian_dataset, get_dataset, generate_dataset\n",
    "from train_picnn import PICNNtrain\n",
    "from train_wasserstein import train_wasserstein\n",
    "from train_makkuva import train_makkuva, train_makkuva_epoch\n",
    "from visualization import plot_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Generate dataset__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dmgtr\\OneDrive - Ecole Polytechnique\\3A\\P2\\MAP588 - EA CondOT\\CondOT\\gaussian_transport.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  u = torch.tensor(u)\n"
     ]
    }
   ],
   "source": [
    "#dataset = get_dataset(d=2, r=100, N=500) #valou\n",
    "#dataset = generate_gaussian_dataset(d=2, r=400, N=10000) #thomas\n",
    "dataset = generate_dataset(d=2, r=500, N=100)\n",
    "gaussian_dataset = get_gaussian_dataset(dataset)\n",
    "gaussian_transport_dataset = get_gaussian_transport_dataset(gaussian_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Initialization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __PICNN training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "layer_sizes = [input_size,64,64,64, 1]\n",
    "n_layers = len(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_embedding(C, c):\n",
    "    scalar_product = torch.matmul(c.float(), C.t().float())\n",
    "    embedding = F.softmax(scalar_product, dim=1)\n",
    "    return(embedding)\n",
    "\n",
    "context_layer_sizes = [2] * n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init_f = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "model_init_g = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dmgtr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([100, 1000, 2])) that is different to the input size (torch.Size([100, 1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 Loss: 0.00930789951235056\n",
      "Epoch 2/150 Loss: 0.020446209236979485\n",
      "Epoch 3/150 Loss: 0.015238824300467968\n",
      "Epoch 4/150 Loss: 0.016683008521795273\n",
      "Epoch 5/150 Loss: 0.008686402812600136\n",
      "Epoch 6/150 Loss: 0.012463388964533806\n",
      "Epoch 7/150 Loss: 0.013700975105166435\n",
      "Epoch 8/150 Loss: 0.009687828831374645\n",
      "Epoch 9/150 Loss: 0.008809277787804604\n",
      "Epoch 10/150 Loss: 0.011523737572133541\n",
      "Epoch 11/150 Loss: 0.010905868373811245\n",
      "Epoch 12/150 Loss: 0.008489670231938362\n",
      "Epoch 13/150 Loss: 0.00861630029976368\n",
      "Epoch 14/150 Loss: 0.009903093799948692\n",
      "Epoch 15/150 Loss: 0.009603611193597317\n",
      "Epoch 16/150 Loss: 0.008255144581198692\n",
      "Epoch 17/150 Loss: 0.008122265338897705\n",
      "Epoch 18/150 Loss: 0.008986946195363998\n",
      "Epoch 19/150 Loss: 0.008835595101118088\n",
      "Epoch 20/150 Loss: 0.007945575751364231\n",
      "Epoch 21/150 Loss: 0.007790749426931143\n",
      "Epoch 22/150 Loss: 0.00825176015496254\n",
      "Epoch 23/150 Loss: 0.008264499716460705\n",
      "Epoch 24/150 Loss: 0.007741152308881283\n",
      "Epoch 25/150 Loss: 0.007537400349974632\n",
      "Epoch 26/150 Loss: 0.007830471731722355\n",
      "Epoch 27/150 Loss: 0.007859387435019016\n",
      "Epoch 28/150 Loss: 0.007494709454476833\n",
      "Epoch 29/150 Loss: 0.007341626565903425\n",
      "Epoch 30/150 Loss: 0.007497701328247786\n",
      "Epoch 31/150 Loss: 0.007519192062318325\n",
      "Epoch 32/150 Loss: 0.007287236396223307\n",
      "Epoch 33/150 Loss: 0.007165498100221157\n",
      "Epoch 34/150 Loss: 0.007264299318194389\n",
      "Epoch 35/150 Loss: 0.007255573756992817\n",
      "Epoch 36/150 Loss: 0.0070818099193274975\n",
      "Epoch 37/150 Loss: 0.007007960230112076\n",
      "Epoch 38/150 Loss: 0.007062207441776991\n",
      "Epoch 39/150 Loss: 0.007032409776002169\n",
      "Epoch 40/150 Loss: 0.006906691938638687\n",
      "Epoch 41/150 Loss: 0.006865362171083689\n",
      "Epoch 42/150 Loss: 0.006895986385643482\n",
      "Epoch 43/150 Loss: 0.006844552233815193\n",
      "Epoch 44/150 Loss: 0.006752460729330778\n",
      "Epoch 45/150 Loss: 0.006736086215823889\n",
      "Epoch 46/150 Loss: 0.006739887874573469\n",
      "Epoch 47/150 Loss: 0.006680320017039776\n",
      "Epoch 48/150 Loss: 0.006619895808398724\n",
      "Epoch 49/150 Loss: 0.006615370977669954\n",
      "Epoch 50/150 Loss: 0.006597180850803852\n",
      "Epoch 51/150 Loss: 0.006538944318890572\n",
      "Epoch 52/150 Loss: 0.006506060250103474\n",
      "Epoch 53/150 Loss: 0.006499192677438259\n",
      "Epoch 54/150 Loss: 0.006465061102062464\n",
      "Epoch 55/150 Loss: 0.006420526187866926\n",
      "Epoch 56/150 Loss: 0.006404574029147625\n",
      "Epoch 57/150 Loss: 0.006387088447809219\n",
      "Epoch 58/150 Loss: 0.006348054856061935\n",
      "Epoch 59/150 Loss: 0.006320698652416468\n",
      "Epoch 60/150 Loss: 0.006307050585746765\n",
      "Epoch 61/150 Loss: 0.0062793102115392685\n",
      "Epoch 62/150 Loss: 0.006248182151466608\n",
      "Epoch 63/150 Loss: 0.006231893319636583\n",
      "Epoch 64/150 Loss: 0.006211841478943825\n",
      "Epoch 65/150 Loss: 0.006182681303471327\n",
      "Epoch 66/150 Loss: 0.006162391509860754\n",
      "Epoch 67/150 Loss: 0.006145525723695755\n",
      "Epoch 68/150 Loss: 0.006120641250163317\n",
      "Epoch 69/150 Loss: 0.006098579149693251\n",
      "Epoch 70/150 Loss: 0.0060822851955890656\n",
      "Epoch 71/150 Loss: 0.006060528568923473\n",
      "Epoch 72/150 Loss: 0.006038121413439512\n",
      "Epoch 73/150 Loss: 0.006021223962306976\n",
      "Epoch 74/150 Loss: 0.006002255715429783\n",
      "Epoch 75/150 Loss: 0.005981484428048134\n",
      "Epoch 76/150 Loss: 0.005964935757219791\n",
      "Epoch 77/150 Loss: 0.005947743076831102\n",
      "Epoch 78/150 Loss: 0.005928349681198597\n",
      "Epoch 79/150 Loss: 0.005911890417337418\n",
      "Epoch 80/150 Loss: 0.005895720329135656\n",
      "Epoch 81/150 Loss: 0.005877663381397724\n",
      "Epoch 82/150 Loss: 0.005861604120582342\n",
      "Epoch 83/150 Loss: 0.005846071057021618\n",
      "Epoch 84/150 Loss: 0.00582898361608386\n",
      "Epoch 85/150 Loss: 0.005813373252749443\n",
      "Epoch 86/150 Loss: 0.005798306316137314\n",
      "Epoch 87/150 Loss: 0.005782369989901781\n",
      "Epoch 88/150 Loss: 0.005767696537077427\n",
      "Epoch 89/150 Loss: 0.005753338802605867\n",
      "Epoch 90/150 Loss: 0.005738324485719204\n",
      "Epoch 91/150 Loss: 0.005724418442696333\n",
      "Epoch 92/150 Loss: 0.005710764322429895\n",
      "Epoch 93/150 Loss: 0.005696802865713835\n",
      "Epoch 94/150 Loss: 0.005683804862201214\n",
      "Epoch 95/150 Loss: 0.005670831073075533\n",
      "Epoch 96/150 Loss: 0.00565779535099864\n",
      "Epoch 97/150 Loss: 0.005645559169352055\n",
      "Epoch 98/150 Loss: 0.005633293651044369\n",
      "Epoch 99/150 Loss: 0.005621347576379776\n",
      "Epoch 100/150 Loss: 0.005609950982034206\n",
      "Epoch 101/150 Loss: 0.005598457530140877\n",
      "Epoch 102/150 Loss: 0.005587444640696049\n",
      "Epoch 103/150 Loss: 0.005576753057539463\n",
      "Epoch 104/150 Loss: 0.00556618208065629\n",
      "Epoch 105/150 Loss: 0.005556222517043352\n",
      "Epoch 106/150 Loss: 0.005546371452510357\n",
      "Epoch 107/150 Loss: 0.0055367834866046906\n",
      "Epoch 108/150 Loss: 0.005527525674551725\n",
      "Epoch 109/150 Loss: 0.005518465768545866\n",
      "Epoch 110/150 Loss: 0.005509695038199425\n",
      "Epoch 111/150 Loss: 0.005501033738255501\n",
      "Epoch 112/150 Loss: 0.005492676980793476\n",
      "Epoch 113/150 Loss: 0.0054845307022333145\n",
      "Epoch 114/150 Loss: 0.005476583261042833\n",
      "Epoch 115/150 Loss: 0.005468837916851044\n",
      "Epoch 116/150 Loss: 0.005461330991238356\n",
      "Epoch 117/150 Loss: 0.005454046186059713\n",
      "Epoch 118/150 Loss: 0.005446947179734707\n",
      "Epoch 119/150 Loss: 0.005440188106149435\n",
      "Epoch 120/150 Loss: 0.005433627404272556\n",
      "Epoch 121/150 Loss: 0.00542718218639493\n",
      "Epoch 122/150 Loss: 0.005420909263193607\n",
      "Epoch 123/150 Loss: 0.0054147872142493725\n",
      "Epoch 124/150 Loss: 0.005408890545368195\n",
      "Epoch 125/150 Loss: 0.005403042770922184\n",
      "Epoch 126/150 Loss: 0.005397358443588018\n",
      "Epoch 127/150 Loss: 0.0053917644545435905\n",
      "Epoch 128/150 Loss: 0.005386332981288433\n",
      "Epoch 129/150 Loss: 0.005381034687161446\n",
      "Epoch 130/150 Loss: 0.0053758579306304455\n",
      "Epoch 131/150 Loss: 0.005370821338146925\n",
      "Epoch 132/150 Loss: 0.005365903954952955\n",
      "Epoch 133/150 Loss: 0.0053611150942742825\n",
      "Epoch 134/150 Loss: 0.005356435663998127\n",
      "Epoch 135/150 Loss: 0.0053518908098340034\n",
      "Epoch 136/150 Loss: 0.005347456783056259\n",
      "Epoch 137/150 Loss: 0.005343133583664894\n",
      "Epoch 138/150 Loss: 0.005338920745998621\n",
      "Epoch 139/150 Loss: 0.005334773566573858\n",
      "Epoch 140/150 Loss: 0.0053307111375033855\n",
      "Epoch 141/150 Loss: 0.005326731130480766\n",
      "Epoch 142/150 Loss: 0.005322842393070459\n",
      "Epoch 143/150 Loss: 0.005319039802998304\n",
      "Epoch 144/150 Loss: 0.005315334070473909\n",
      "Epoch 145/150 Loss: 0.005311714019626379\n",
      "Epoch 146/150 Loss: 0.005308195948600769\n",
      "Epoch 147/150 Loss: 0.0053047607652843\n",
      "Epoch 148/150 Loss: 0.005301409866660833\n",
      "Epoch 149/150 Loss: 0.00529817771166563\n",
      "Epoch 150/150 Loss: 0.005295006558299065\n"
     ]
    }
   ],
   "source": [
    "print('training f')\n",
    "gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "PICNNtrain(model_init_f, gaussian_transport_dataloader, lr=0.001, epochs=150, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)\n",
    "#PICNNtrain(model_init_f, gaussian_transport_dataloader, lr=0.0001, epochs=1, init_z = lambda x: x)\n",
    "\n",
    "\n",
    "# print('training g')\n",
    "# reversed_gaussian_dataset = MyDataset(gaussian_dataset.Y, gaussian_dataset.C, gaussian_dataset.X)\n",
    "# gaussian_transport_dataset_reversed = get_gaussian_transport_dataset(reversed_gaussian_dataset)\n",
    "# gaussian_transport_dataloader_reversed = DataLoader(gaussian_transport_dataset_reversed, batch_size=250, shuffle=True)\n",
    "# #PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, lr=0.0001, epochs=25, init_z = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2)\n",
    "# PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, lr=0.001, epochs=150, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_init_f = model_init_f.state_dict()\n",
    "state_dict_init_g = model_init_g.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=85\n",
    "# n_points = 1000\n",
    "# plot_transport(dataset, test, model_init_f, model_init_g, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('training f')\n",
    "# gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "# train_wasserstein(model_init_f, gaussian_transport_dataloader, lr=0.1, epochs=10, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y, C = gaussian_dataset.X, gaussian_dataset.Y, gaussian_dataset.C\n",
    "# #Calcul de la dérivée du PICNN\n",
    "\n",
    "# for test in range(20):\n",
    "#     x_i = X[test, :, :]\n",
    "#     y_i = Y[test, :, :]\n",
    "#     c_i = C[test, :, :]\n",
    "\n",
    "#     locs = c_i[:,0]\n",
    "#     #print(locs)\n",
    "\n",
    "#     scales = c_i[:,1]\n",
    "#     #print(scales)  \n",
    "\n",
    "\n",
    "#     y_i.requires_grad_(True)\n",
    "#     x_i.requires_grad_(True)\n",
    "#     #c_i.requires_grad_(True)    \n",
    "\n",
    "#     output_model_f = model_init_f(x_i, c_i)\n",
    "#     grad_model_f = torch.autograd.grad(outputs=output_model_f, inputs=x_i, grad_outputs=torch.ones_like(output_model_f), create_graph=True)[0].detach().numpy()\n",
    "\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True)\n",
    "#     plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True)\n",
    "#     plt.hist(grad_model_f[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     output_model_g = model_init_g(y_i, c_i)\n",
    "#     grad_model_g = torch.autograd.grad(outputs=output_model_g, inputs=y_i, grad_outputs=torch.ones_like(output_model_g), create_graph=True)[0].detach().numpy()\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True, color = 'red')\n",
    "#     #plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True, color = 'blue')\n",
    "#     plt.hist(grad_model_g[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     #plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Makkuva__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict_init_f = torch.load('trained_models/training7/models/model_f_0.pth')\n",
    "# state_dict_init_g = torch.load('trained_models/training7/models/model_g_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "old_ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "old_ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "# Load the state dictionary into ICNNf and ICNNg\n",
    "ICNNf.load_state_dict(state_dict_init_f)\n",
    "ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "old_ICNNf.load_state_dict(state_dict_init_f)\n",
    "old_ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "l_ICNNf = [ICNNf, old_ICNNf]\n",
    "l_ICNNg = [ICNNg, old_ICNNg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 1000\n",
    "test = 1\n",
    "\n",
    "filepath_pth_f = 'trained_models/training7/models/model_f_'\n",
    "filepath_pth_g = 'trained_models/training7/models/model_g_'\n",
    "\n",
    "filepath_plt_f = 'trained_models/training7/plots/model_f_'\n",
    "filepath_plt_g = 'trained_models/training7/plots/model_g_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pth_f = filepath_pth_f + str(0) + '.pth'\n",
    "filename_pth_g = filepath_pth_g + str(0) + '.pth'\n",
    "torch.save(ICNNf.state_dict(), filename_pth_f)\n",
    "torch.save(ICNNg.state_dict(), filename_pth_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename_plt_f = filepath_plt_f + str(0) + '.png'\n",
    "filename_plt_g = filepath_plt_g + str(0) + '.png'\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, filename_f = filename_plt_f, filename_g = filename_plt_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8513221740722656, loss_f: -0.14827898144721985\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8790445327758789, loss_f: 0.005424084607511759\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9345914125442505, loss_f: 0.045086801052093506\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8980900049209595, loss_f: 0.04262238368391991\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9109423756599426, loss_f: 0.03842322900891304\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7769259214401245, loss_f: -0.1907612681388855\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7901700139045715, loss_f: -0.2497549206018448\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9639826416969299, loss_f: 0.06952670216560364\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9895268678665161, loss_f: 0.19078482687473297\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8184962272644043, loss_f: -0.21198277175426483\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8220099210739136, loss_f: -0.09197838604450226\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9589150547981262, loss_f: 0.12439444661140442\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9119579792022705, loss_f: 0.09850722551345825\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8711416721343994, loss_f: -0.11259204149246216\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8270872235298157, loss_f: -0.15632280707359314\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9219125509262085, loss_f: 0.04727530851960182\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9239761829376221, loss_f: 0.03961414843797684\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8952189683914185, loss_f: 0.03605497628450394\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8928579688072205, loss_f: -0.06310093402862549\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.801038920879364, loss_f: -0.1443682461977005\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9215595126152039, loss_f: 0.051105812191963196\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9560186862945557, loss_f: 0.08225013315677643\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9036725759506226, loss_f: -0.004855766426771879\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7531381845474243, loss_f: -0.2657952606678009\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8746644854545593, loss_f: 0.030947290360927582\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9230936765670776, loss_f: 0.20716895163059235\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8737240433692932, loss_f: -0.10214466601610184\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7725694179534912, loss_f: -0.04814242571592331\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9351451992988586, loss_f: 0.06228938326239586\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7869825959205627, loss_f: -0.01921173371374607\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8922739028930664, loss_f: -0.03416843339800835\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8653253316879272, loss_f: 0.00836229044944048\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9063429832458496, loss_f: 0.09536775201559067\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8097092509269714, loss_f: 0.010086080059409142\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8346328735351562, loss_f: -0.14750023186206818\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8443603515625, loss_f: -0.03132428973913193\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9242963194847107, loss_f: 0.042596034705638885\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8347194194793701, loss_f: 0.12270325422286987\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9103397727012634, loss_f: -0.02795254811644554\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7973902821540833, loss_f: -0.12717106938362122\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9074259400367737, loss_f: 0.1016254872083664\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8774201273918152, loss_f: 0.018921855837106705\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8393822908401489, loss_f: -0.11902669072151184\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8980293869972229, loss_f: 0.07365439087152481\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9123333692550659, loss_f: -0.006904115434736013\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7586597204208374, loss_f: -0.2669472396373749\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9549022316932678, loss_f: 0.09713462740182877\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9358381032943726, loss_f: 0.21855615079402924\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8092784285545349, loss_f: -0.20029675960540771\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7752913236618042, loss_f: -0.059032637625932693\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9896020889282227, loss_f: 0.1512870192527771\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8357123136520386, loss_f: 0.10596498101949692\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8362168073654175, loss_f: -0.11941223591566086\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7603656053543091, loss_f: -0.1337987333536148\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9341976046562195, loss_f: 0.042783379554748535\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8385521769523621, loss_f: 0.020359830930829048\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8891034126281738, loss_f: -0.12854300439357758\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.844460129737854, loss_f: -0.11484595388174057\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9502179622650146, loss_f: 0.09573839604854584\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8973586559295654, loss_f: 0.068885937333107\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8699763417243958, loss_f: -0.10442080348730087\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8614328503608704, loss_f: -0.0348915234208107\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9415264129638672, loss_f: 0.06494956463575363\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8688251376152039, loss_f: 0.06575711071491241\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8782906532287598, loss_f: -0.11979196965694427\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8326296210289001, loss_f: -0.10153969377279282\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9540655016899109, loss_f: 0.09981013834476471\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8882195353507996, loss_f: 0.021247398108243942\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.871982216835022, loss_f: -0.09246820956468582\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8901852369308472, loss_f: 0.07617093622684479\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9536230564117432, loss_f: 0.07669221609830856\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7599798440933228, loss_f: -0.24386712908744812\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8676955699920654, loss_f: -0.10534220933914185\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9294159412384033, loss_f: 0.22059834003448486\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9581181406974792, loss_f: 0.08624353259801865\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7773975729942322, loss_f: -0.05464915186166763\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.868306577205658, loss_f: -0.09605289995670319\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8412335515022278, loss_f: 0.09847988188266754\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9593388438224792, loss_f: 0.08345986157655716\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.7499525547027588, loss_f: -0.10881669819355011\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8706681728363037, loss_f: -0.10364630818367004\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8408589959144592, loss_f: 0.013552358373999596\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9629859924316406, loss_f: 0.0883219763636589\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8281334638595581, loss_f: -0.08649050444364548\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8738843202590942, loss_f: -0.10159937292337418\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8623626828193665, loss_f: 0.06556658446788788\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9660815596580505, loss_f: 0.08733291178941727\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8349405527114868, loss_f: -0.08465300500392914\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8772419095039368, loss_f: -0.10059493780136108\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8662604093551636, loss_f: 0.0772566869854927\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.968183696269989, loss_f: 0.09102234244346619\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8371996283531189, loss_f: -0.06325548887252808\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8787521719932556, loss_f: -0.10384871065616608\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8619167804718018, loss_f: 0.045258525758981705\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9707323312759399, loss_f: 0.08426246047019958\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8585793375968933, loss_f: -0.014123082160949707\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8813470005989075, loss_f: -0.11556578427553177\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8369619250297546, loss_f: 0.02718275599181652\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.9735069870948792, loss_f: 0.09499820321798325\n",
      "train_freq_g 10\n",
      "train_freq_f 1\n",
      "loss_g: 0.8367645740509033, loss_f: -0.059217050671577454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=500, shuffle=True)\n",
    "\n",
    "loss_f = list()\n",
    "loss_g = list()\n",
    "\n",
    "for epoch in range(251, 351) :\n",
    "    print('epoch :', epoch, end=('\\r'))\n",
    "    mean_loss_f, mean_loss_g = train_makkuva_epoch(l_ICNNf[epoch%2], l_ICNNg[epoch%2], l_ICNNf[1 - epoch%2], l_ICNNg[1 - epoch%2], dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, lr=0.001, train_freq_g=10, train_freq_f=1, gaussian_transport=False, regularize_f = False, regularize_g = True)\n",
    "    #mean_loss_f, mean_loss_g = train_makkuva_epoch(ICNNf, ICNNg, None, None, dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2, lr=0.0001, train_freq_g=10, train_freq_f=1, gaussian_transport=False)\n",
    "\n",
    "    loss_f.append(mean_loss_f)\n",
    "    loss_g.append(mean_loss_g)\n",
    "\n",
    "    filename_pth_f = filepath_pth_f + str(epoch) + '.pth'\n",
    "    filename_pth_g = filepath_pth_g + str(epoch) + '.pth'\n",
    "    torch.save(l_ICNNf[epoch%2].state_dict(), filename_pth_f)\n",
    "    torch.save(l_ICNNg[epoch%2].state_dict(), filename_pth_g)\n",
    "\n",
    "    filename_plt_f = filepath_plt_f + str(epoch) + '.png'\n",
    "    filename_plt_g = filepath_plt_g + str(epoch) + '.png'\n",
    "    plot_transport(dataset, test, l_ICNNf[epoch%2], l_ICNNg[epoch%2], filename_f = filename_plt_f, filename_g = filename_plt_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.14827898144721985\n",
      "0.005424084607511759\n",
      "0.045086801052093506\n",
      "0.04262238368391991\n",
      "0.03842322900891304\n",
      "-0.1907612681388855\n",
      "-0.2497549206018448\n",
      "0.06952670216560364\n",
      "0.19078482687473297\n",
      "-0.21198277175426483\n",
      "-0.09197838604450226\n",
      "0.12439444661140442\n",
      "0.09850722551345825\n",
      "-0.11259204149246216\n",
      "-0.15632280707359314\n",
      "0.04727530851960182\n",
      "0.03961414843797684\n",
      "0.03605497628450394\n",
      "-0.06310093402862549\n",
      "-0.1443682461977005\n",
      "0.051105812191963196\n",
      "0.08225013315677643\n",
      "-0.004855766426771879\n",
      "-0.2657952606678009\n",
      "0.030947290360927582\n",
      "0.20716895163059235\n",
      "-0.10214466601610184\n",
      "-0.04814242571592331\n",
      "0.06228938326239586\n",
      "-0.01921173371374607\n",
      "-0.03416843339800835\n",
      "0.00836229044944048\n",
      "0.09536775201559067\n",
      "0.010086080059409142\n",
      "-0.14750023186206818\n",
      "-0.03132428973913193\n",
      "0.042596034705638885\n",
      "0.12270325422286987\n",
      "-0.02795254811644554\n",
      "-0.12717106938362122\n",
      "0.1016254872083664\n",
      "0.018921855837106705\n",
      "-0.11902669072151184\n",
      "0.07365439087152481\n",
      "-0.006904115434736013\n",
      "-0.2669472396373749\n",
      "0.09713462740182877\n",
      "0.21855615079402924\n",
      "-0.20029675960540771\n",
      "-0.059032637625932693\n",
      "0.1512870192527771\n",
      "0.10596498101949692\n",
      "-0.11941223591566086\n",
      "-0.1337987333536148\n",
      "0.042783379554748535\n",
      "0.020359830930829048\n",
      "-0.12854300439357758\n",
      "-0.11484595388174057\n",
      "0.09573839604854584\n",
      "0.068885937333107\n",
      "-0.10442080348730087\n",
      "-0.0348915234208107\n",
      "0.06494956463575363\n",
      "0.06575711071491241\n",
      "-0.11979196965694427\n",
      "-0.10153969377279282\n",
      "0.09981013834476471\n",
      "0.021247398108243942\n",
      "-0.09246820956468582\n",
      "0.07617093622684479\n",
      "0.07669221609830856\n",
      "-0.24386712908744812\n",
      "-0.10534220933914185\n",
      "0.22059834003448486\n",
      "0.08624353259801865\n",
      "-0.05464915186166763\n",
      "-0.09605289995670319\n",
      "0.09847988188266754\n",
      "0.08345986157655716\n",
      "-0.10881669819355011\n",
      "-0.10364630818367004\n",
      "0.013552358373999596\n",
      "0.0883219763636589\n",
      "-0.08649050444364548\n",
      "-0.10159937292337418\n",
      "0.06556658446788788\n",
      "0.08733291178941727\n",
      "-0.08465300500392914\n",
      "-0.10059493780136108\n",
      "0.0772566869854927\n",
      "0.09102234244346619\n",
      "-0.06325548887252808\n",
      "-0.10384871065616608\n",
      "0.045258525758981705\n",
      "0.08426246047019958\n",
      "-0.014123082160949707\n",
      "-0.11556578427553177\n",
      "0.02718275599181652\n",
      "0.09499820321798325\n",
      "-0.059217050671577454\n",
      "stop\n",
      "0.8513221740722656\n",
      "0.8790445327758789\n",
      "0.9345914125442505\n",
      "0.8980900049209595\n",
      "0.9109423756599426\n",
      "0.7769259214401245\n",
      "0.7901700139045715\n",
      "0.9639826416969299\n",
      "0.9895268678665161\n",
      "0.8184962272644043\n",
      "0.8220099210739136\n",
      "0.9589150547981262\n",
      "0.9119579792022705\n",
      "0.8711416721343994\n",
      "0.8270872235298157\n",
      "0.9219125509262085\n",
      "0.9239761829376221\n",
      "0.8952189683914185\n",
      "0.8928579688072205\n",
      "0.801038920879364\n",
      "0.9215595126152039\n",
      "0.9560186862945557\n",
      "0.9036725759506226\n",
      "0.7531381845474243\n",
      "0.8746644854545593\n",
      "0.9230936765670776\n",
      "0.8737240433692932\n",
      "0.7725694179534912\n",
      "0.9351451992988586\n",
      "0.7869825959205627\n",
      "0.8922739028930664\n",
      "0.8653253316879272\n",
      "0.9063429832458496\n",
      "0.8097092509269714\n",
      "0.8346328735351562\n",
      "0.8443603515625\n",
      "0.9242963194847107\n",
      "0.8347194194793701\n",
      "0.9103397727012634\n",
      "0.7973902821540833\n",
      "0.9074259400367737\n",
      "0.8774201273918152\n",
      "0.8393822908401489\n",
      "0.8980293869972229\n",
      "0.9123333692550659\n",
      "0.7586597204208374\n",
      "0.9549022316932678\n",
      "0.9358381032943726\n",
      "0.8092784285545349\n",
      "0.7752913236618042\n",
      "0.9896020889282227\n",
      "0.8357123136520386\n",
      "0.8362168073654175\n",
      "0.7603656053543091\n",
      "0.9341976046562195\n",
      "0.8385521769523621\n",
      "0.8891034126281738\n",
      "0.844460129737854\n",
      "0.9502179622650146\n",
      "0.8973586559295654\n",
      "0.8699763417243958\n",
      "0.8614328503608704\n",
      "0.9415264129638672\n",
      "0.8688251376152039\n",
      "0.8782906532287598\n",
      "0.8326296210289001\n",
      "0.9540655016899109\n",
      "0.8882195353507996\n",
      "0.871982216835022\n",
      "0.8901852369308472\n",
      "0.9536230564117432\n",
      "0.7599798440933228\n",
      "0.8676955699920654\n",
      "0.9294159412384033\n",
      "0.9581181406974792\n",
      "0.7773975729942322\n",
      "0.868306577205658\n",
      "0.8412335515022278\n",
      "0.9593388438224792\n",
      "0.7499525547027588\n",
      "0.8706681728363037\n",
      "0.8408589959144592\n",
      "0.9629859924316406\n",
      "0.8281334638595581\n",
      "0.8738843202590942\n",
      "0.8623626828193665\n",
      "0.9660815596580505\n",
      "0.8349405527114868\n",
      "0.8772419095039368\n",
      "0.8662604093551636\n",
      "0.968183696269989\n",
      "0.8371996283531189\n",
      "0.8787521719932556\n",
      "0.8619167804718018\n",
      "0.9707323312759399\n",
      "0.8585793375968933\n",
      "0.8813470005989075\n",
      "0.8369619250297546\n",
      "0.9735069870948792\n",
      "0.8367645740509033\n"
     ]
    }
   ],
   "source": [
    "for ele in loss_f:\n",
    "    print(ele)\n",
    "\n",
    "print('stop')\n",
    "\n",
    "for ele in loss_g:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICNNf.load_state_dict(torch.load(filepath_pth_f + '0.pth'))\n",
    "ICNNg.load_state_dict(torch.load(filepath_pth_g + '0.pth'))\n",
    "\n",
    "filename_plt_f = 'trained_models/training7/plots/model_f_test'\n",
    "filename_plt_g = 'trained_models/training7/plots/model_g_test'\n",
    "\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, filename_f = filename_plt_f, filename_g = filename_plt_g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
