{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icnnet import ICNNet\n",
    "from mydataset import MyDataset, get_gaussian_dataset, get_gaussian_transport_dataset\n",
    "from toy_data_dataloader_gaussian import generate_gaussian_dataset, get_dataset, generate_dataset\n",
    "from train_picnn import PICNNtrain\n",
    "from train_wasserstein import train_wasserstein\n",
    "from train_makkuva import train_makkuva, train_makkuva_epoch\n",
    "from visualization import plot_transport\n",
    "from gaussian_transport import get_gaussian_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Generate dataset__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = get_dataset(d=2, r=100, N=500) #valou\n",
    "#dataset = generate_gaussian_dataset(d=2, r=400, N=10000) #thomas\n",
    "dataset = generate_dataset(d=2, r=1000, N=50)\n",
    "gaussian_dataset = get_gaussian_dataset(dataset)\n",
    "gaussian_transport_dataset = get_gaussian_transport_dataset(gaussian_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(batch):\n",
    "    means = torch.mean(batch, dim=1)\n",
    "    average_mean = torch.mean(means, dim=0)\n",
    "    return(average_mean)\n",
    "\n",
    "def get_covariance(batch):\n",
    "    n = batch.size(1) - 1\n",
    "    mean = torch.mean(batch, dim=1, keepdim=True)\n",
    "    batch = batch - mean  # Centering the data\n",
    "    cov = torch.matmul(batch.transpose(1, 2), batch) / n\n",
    "    return(torch.mean(cov, dim=0))\n",
    "\n",
    "mean1 = get_mean(dataset.X)\n",
    "cov1 = get_covariance(dataset.X)\n",
    "mean2 = get_mean(dataset.Y)\n",
    "cov2 = get_covariance(dataset.Y)\n",
    "\n",
    "def init_z_f(x):\n",
    "    #return (1/2) * torch.norm(x, dim=-1, keepdim=True)**2\n",
    "    return(get_gaussian_transport(u=x, cov1 = cov1, cov2 = cov2, m1=mean1, m2=mean2))\n",
    "\n",
    "def init_z_g(x) :\n",
    "    #return (1/2) * torch.norm(x, dim=-1, keepdim=True)**2\n",
    "    return(get_gaussian_transport(u=x, cov1 = cov2, cov2 = cov2, m1=mean2, m2=mean1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Initialization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __PICNN training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "layer_sizes = [input_size,64, 64, 64, 1]\n",
    "n_layers = len(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# def get_embedding(C, c):\n",
    "#     scalar_product = torch.matmul(c.float(), C.t().float())\n",
    "#     embedding = F.softmax(scalar_product, dim=1)\n",
    "#     return(embedding)\n",
    "\n",
    "context_layer_sizes = [12] * n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init_f = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "model_init_g = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training f\n",
      "Epoch 1/500 Loss: 38.021671295166016\n",
      "Epoch 2/500 Loss: 37.6040153503418\n",
      "Epoch 3/500 Loss: 37.47692108154297\n",
      "Epoch 4/500 Loss: 37.34761428833008\n",
      "Epoch 5/500 Loss: 37.213741302490234\n",
      "Epoch 6/500 Loss: 37.0707893371582\n",
      "Epoch 7/500 Loss: 36.913421630859375\n",
      "Epoch 8/500 Loss: 36.73466873168945\n",
      "Epoch 9/500 Loss: 36.5279655456543\n",
      "Epoch 10/500 Loss: 36.287384033203125\n",
      "Epoch 11/500 Loss: 36.00726318359375\n",
      "Epoch 12/500 Loss: 35.6817741394043\n",
      "Epoch 13/500 Loss: 35.30423355102539\n",
      "Epoch 14/500 Loss: 34.86676025390625\n",
      "Epoch 15/500 Loss: 34.35957336425781\n",
      "Epoch 16/500 Loss: 33.77012634277344\n",
      "Epoch 17/500 Loss: 33.082374572753906\n",
      "Epoch 18/500 Loss: 32.27539825439453\n",
      "Epoch 19/500 Loss: 31.320959091186523\n",
      "Epoch 20/500 Loss: 30.17909049987793\n",
      "Epoch 21/500 Loss: 28.79749298095703\n",
      "Epoch 22/500 Loss: 27.10463523864746\n",
      "Epoch 23/500 Loss: 25.004173278808594\n",
      "Epoch 24/500 Loss: 22.377017974853516\n",
      "Epoch 25/500 Loss: 19.078102111816406\n",
      "Epoch 26/500 Loss: 14.976136207580566\n",
      "Epoch 27/500 Loss: 10.064568519592285\n",
      "Epoch 28/500 Loss: 4.796741008758545\n",
      "Epoch 29/500 Loss: 0.9453250765800476\n",
      "Epoch 30/500 Loss: 2.8779208660125732\n",
      "Epoch 31/500 Loss: 7.927917957305908\n",
      "Epoch 32/500 Loss: 6.808036804199219\n",
      "Epoch 33/500 Loss: 3.327338457107544\n",
      "Epoch 34/500 Loss: 1.1229431629180908\n",
      "Epoch 35/500 Loss: 0.8367627263069153\n",
      "Epoch 36/500 Loss: 1.592065453529358\n",
      "Epoch 37/500 Loss: 2.5328171253204346\n",
      "Epoch 38/500 Loss: 3.232271909713745\n",
      "Epoch 39/500 Loss: 3.5554871559143066\n",
      "Epoch 40/500 Loss: 3.5001869201660156\n",
      "Epoch 41/500 Loss: 3.1205968856811523\n",
      "Epoch 42/500 Loss: 2.5091865062713623\n",
      "Epoch 43/500 Loss: 1.8000775575637817\n",
      "Epoch 44/500 Loss: 1.1735637187957764\n",
      "Epoch 45/500 Loss: 0.8326789140701294\n",
      "Epoch 46/500 Loss: 0.9142588973045349\n",
      "Epoch 47/500 Loss: 1.3404552936553955\n",
      "Epoch 48/500 Loss: 1.767746925354004\n",
      "Epoch 49/500 Loss: 1.83319091796875\n",
      "Epoch 50/500 Loss: 1.4974496364593506\n",
      "Epoch 51/500 Loss: 1.0323102474212646\n",
      "Epoch 52/500 Loss: 0.7181148529052734\n",
      "Epoch 53/500 Loss: 0.6480979919433594\n",
      "Epoch 54/500 Loss: 0.7524378895759583\n",
      "Epoch 55/500 Loss: 0.9062746167182922\n",
      "Epoch 56/500 Loss: 1.0069223642349243\n",
      "Epoch 57/500 Loss: 1.002163290977478\n",
      "Epoch 58/500 Loss: 0.8909856081008911\n",
      "Epoch 59/500 Loss: 0.7140817046165466\n",
      "Epoch 60/500 Loss: 0.5384369492530823\n",
      "Epoch 61/500 Loss: 0.43226686120033264\n",
      "Epoch 62/500 Loss: 0.43032747507095337\n",
      "Epoch 63/500 Loss: 0.5054935812950134\n",
      "Epoch 64/500 Loss: 0.5784029960632324\n",
      "Epoch 65/500 Loss: 0.5762062668800354\n",
      "Epoch 66/500 Loss: 0.4917193353176117\n",
      "Epoch 67/500 Loss: 0.3808421790599823\n",
      "Epoch 68/500 Loss: 0.30681371688842773\n",
      "Epoch 69/500 Loss: 0.2933453619480133\n",
      "Epoch 70/500 Loss: 0.32089316844940186\n",
      "Epoch 71/500 Loss: 0.3516017496585846\n",
      "Epoch 72/500 Loss: 0.35565808415412903\n",
      "Epoch 73/500 Loss: 0.32509154081344604\n",
      "Epoch 74/500 Loss: 0.2738341689109802\n",
      "Epoch 75/500 Loss: 0.22738642990589142\n",
      "Epoch 76/500 Loss: 0.20676830410957336\n",
      "Epoch 77/500 Loss: 0.21463263034820557\n",
      "Epoch 78/500 Loss: 0.23343004286289215\n",
      "Epoch 79/500 Loss: 0.23965193331241608\n",
      "Epoch 80/500 Loss: 0.22320356965065002\n",
      "Epoch 81/500 Loss: 0.19390012323856354\n",
      "Epoch 82/500 Loss: 0.17010238766670227\n",
      "Epoch 83/500 Loss: 0.16260235011577606\n",
      "Epoch 84/500 Loss: 0.16831108927726746\n",
      "Epoch 85/500 Loss: 0.17577268183231354\n",
      "Epoch 86/500 Loss: 0.17499089241027832\n",
      "Epoch 87/500 Loss: 0.1638881266117096\n",
      "Epoch 88/500 Loss: 0.1483006775379181\n",
      "Epoch 89/500 Loss: 0.1367368996143341\n",
      "Epoch 90/500 Loss: 0.13382425904273987\n",
      "Epoch 91/500 Loss: 0.13711634278297424\n",
      "Epoch 92/500 Loss: 0.13974308967590332\n",
      "Epoch 93/500 Loss: 0.13659575581550598\n",
      "Epoch 94/500 Loss: 0.1283702701330185\n",
      "Epoch 95/500 Loss: 0.12001711130142212\n",
      "Epoch 96/500 Loss: 0.11582040041685104\n",
      "Epoch 97/500 Loss: 0.1160232350230217\n",
      "Epoch 98/500 Loss: 0.11746254563331604\n",
      "Epoch 99/500 Loss: 0.11674553900957108\n",
      "Epoch 100/500 Loss: 0.11293181031942368\n",
      "Epoch 101/500 Loss: 0.1078287810087204\n",
      "Epoch 102/500 Loss: 0.10410228371620178\n",
      "Epoch 103/500 Loss: 0.1029680073261261\n",
      "Epoch 104/500 Loss: 0.10335992276668549\n",
      "Epoch 105/500 Loss: 0.10315035283565521\n",
      "Epoch 106/500 Loss: 0.10115872323513031\n",
      "Epoch 107/500 Loss: 0.09803466498851776\n",
      "Epoch 108/500 Loss: 0.0953679084777832\n",
      "Epoch 109/500 Loss: 0.09411698579788208\n",
      "Epoch 110/500 Loss: 0.09389375895261765\n",
      "Epoch 111/500 Loss: 0.09355301409959793\n",
      "Epoch 112/500 Loss: 0.09231459349393845\n",
      "Epoch 113/500 Loss: 0.09036557376384735\n",
      "Epoch 114/500 Loss: 0.08852894604206085\n",
      "Epoch 115/500 Loss: 0.08743783086538315\n",
      "Epoch 116/500 Loss: 0.08699584007263184\n",
      "Epoch 117/500 Loss: 0.0865754708647728\n",
      "Epoch 118/500 Loss: 0.0856737270951271\n",
      "Epoch 119/500 Loss: 0.08435401320457458\n",
      "Epoch 120/500 Loss: 0.08308767527341843\n",
      "Epoch 121/500 Loss: 0.08224302530288696\n",
      "Epoch 122/500 Loss: 0.0817616805434227\n",
      "Epoch 123/500 Loss: 0.08128911256790161\n",
      "Epoch 124/500 Loss: 0.08055342733860016\n",
      "Epoch 125/500 Loss: 0.07959946990013123\n",
      "Epoch 126/500 Loss: 0.07869153469800949\n",
      "Epoch 127/500 Loss: 0.0780269205570221\n",
      "Epoch 128/500 Loss: 0.07756008952856064\n",
      "Epoch 129/500 Loss: 0.07708451896905899\n",
      "Epoch 130/500 Loss: 0.07645754516124725\n",
      "Epoch 131/500 Loss: 0.07572710514068604\n",
      "Epoch 132/500 Loss: 0.07505100965499878\n",
      "Epoch 133/500 Loss: 0.0745231956243515\n",
      "Epoch 134/500 Loss: 0.07409301400184631\n",
      "Epoch 135/500 Loss: 0.07363997399806976\n",
      "Epoch 136/500 Loss: 0.07310277223587036\n",
      "Epoch 137/500 Loss: 0.07252848893404007\n",
      "Epoch 138/500 Loss: 0.07200635969638824\n",
      "Epoch 139/500 Loss: 0.07157169282436371\n",
      "Epoch 140/500 Loss: 0.07117908447980881\n",
      "Epoch 141/500 Loss: 0.0707613006234169\n",
      "Epoch 142/500 Loss: 0.07030028104782104\n",
      "Epoch 143/500 Loss: 0.06983675807714462\n",
      "Epoch 144/500 Loss: 0.06941774487495422\n",
      "Epoch 145/500 Loss: 0.06904707849025726\n",
      "Epoch 146/500 Loss: 0.06868860125541687\n",
      "Epoch 147/500 Loss: 0.06831000745296478\n",
      "Epoch 148/500 Loss: 0.06791502982378006\n",
      "Epoch 149/500 Loss: 0.06753292679786682\n",
      "Epoch 150/500 Loss: 0.06718319654464722\n",
      "Epoch 151/500 Loss: 0.06685682386159897\n",
      "Epoch 152/500 Loss: 0.06653011590242386\n",
      "Epoch 153/500 Loss: 0.06619216501712799\n",
      "Epoch 154/500 Loss: 0.06585400551557541\n",
      "Epoch 155/500 Loss: 0.06553258746862411\n",
      "Epoch 156/500 Loss: 0.06523161381483078\n",
      "Epoch 157/500 Loss: 0.06493936479091644\n",
      "Epoch 158/500 Loss: 0.06464403122663498\n",
      "Epoch 159/500 Loss: 0.06434596329927444\n",
      "Epoch 160/500 Loss: 0.06405504047870636\n",
      "Epoch 161/500 Loss: 0.06377797573804855\n",
      "Epoch 162/500 Loss: 0.06351123005151749\n",
      "Epoch 163/500 Loss: 0.06324668973684311\n",
      "Epoch 164/500 Loss: 0.06298107653856277\n",
      "Epoch 165/500 Loss: 0.0627187043428421\n",
      "Epoch 166/500 Loss: 0.06246509775519371\n",
      "Epoch 167/500 Loss: 0.0622204914689064\n",
      "Epoch 168/500 Loss: 0.061980240046978\n",
      "Epoch 169/500 Loss: 0.061740703880786896\n",
      "Epoch 170/500 Loss: 0.06150303781032562\n",
      "Epoch 171/500 Loss: 0.061270955950021744\n",
      "Epoch 172/500 Loss: 0.06104593724012375\n",
      "Epoch 173/500 Loss: 0.06082567945122719\n",
      "Epoch 174/500 Loss: 0.060607511550188065\n",
      "Epoch 175/500 Loss: 0.06039125844836235\n",
      "Epoch 176/500 Loss: 0.06017901375889778\n",
      "Epoch 177/500 Loss: 0.059971973299980164\n",
      "Epoch 178/500 Loss: 0.059769198298454285\n",
      "Epoch 179/500 Loss: 0.05956884101033211\n",
      "Epoch 180/500 Loss: 0.05937045067548752\n",
      "Epoch 181/500 Loss: 0.05917507782578468\n",
      "Epoch 182/500 Loss: 0.05898367986083031\n",
      "Epoch 183/500 Loss: 0.058795977383852005\n",
      "Epoch 184/500 Loss: 0.05861075595021248\n",
      "Epoch 185/500 Loss: 0.05842765048146248\n",
      "Epoch 186/500 Loss: 0.05824722722172737\n",
      "Epoch 187/500 Loss: 0.05807005241513252\n",
      "Epoch 188/500 Loss: 0.0578959621489048\n",
      "Epoch 189/500 Loss: 0.05772415176033974\n",
      "Epoch 190/500 Loss: 0.0575542151927948\n",
      "Epoch 191/500 Loss: 0.05738656222820282\n",
      "Epoch 192/500 Loss: 0.0572216734290123\n",
      "Epoch 193/500 Loss: 0.05705936625599861\n",
      "Epoch 194/500 Loss: 0.05689917877316475\n",
      "Epoch 195/500 Loss: 0.05674092099070549\n",
      "Epoch 196/500 Loss: 0.05658480525016785\n",
      "Epoch 197/500 Loss: 0.056430988013744354\n",
      "Epoch 198/500 Loss: 0.05627930164337158\n",
      "Epoch 199/500 Loss: 0.05612938478589058\n",
      "Epoch 200/500 Loss: 0.05598127096891403\n",
      "Epoch 201/500 Loss: 0.05583500862121582\n",
      "Epoch 202/500 Loss: 0.05569073185324669\n",
      "Epoch 203/500 Loss: 0.05554832145571709\n",
      "Epoch 204/500 Loss: 0.055407606065273285\n",
      "Epoch 205/500 Loss: 0.05526858568191528\n",
      "Epoch 206/500 Loss: 0.055131327360868454\n",
      "Epoch 207/500 Loss: 0.05499584972858429\n",
      "Epoch 208/500 Loss: 0.054861944168806076\n",
      "Epoch 209/500 Loss: 0.05472945421934128\n",
      "Epoch 210/500 Loss: 0.054598476737737656\n",
      "Epoch 211/500 Loss: 0.054469119757413864\n",
      "Epoch 212/500 Loss: 0.05434132367372513\n",
      "Epoch 213/500 Loss: 0.054214999079704285\n",
      "Epoch 214/500 Loss: 0.054090145975351334\n",
      "Epoch 215/500 Loss: 0.05396682024002075\n",
      "Epoch 216/500 Loss: 0.05384499207139015\n",
      "Epoch 217/500 Loss: 0.05372454226016998\n",
      "Epoch 218/500 Loss: 0.05360542982816696\n",
      "Epoch 219/500 Loss: 0.053487684577703476\n",
      "Epoch 220/500 Loss: 0.05337131395936012\n",
      "Epoch 221/500 Loss: 0.0532563142478466\n",
      "Epoch 222/500 Loss: 0.05314264073967934\n",
      "Epoch 223/500 Loss: 0.05303027480840683\n",
      "Epoch 224/500 Loss: 0.052919212728738785\n",
      "Epoch 225/500 Loss: 0.052809473127126694\n",
      "Epoch 226/500 Loss: 0.052701011300086975\n",
      "Epoch 227/500 Loss: 0.052593763917684555\n",
      "Epoch 228/500 Loss: 0.052487812936306\n",
      "Epoch 229/500 Loss: 0.05238313972949982\n",
      "Epoch 230/500 Loss: 0.0522797554731369\n",
      "Epoch 231/500 Loss: 0.05217762663960457\n",
      "Epoch 232/500 Loss: 0.05207677558064461\n",
      "Epoch 233/500 Loss: 0.05197720229625702\n",
      "Epoch 234/500 Loss: 0.051878876984119415\n",
      "Epoch 235/500 Loss: 0.05178175866603851\n",
      "Epoch 236/500 Loss: 0.051685869693756104\n",
      "Epoch 237/500 Loss: 0.05159122869372368\n",
      "Epoch 238/500 Loss: 0.05149786174297333\n",
      "Epoch 239/500 Loss: 0.05140577256679535\n",
      "Epoch 240/500 Loss: 0.05131491273641586\n",
      "Epoch 241/500 Loss: 0.05122530087828636\n",
      "Epoch 242/500 Loss: 0.05113694444298744\n",
      "Epoch 243/500 Loss: 0.05104978382587433\n",
      "Epoch 244/500 Loss: 0.050963837653398514\n",
      "Epoch 245/500 Loss: 0.05087916553020477\n",
      "Epoch 246/500 Loss: 0.05079572647809982\n",
      "Epoch 247/500 Loss: 0.05071350932121277\n",
      "Epoch 248/500 Loss: 0.050632525235414505\n",
      "Epoch 249/500 Loss: 0.05055275559425354\n",
      "Epoch 250/500 Loss: 0.05047420412302017\n",
      "Epoch 251/500 Loss: 0.0503968670964241\n",
      "Epoch 252/500 Loss: 0.05032074823975563\n",
      "Epoch 253/500 Loss: 0.05024581402540207\n",
      "Epoch 254/500 Loss: 0.050172120332717896\n",
      "Epoch 255/500 Loss: 0.05009963735938072\n",
      "Epoch 256/500 Loss: 0.05002836510539055\n",
      "Epoch 257/500 Loss: 0.049958329647779465\n",
      "Epoch 258/500 Loss: 0.04988951236009598\n",
      "Epoch 259/500 Loss: 0.04982186481356621\n",
      "Epoch 260/500 Loss: 0.049755390733480453\n",
      "Epoch 261/500 Loss: 0.04969009757041931\n",
      "Epoch 262/500 Loss: 0.04962596669793129\n",
      "Epoch 263/500 Loss: 0.04956298694014549\n",
      "Epoch 264/500 Loss: 0.04950116202235222\n",
      "Epoch 265/500 Loss: 0.04944046959280968\n",
      "Epoch 266/500 Loss: 0.04938088729977608\n",
      "Epoch 267/500 Loss: 0.049322426319122314\n",
      "Epoch 268/500 Loss: 0.049265019595623016\n",
      "Epoch 269/500 Loss: 0.04920865595340729\n",
      "Epoch 270/500 Loss: 0.049153316766023636\n",
      "Epoch 271/500 Loss: 0.04909899830818176\n",
      "Epoch 272/500 Loss: 0.04904570430517197\n",
      "Epoch 273/500 Loss: 0.04899336397647858\n",
      "Epoch 274/500 Loss: 0.048941973596811295\n",
      "Epoch 275/500 Loss: 0.04889148846268654\n",
      "Epoch 276/500 Loss: 0.04884186387062073\n",
      "Epoch 277/500 Loss: 0.04879307746887207\n",
      "Epoch 278/500 Loss: 0.048745136708021164\n",
      "Epoch 279/500 Loss: 0.04869800806045532\n",
      "Epoch 280/500 Loss: 0.0486515611410141\n",
      "Epoch 281/500 Loss: 0.04860581457614899\n",
      "Epoch 282/500 Loss: 0.048560842871665955\n",
      "Epoch 283/500 Loss: 0.04851645603775978\n",
      "Epoch 284/500 Loss: 0.04847259819507599\n",
      "Epoch 285/500 Loss: 0.048429254442453384\n",
      "Epoch 286/500 Loss: 0.04838638752698898\n",
      "Epoch 287/500 Loss: 0.04834391921758652\n",
      "Epoch 288/500 Loss: 0.0483020544052124\n",
      "Epoch 289/500 Loss: 0.04826103150844574\n",
      "Epoch 290/500 Loss: 0.04822060465812683\n",
      "Epoch 291/500 Loss: 0.04818064346909523\n",
      "Epoch 292/500 Loss: 0.048141151666641235\n",
      "Epoch 293/500 Loss: 0.04810219258069992\n",
      "Epoch 294/500 Loss: 0.0480639711022377\n",
      "Epoch 295/500 Loss: 0.04802617058157921\n",
      "Epoch 296/500 Loss: 0.04798905923962593\n",
      "Epoch 297/500 Loss: 0.04795248433947563\n",
      "Epoch 298/500 Loss: 0.04791649430990219\n",
      "Epoch 299/500 Loss: 0.0478811040520668\n",
      "Epoch 300/500 Loss: 0.04784660041332245\n",
      "Epoch 301/500 Loss: 0.04781273752450943\n",
      "Epoch 302/500 Loss: 0.047779522836208344\n",
      "Epoch 303/500 Loss: 0.04774677753448486\n",
      "Epoch 304/500 Loss: 0.04771453142166138\n",
      "Epoch 305/500 Loss: 0.04768283665180206\n",
      "Epoch 306/500 Loss: 0.04765181243419647\n",
      "Epoch 307/500 Loss: 0.04762139543890953\n",
      "Epoch 308/500 Loss: 0.04759158194065094\n",
      "Epoch 309/500 Loss: 0.04756227880716324\n",
      "Epoch 310/500 Loss: 0.047533467411994934\n",
      "Epoch 311/500 Loss: 0.04750513657927513\n",
      "Epoch 312/500 Loss: 0.04747731611132622\n",
      "Epoch 313/500 Loss: 0.04744995757937431\n",
      "Epoch 314/500 Loss: 0.04742307588458061\n",
      "Epoch 315/500 Loss: 0.047396622598171234\n",
      "Epoch 316/500 Loss: 0.0473705418407917\n",
      "Epoch 317/500 Loss: 0.04734485223889351\n",
      "Epoch 318/500 Loss: 0.04731956869363785\n",
      "Epoch 319/500 Loss: 0.04729468747973442\n",
      "Epoch 320/500 Loss: 0.04727017506957054\n",
      "Epoch 321/500 Loss: 0.04724600911140442\n",
      "Epoch 322/500 Loss: 0.047222211956977844\n",
      "Epoch 323/500 Loss: 0.04719876870512962\n",
      "Epoch 324/500 Loss: 0.04717566817998886\n",
      "Epoch 325/500 Loss: 0.04715288057923317\n",
      "Epoch 326/500 Loss: 0.047130435705184937\n",
      "Epoch 327/500 Loss: 0.04710830748081207\n",
      "Epoch 328/500 Loss: 0.04708651453256607\n",
      "Epoch 329/500 Loss: 0.04706503078341484\n",
      "Epoch 330/500 Loss: 0.047043826431035995\n",
      "Epoch 331/500 Loss: 0.04702293872833252\n",
      "Epoch 332/500 Loss: 0.04700232297182083\n",
      "Epoch 333/500 Loss: 0.04698200151324272\n",
      "Epoch 334/500 Loss: 0.0469619445502758\n",
      "Epoch 335/500 Loss: 0.046942129731178284\n",
      "Epoch 336/500 Loss: 0.04692259430885315\n",
      "Epoch 337/500 Loss: 0.04690328985452652\n",
      "Epoch 338/500 Loss: 0.04688423499464989\n",
      "Epoch 339/500 Loss: 0.04686543345451355\n",
      "Epoch 340/500 Loss: 0.046846866607666016\n",
      "Epoch 341/500 Loss: 0.04682851582765579\n",
      "Epoch 342/500 Loss: 0.046810392290353775\n",
      "Epoch 343/500 Loss: 0.04679250344634056\n",
      "Epoch 344/500 Loss: 0.04677483066916466\n",
      "Epoch 345/500 Loss: 0.04675736278295517\n",
      "Epoch 346/500 Loss: 0.046740103513002396\n",
      "Epoch 347/500 Loss: 0.04672304540872574\n",
      "Epoch 348/500 Loss: 0.0467061810195446\n",
      "Epoch 349/500 Loss: 0.046689484268426895\n",
      "Epoch 350/500 Loss: 0.04667295888066292\n",
      "Epoch 351/500 Loss: 0.04665663465857506\n",
      "Epoch 352/500 Loss: 0.046640489250421524\n",
      "Epoch 353/500 Loss: 0.04662450775504112\n",
      "Epoch 354/500 Loss: 0.04660866782069206\n",
      "Epoch 355/500 Loss: 0.04659297317266464\n",
      "Epoch 356/500 Loss: 0.04657739773392677\n",
      "Epoch 357/500 Loss: 0.046561937779188156\n",
      "Epoch 358/500 Loss: 0.04654659330844879\n",
      "Epoch 359/500 Loss: 0.046531371772289276\n",
      "Epoch 360/500 Loss: 0.04651634767651558\n",
      "Epoch 361/500 Loss: 0.046501561999320984\n",
      "Epoch 362/500 Loss: 0.04648704454302788\n",
      "Epoch 363/500 Loss: 0.04647272825241089\n",
      "Epoch 364/500 Loss: 0.04645860940217972\n",
      "Epoch 365/500 Loss: 0.04644462838768959\n",
      "Epoch 366/500 Loss: 0.046430762857198715\n",
      "Epoch 367/500 Loss: 0.04641704261302948\n",
      "Epoch 368/500 Loss: 0.0464034266769886\n",
      "Epoch 369/500 Loss: 0.046389952301979065\n",
      "Epoch 370/500 Loss: 0.04637661948800087\n",
      "Epoch 371/500 Loss: 0.04636341705918312\n",
      "Epoch 372/500 Loss: 0.04635034501552582\n",
      "Epoch 373/500 Loss: 0.04633742570877075\n",
      "Epoch 374/500 Loss: 0.04632464051246643\n",
      "Epoch 375/500 Loss: 0.04631196707487106\n",
      "Epoch 376/500 Loss: 0.046299442648887634\n",
      "Epoch 377/500 Loss: 0.04628705605864525\n",
      "Epoch 378/500 Loss: 0.046274833381175995\n",
      "Epoch 379/500 Loss: 0.046262793242931366\n",
      "Epoch 380/500 Loss: 0.04625094309449196\n",
      "Epoch 381/500 Loss: 0.046239223331213\n",
      "Epoch 382/500 Loss: 0.04622766003012657\n",
      "Epoch 383/500 Loss: 0.046216242015361786\n",
      "Epoch 384/500 Loss: 0.04620502516627312\n",
      "Epoch 385/500 Loss: 0.04619402438402176\n",
      "Epoch 386/500 Loss: 0.04618338868021965\n",
      "Epoch 387/500 Loss: 0.04617336764931679\n",
      "Epoch 388/500 Loss: 0.04616374894976616\n",
      "Epoch 389/500 Loss: 0.04615432769060135\n",
      "Epoch 390/500 Loss: 0.0461428239941597\n",
      "Epoch 391/500 Loss: 0.04613068699836731\n",
      "Epoch 392/500 Loss: 0.046119559556245804\n",
      "Epoch 393/500 Loss: 0.04611034318804741\n",
      "Epoch 394/500 Loss: 0.046101201325654984\n",
      "Epoch 395/500 Loss: 0.04609067365527153\n",
      "Epoch 396/500 Loss: 0.046080078929662704\n",
      "Epoch 397/500 Loss: 0.04607049748301506\n",
      "Epoch 398/500 Loss: 0.046061549335718155\n",
      "Epoch 399/500 Loss: 0.046052154153585434\n",
      "Epoch 400/500 Loss: 0.046042270958423615\n",
      "Epoch 401/500 Loss: 0.04603276401758194\n",
      "Epoch 402/500 Loss: 0.04602385312318802\n",
      "Epoch 403/500 Loss: 0.04601505771279335\n",
      "Epoch 404/500 Loss: 0.04600600525736809\n",
      "Epoch 405/500 Loss: 0.04599682614207268\n",
      "Epoch 406/500 Loss: 0.045987870544195175\n",
      "Epoch 407/500 Loss: 0.04597923159599304\n",
      "Epoch 408/500 Loss: 0.045970745384693146\n",
      "Epoch 409/500 Loss: 0.04596220329403877\n",
      "Epoch 410/500 Loss: 0.04595359414815903\n",
      "Epoch 411/500 Loss: 0.04594501480460167\n",
      "Epoch 412/500 Loss: 0.045936573296785355\n",
      "Epoch 413/500 Loss: 0.04592829942703247\n",
      "Epoch 414/500 Loss: 0.04592016711831093\n",
      "Epoch 415/500 Loss: 0.04591209441423416\n",
      "Epoch 416/500 Loss: 0.045904047787189484\n",
      "Epoch 417/500 Loss: 0.04589604586362839\n",
      "Epoch 418/500 Loss: 0.045888081192970276\n",
      "Epoch 419/500 Loss: 0.045880187302827835\n",
      "Epoch 420/500 Loss: 0.045872364193201065\n",
      "Epoch 421/500 Loss: 0.04586463421583176\n",
      "Epoch 422/500 Loss: 0.04585697129368782\n",
      "Epoch 423/500 Loss: 0.04584939032793045\n",
      "Epoch 424/500 Loss: 0.04584188386797905\n",
      "Epoch 425/500 Loss: 0.04583445191383362\n",
      "Epoch 426/500 Loss: 0.04582710564136505\n",
      "Epoch 427/500 Loss: 0.045819852501153946\n",
      "Epoch 428/500 Loss: 0.045812711119651794\n",
      "Epoch 429/500 Loss: 0.04580575227737427\n",
      "Epoch 430/500 Loss: 0.045799072831869125\n",
      "Epoch 431/500 Loss: 0.04579295963048935\n",
      "Epoch 432/500 Loss: 0.0457877442240715\n",
      "Epoch 433/500 Loss: 0.045784104615449905\n",
      "Epoch 434/500 Loss: 0.045780282467603683\n",
      "Epoch 435/500 Loss: 0.04577488452196121\n",
      "Epoch 436/500 Loss: 0.04576309397816658\n",
      "Epoch 437/500 Loss: 0.045750685036182404\n",
      "Epoch 438/500 Loss: 0.045742787420749664\n",
      "Epoch 439/500 Loss: 0.045739173889160156\n",
      "Epoch 440/500 Loss: 0.045734141021966934\n",
      "Epoch 441/500 Loss: 0.0457247719168663\n",
      "Epoch 442/500 Loss: 0.04571533203125\n",
      "Epoch 443/500 Loss: 0.045708827674388885\n",
      "Epoch 444/500 Loss: 0.04570365697145462\n",
      "Epoch 445/500 Loss: 0.045696962624788284\n",
      "Epoch 446/500 Loss: 0.045688219368457794\n",
      "Epoch 447/500 Loss: 0.04567971080541611\n",
      "Epoch 448/500 Loss: 0.04567261412739754\n",
      "Epoch 449/500 Loss: 0.045665938407182693\n",
      "Epoch 450/500 Loss: 0.04565859213471413\n",
      "Epoch 451/500 Loss: 0.04565019905567169\n",
      "Epoch 452/500 Loss: 0.04564155638217926\n",
      "Epoch 453/500 Loss: 0.04563317447900772\n",
      "Epoch 454/500 Loss: 0.04562530294060707\n",
      "Epoch 455/500 Loss: 0.04561762139201164\n",
      "Epoch 456/500 Loss: 0.04560953006148338\n",
      "Epoch 457/500 Loss: 0.04560131952166557\n",
      "Epoch 458/500 Loss: 0.04559307545423508\n",
      "Epoch 459/500 Loss: 0.04558529332280159\n",
      "Epoch 460/500 Loss: 0.04557803273200989\n",
      "Epoch 461/500 Loss: 0.04557124897837639\n",
      "Epoch 462/500 Loss: 0.045565005391836166\n",
      "Epoch 463/500 Loss: 0.04555938020348549\n",
      "Epoch 464/500 Loss: 0.04555618017911911\n",
      "Epoch 465/500 Loss: 0.04556012153625488\n",
      "Epoch 466/500 Loss: 0.04560035467147827\n",
      "Epoch 467/500 Loss: 0.045718614012002945\n",
      "Epoch 468/500 Loss: 0.04572976380586624\n",
      "Epoch 469/500 Loss: 0.04554923251271248\n",
      "Epoch 470/500 Loss: 0.045560020953416824\n",
      "Epoch 471/500 Loss: 0.04557531327009201\n",
      "Epoch 472/500 Loss: 0.04551076143980026\n",
      "Epoch 473/500 Loss: 0.045544371008872986\n",
      "Epoch 474/500 Loss: 0.04551057517528534\n",
      "Epoch 475/500 Loss: 0.04550682008266449\n",
      "Epoch 476/500 Loss: 0.04551122710108757\n",
      "Epoch 477/500 Loss: 0.04548776522278786\n",
      "Epoch 478/500 Loss: 0.04549691453576088\n",
      "Epoch 479/500 Loss: 0.045479223132133484\n",
      "Epoch 480/500 Loss: 0.045481566339731216\n",
      "Epoch 481/500 Loss: 0.04547295719385147\n",
      "Epoch 482/500 Loss: 0.04546866565942764\n",
      "Epoch 483/500 Loss: 0.045466821640729904\n",
      "Epoch 484/500 Loss: 0.04545743763446808\n",
      "Epoch 485/500 Loss: 0.045457709580659866\n",
      "Epoch 486/500 Loss: 0.04544961079955101\n",
      "Epoch 487/500 Loss: 0.045446280390024185\n",
      "Epoch 488/500 Loss: 0.0454435758292675\n",
      "Epoch 489/500 Loss: 0.04543643072247505\n",
      "Epoch 490/500 Loss: 0.045434482395648956\n",
      "Epoch 491/500 Loss: 0.04542939364910126\n",
      "Epoch 492/500 Loss: 0.04542434215545654\n",
      "Epoch 493/500 Loss: 0.0454220250248909\n",
      "Epoch 494/500 Loss: 0.04541666433215141\n",
      "Epoch 495/500 Loss: 0.045413047075271606\n",
      "Epoch 496/500 Loss: 0.04540984332561493\n",
      "Epoch 497/500 Loss: 0.045404817909002304\n",
      "Epoch 498/500 Loss: 0.04540160670876503\n",
      "Epoch 499/500 Loss: 0.0453980378806591\n",
      "Epoch 500/500 Loss: 0.045393556356430054\n",
      "training g\n",
      "Epoch 1/500 Loss: 1.5849897861480713\n",
      "Epoch 2/500 Loss: 1.224472165107727\n",
      "Epoch 3/500 Loss: 1.0418282747268677\n",
      "Epoch 4/500 Loss: 0.8550621867179871\n",
      "Epoch 5/500 Loss: 0.6844468712806702\n",
      "Epoch 6/500 Loss: 0.5380886793136597\n",
      "Epoch 7/500 Loss: 0.4192802309989929\n",
      "Epoch 8/500 Loss: 0.32861757278442383\n",
      "Epoch 9/500 Loss: 0.2648319602012634\n",
      "Epoch 10/500 Loss: 0.22515366971492767\n",
      "Epoch 11/500 Loss: 0.20566706359386444\n",
      "Epoch 12/500 Loss: 0.20168519020080566\n",
      "Epoch 13/500 Loss: 0.20812422037124634\n",
      "Epoch 14/500 Loss: 0.2198573648929596\n",
      "Epoch 15/500 Loss: 0.2321828454732895\n",
      "Epoch 16/500 Loss: 0.2409266233444214\n",
      "Epoch 17/500 Loss: 0.24364951252937317\n",
      "Epoch 18/500 Loss: 0.2390538603067398\n",
      "Epoch 19/500 Loss: 0.22692683339118958\n",
      "Epoch 20/500 Loss: 0.20841354131698608\n",
      "Epoch 21/500 Loss: 0.18577535450458527\n",
      "Epoch 22/500 Loss: 0.16201692819595337\n",
      "Epoch 23/500 Loss: 0.13998818397521973\n",
      "Epoch 24/500 Loss: 0.12195326387882233\n",
      "Epoch 25/500 Loss: 0.10924522578716278\n",
      "Epoch 26/500 Loss: 0.10206722468137741\n",
      "Epoch 27/500 Loss: 0.09954704344272614\n",
      "Epoch 28/500 Loss: 0.0999419167637825\n",
      "Epoch 29/500 Loss: 0.10094722360372543\n",
      "Epoch 30/500 Loss: 0.10004771500825882\n",
      "Epoch 31/500 Loss: 0.09529634565114975\n",
      "Epoch 32/500 Loss: 0.08607204258441925\n",
      "Epoch 33/500 Loss: 0.07342451065778732\n",
      "Epoch 34/500 Loss: 0.059521131217479706\n",
      "Epoch 35/500 Loss: 0.046619854867458344\n",
      "Epoch 36/500 Loss: 0.03691711276769638\n",
      "Epoch 37/500 Loss: 0.02994207851588726\n",
      "Epoch 38/500 Loss: 0.02522554248571396\n",
      "Epoch 39/500 Loss: 0.021983051672577858\n",
      "Epoch 40/500 Loss: 0.01954711601138115\n",
      "Epoch 41/500 Loss: 0.017660895362496376\n",
      "Epoch 42/500 Loss: 0.01666266657412052\n",
      "Epoch 43/500 Loss: 0.017307354137301445\n",
      "Epoch 44/500 Loss: 0.019703814759850502\n",
      "Epoch 45/500 Loss: 0.02199798822402954\n",
      "Epoch 46/500 Loss: 0.022600460797548294\n",
      "Epoch 47/500 Loss: 0.022151565179228783\n",
      "Epoch 48/500 Loss: 0.02183109149336815\n",
      "Epoch 49/500 Loss: 0.02193528786301613\n",
      "Epoch 50/500 Loss: 0.02198992669582367\n",
      "Epoch 51/500 Loss: 0.02146323397755623\n",
      "Epoch 52/500 Loss: 0.020237214863300323\n",
      "Epoch 53/500 Loss: 0.01865428499877453\n",
      "Epoch 54/500 Loss: 0.017208505421876907\n",
      "Epoch 55/500 Loss: 0.01619253121316433\n",
      "Epoch 56/500 Loss: 0.0155819496139884\n",
      "Epoch 57/500 Loss: 0.015156600624322891\n",
      "Epoch 58/500 Loss: 0.014718298800289631\n",
      "Epoch 59/500 Loss: 0.014242634177207947\n",
      "Epoch 60/500 Loss: 0.013844969682395458\n",
      "Epoch 61/500 Loss: 0.013641939498484135\n",
      "Epoch 62/500 Loss: 0.013648731634020805\n",
      "Epoch 63/500 Loss: 0.01377637218683958\n",
      "Epoch 64/500 Loss: 0.01390169095247984\n",
      "Epoch 65/500 Loss: 0.013928032480180264\n",
      "Epoch 66/500 Loss: 0.01381783839315176\n",
      "Epoch 67/500 Loss: 0.013595450669527054\n",
      "Epoch 68/500 Loss: 0.013331687077879906\n",
      "Epoch 69/500 Loss: 0.013110442087054253\n",
      "Epoch 70/500 Loss: 0.012976719997823238\n",
      "Epoch 71/500 Loss: 0.012891033664345741\n",
      "Epoch 72/500 Loss: 0.012764806859195232\n",
      "Epoch 73/500 Loss: 0.012583133764564991\n",
      "Epoch 74/500 Loss: 0.01243535801768303\n",
      "Epoch 75/500 Loss: 0.012403713539242744\n",
      "Epoch 76/500 Loss: 0.01246570236980915\n",
      "Epoch 77/500 Loss: 0.012528674677014351\n",
      "Epoch 78/500 Loss: 0.012531605549156666\n",
      "Epoch 79/500 Loss: 0.01249138917773962\n",
      "Epoch 80/500 Loss: 0.012451530434191227\n",
      "Epoch 81/500 Loss: 0.012397761456668377\n",
      "Epoch 82/500 Loss: 0.012296440079808235\n",
      "Epoch 83/500 Loss: 0.012171882204711437\n",
      "Epoch 84/500 Loss: 0.012077150866389275\n",
      "Epoch 85/500 Loss: 0.012029017321765423\n",
      "Epoch 86/500 Loss: 0.012006400153040886\n",
      "Epoch 87/500 Loss: 0.011986720375716686\n",
      "Epoch 88/500 Loss: 0.01196469459682703\n",
      "Epoch 89/500 Loss: 0.011943541467189789\n",
      "Epoch 90/500 Loss: 0.011926919221878052\n",
      "Epoch 91/500 Loss: 0.011906295083463192\n",
      "Epoch 92/500 Loss: 0.011868461035192013\n",
      "Epoch 93/500 Loss: 0.011814495548605919\n",
      "Epoch 94/500 Loss: 0.011759760789573193\n",
      "Epoch 95/500 Loss: 0.011716747656464577\n",
      "Epoch 96/500 Loss: 0.011684492230415344\n",
      "Epoch 97/500 Loss: 0.011659255251288414\n",
      "Epoch 98/500 Loss: 0.011642800644040108\n",
      "Epoch 99/500 Loss: 0.011631055735051632\n",
      "Epoch 100/500 Loss: 0.011610409244894981\n",
      "Epoch 101/500 Loss: 0.011578239500522614\n",
      "Epoch 102/500 Loss: 0.01154522318392992\n",
      "Epoch 103/500 Loss: 0.011513735167682171\n",
      "Epoch 104/500 Loss: 0.011479275301098824\n",
      "Epoch 105/500 Loss: 0.011445662006735802\n",
      "Epoch 106/500 Loss: 0.011419318616390228\n",
      "Epoch 107/500 Loss: 0.01139820646494627\n",
      "Epoch 108/500 Loss: 0.01137659139931202\n",
      "Epoch 109/500 Loss: 0.011354460380971432\n",
      "Epoch 110/500 Loss: 0.011332789435982704\n",
      "Epoch 111/500 Loss: 0.011310311034321785\n",
      "Epoch 112/500 Loss: 0.011286660097539425\n",
      "Epoch 113/500 Loss: 0.011266321875154972\n",
      "Epoch 114/500 Loss: 0.011248300783336163\n",
      "Epoch 115/500 Loss: 0.011228461749851704\n",
      "Epoch 116/500 Loss: 0.011207478120923042\n",
      "Epoch 117/500 Loss: 0.011186174117028713\n",
      "Epoch 118/500 Loss: 0.011163037270307541\n",
      "Epoch 119/500 Loss: 0.011140020564198494\n",
      "Epoch 120/500 Loss: 0.011118628084659576\n",
      "Epoch 121/500 Loss: 0.01109674759209156\n",
      "Epoch 122/500 Loss: 0.01107487641274929\n",
      "Epoch 123/500 Loss: 0.011054227128624916\n",
      "Epoch 124/500 Loss: 0.011033152230083942\n",
      "Epoch 125/500 Loss: 0.011011503636837006\n",
      "Epoch 126/500 Loss: 0.010989675298333168\n",
      "Epoch 127/500 Loss: 0.01096667256206274\n",
      "Epoch 128/500 Loss: 0.010943352244794369\n",
      "Epoch 129/500 Loss: 0.010920432396233082\n",
      "Epoch 130/500 Loss: 0.010897002182900906\n",
      "Epoch 131/500 Loss: 0.010873259045183659\n",
      "Epoch 132/500 Loss: 0.010849040932953358\n",
      "Epoch 133/500 Loss: 0.010824096389114857\n",
      "Epoch 134/500 Loss: 0.010798696428537369\n",
      "Epoch 135/500 Loss: 0.010772360488772392\n",
      "Epoch 136/500 Loss: 0.010745517909526825\n",
      "Epoch 137/500 Loss: 0.010719123296439648\n",
      "Epoch 138/500 Loss: 0.01069206278771162\n",
      "Epoch 139/500 Loss: 0.010664297267794609\n",
      "Epoch 140/500 Loss: 0.0106353173032403\n",
      "Epoch 141/500 Loss: 0.010605180636048317\n",
      "Epoch 142/500 Loss: 0.010575024411082268\n",
      "Epoch 143/500 Loss: 0.01054474338889122\n",
      "Epoch 144/500 Loss: 0.010514338500797749\n",
      "Epoch 145/500 Loss: 0.010483820922672749\n",
      "Epoch 146/500 Loss: 0.010453355498611927\n",
      "Epoch 147/500 Loss: 0.010422982275485992\n",
      "Epoch 148/500 Loss: 0.010392452590167522\n",
      "Epoch 149/500 Loss: 0.01036163792014122\n",
      "Epoch 150/500 Loss: 0.010330384597182274\n",
      "Epoch 151/500 Loss: 0.010298755951225758\n",
      "Epoch 152/500 Loss: 0.010266812518239021\n",
      "Epoch 153/500 Loss: 0.010234750807285309\n",
      "Epoch 154/500 Loss: 0.010202412493526936\n",
      "Epoch 155/500 Loss: 0.010168933309614658\n",
      "Epoch 156/500 Loss: 0.010134228505194187\n",
      "Epoch 157/500 Loss: 0.010099542327225208\n",
      "Epoch 158/500 Loss: 0.010064647532999516\n",
      "Epoch 159/500 Loss: 0.010029114782810211\n",
      "Epoch 160/500 Loss: 0.00999292079359293\n",
      "Epoch 161/500 Loss: 0.009955969639122486\n",
      "Epoch 162/500 Loss: 0.009918563067913055\n",
      "Epoch 163/500 Loss: 0.009881398640573025\n",
      "Epoch 164/500 Loss: 0.009844375774264336\n",
      "Epoch 165/500 Loss: 0.009806955233216286\n",
      "Epoch 166/500 Loss: 0.009768410585820675\n",
      "Epoch 167/500 Loss: 0.00972842425107956\n",
      "Epoch 168/500 Loss: 0.009687362238764763\n",
      "Epoch 169/500 Loss: 0.009645594283938408\n",
      "Epoch 170/500 Loss: 0.00960349477827549\n",
      "Epoch 171/500 Loss: 0.009561514481902122\n",
      "Epoch 172/500 Loss: 0.009519778192043304\n",
      "Epoch 173/500 Loss: 0.009479496628046036\n",
      "Epoch 174/500 Loss: 0.009441114962100983\n",
      "Epoch 175/500 Loss: 0.009401367045938969\n",
      "Epoch 176/500 Loss: 0.009360086172819138\n",
      "Epoch 177/500 Loss: 0.00931843463331461\n",
      "Epoch 178/500 Loss: 0.00927795097231865\n",
      "Epoch 179/500 Loss: 0.009239571169018745\n",
      "Epoch 180/500 Loss: 0.009203714318573475\n",
      "Epoch 181/500 Loss: 0.009169302880764008\n",
      "Epoch 182/500 Loss: 0.009135393425822258\n",
      "Epoch 183/500 Loss: 0.009100483730435371\n",
      "Epoch 184/500 Loss: 0.009064353071153164\n",
      "Epoch 185/500 Loss: 0.009028022177517414\n",
      "Epoch 186/500 Loss: 0.008990935981273651\n",
      "Epoch 187/500 Loss: 0.008953583426773548\n",
      "Epoch 188/500 Loss: 0.00891619548201561\n",
      "Epoch 189/500 Loss: 0.008877847343683243\n",
      "Epoch 190/500 Loss: 0.008839919231832027\n",
      "Epoch 191/500 Loss: 0.008803644217550755\n",
      "Epoch 192/500 Loss: 0.00877075269818306\n",
      "Epoch 193/500 Loss: 0.00874185562133789\n",
      "Epoch 194/500 Loss: 0.008712162263691425\n",
      "Epoch 195/500 Loss: 0.008681297302246094\n",
      "Epoch 196/500 Loss: 0.008648128248751163\n",
      "Epoch 197/500 Loss: 0.00861740205436945\n",
      "Epoch 198/500 Loss: 0.008590934798121452\n",
      "Epoch 199/500 Loss: 0.008568412624299526\n",
      "Epoch 200/500 Loss: 0.008545986376702785\n",
      "Epoch 201/500 Loss: 0.0085203992202878\n",
      "Epoch 202/500 Loss: 0.008493700064718723\n",
      "Epoch 203/500 Loss: 0.008467696607112885\n",
      "Epoch 204/500 Loss: 0.00844467431306839\n",
      "Epoch 205/500 Loss: 0.008423620834946632\n",
      "Epoch 206/500 Loss: 0.008402085863053799\n",
      "Epoch 207/500 Loss: 0.008379279635846615\n",
      "Epoch 208/500 Loss: 0.008353929035365582\n",
      "Epoch 209/500 Loss: 0.008327808231115341\n",
      "Epoch 210/500 Loss: 0.008302395232021809\n",
      "Epoch 211/500 Loss: 0.008277851156890392\n",
      "Epoch 212/500 Loss: 0.00825407076627016\n",
      "Epoch 213/500 Loss: 0.008231986314058304\n",
      "Epoch 214/500 Loss: 0.008210109546780586\n",
      "Epoch 215/500 Loss: 0.008187191560864449\n",
      "Epoch 216/500 Loss: 0.00816362164914608\n",
      "Epoch 217/500 Loss: 0.008136481046676636\n",
      "Epoch 218/500 Loss: 0.008106994442641735\n",
      "Epoch 219/500 Loss: 0.00807517021894455\n",
      "Epoch 220/500 Loss: 0.008043410256505013\n",
      "Epoch 221/500 Loss: 0.008012444712221622\n",
      "Epoch 222/500 Loss: 0.007981429807841778\n",
      "Epoch 223/500 Loss: 0.00794947799295187\n",
      "Epoch 224/500 Loss: 0.007915840484201908\n",
      "Epoch 225/500 Loss: 0.007881421595811844\n",
      "Epoch 226/500 Loss: 0.007843668572604656\n",
      "Epoch 227/500 Loss: 0.007804328575730324\n",
      "Epoch 228/500 Loss: 0.007762630004435778\n",
      "Epoch 229/500 Loss: 0.007722546812146902\n",
      "Epoch 230/500 Loss: 0.007686876691877842\n",
      "Epoch 231/500 Loss: 0.007657590322196484\n",
      "Epoch 232/500 Loss: 0.0076354932971298695\n",
      "Epoch 233/500 Loss: 0.007618497125804424\n",
      "Epoch 234/500 Loss: 0.007604178506880999\n",
      "Epoch 235/500 Loss: 0.007592312525957823\n",
      "Epoch 236/500 Loss: 0.007583121303468943\n",
      "Epoch 237/500 Loss: 0.007574369665235281\n",
      "Epoch 238/500 Loss: 0.0075645106844604015\n",
      "Epoch 239/500 Loss: 0.007544766180217266\n",
      "Epoch 240/500 Loss: 0.007519932463765144\n",
      "Epoch 241/500 Loss: 0.007498124614357948\n",
      "Epoch 242/500 Loss: 0.007486589252948761\n",
      "Epoch 243/500 Loss: 0.007480670232325792\n",
      "Epoch 244/500 Loss: 0.007470764219760895\n",
      "Epoch 245/500 Loss: 0.007453502155840397\n",
      "Epoch 246/500 Loss: 0.007433054968714714\n",
      "Epoch 247/500 Loss: 0.007418051827698946\n",
      "Epoch 248/500 Loss: 0.007409240584820509\n",
      "Epoch 249/500 Loss: 0.007400941103696823\n",
      "Epoch 250/500 Loss: 0.007388317491859198\n",
      "Epoch 251/500 Loss: 0.007371647749096155\n",
      "Epoch 252/500 Loss: 0.007356228772550821\n",
      "Epoch 253/500 Loss: 0.0073448424227535725\n",
      "Epoch 254/500 Loss: 0.007336037699133158\n",
      "Epoch 255/500 Loss: 0.0073269568383693695\n",
      "Epoch 256/500 Loss: 0.0073154219426214695\n",
      "Epoch 257/500 Loss: 0.0073022181168198586\n",
      "Epoch 258/500 Loss: 0.007288623135536909\n",
      "Epoch 259/500 Loss: 0.007276414893567562\n",
      "Epoch 260/500 Loss: 0.007265920285135508\n",
      "Epoch 261/500 Loss: 0.007256616372615099\n",
      "Epoch 262/500 Loss: 0.007247859612107277\n",
      "Epoch 263/500 Loss: 0.0072390008717775345\n",
      "Epoch 264/500 Loss: 0.00722994701936841\n",
      "Epoch 265/500 Loss: 0.00722031993791461\n",
      "Epoch 266/500 Loss: 0.007210377138108015\n",
      "Epoch 267/500 Loss: 0.007199857383966446\n",
      "Epoch 268/500 Loss: 0.007189072202891111\n",
      "Epoch 269/500 Loss: 0.007177951745688915\n",
      "Epoch 270/500 Loss: 0.007166933733969927\n",
      "Epoch 271/500 Loss: 0.007155903149396181\n",
      "Epoch 272/500 Loss: 0.007145218085497618\n",
      "Epoch 273/500 Loss: 0.007136223372071981\n",
      "Epoch 274/500 Loss: 0.0071276770904660225\n",
      "Epoch 275/500 Loss: 0.007119282148778439\n",
      "Epoch 276/500 Loss: 0.007111367769539356\n",
      "Epoch 277/500 Loss: 0.007104178890585899\n",
      "Epoch 278/500 Loss: 0.00709797628223896\n",
      "Epoch 279/500 Loss: 0.007093219086527824\n",
      "Epoch 280/500 Loss: 0.007091526873409748\n",
      "Epoch 281/500 Loss: 0.0070950621739029884\n",
      "Epoch 282/500 Loss: 0.007106667384505272\n",
      "Epoch 283/500 Loss: 0.007122684270143509\n",
      "Epoch 284/500 Loss: 0.007127971854060888\n",
      "Epoch 285/500 Loss: 0.007092119660228491\n",
      "Epoch 286/500 Loss: 0.007047444581985474\n",
      "Epoch 287/500 Loss: 0.007036116905510426\n",
      "Epoch 288/500 Loss: 0.007053200621157885\n",
      "Epoch 289/500 Loss: 0.007052335422486067\n",
      "Epoch 290/500 Loss: 0.00702388072386384\n",
      "Epoch 291/500 Loss: 0.007009583059698343\n",
      "Epoch 292/500 Loss: 0.007016473449766636\n",
      "Epoch 293/500 Loss: 0.007014882750809193\n",
      "Epoch 294/500 Loss: 0.00699724443256855\n",
      "Epoch 295/500 Loss: 0.006983891595155001\n",
      "Epoch 296/500 Loss: 0.006984814070165157\n",
      "Epoch 297/500 Loss: 0.006983262486755848\n",
      "Epoch 298/500 Loss: 0.006969701964408159\n",
      "Epoch 299/500 Loss: 0.006957342382520437\n",
      "Epoch 300/500 Loss: 0.006953790783882141\n",
      "Epoch 301/500 Loss: 0.006951057352125645\n",
      "Epoch 302/500 Loss: 0.0069417804479599\n",
      "Epoch 303/500 Loss: 0.006930234842002392\n",
      "Epoch 304/500 Loss: 0.006923675537109375\n",
      "Epoch 305/500 Loss: 0.006920116487890482\n",
      "Epoch 306/500 Loss: 0.006913515739142895\n",
      "Epoch 307/500 Loss: 0.006903717759996653\n",
      "Epoch 308/500 Loss: 0.00689483480527997\n",
      "Epoch 309/500 Loss: 0.006889088079333305\n",
      "Epoch 310/500 Loss: 0.006884204689413309\n",
      "Epoch 311/500 Loss: 0.0068773203529417515\n",
      "Epoch 312/500 Loss: 0.006868846248835325\n",
      "Epoch 313/500 Loss: 0.006860921625047922\n",
      "Epoch 314/500 Loss: 0.006854773964732885\n",
      "Epoch 315/500 Loss: 0.006849540397524834\n",
      "Epoch 316/500 Loss: 0.006843759212642908\n",
      "Epoch 317/500 Loss: 0.006837047636508942\n",
      "Epoch 318/500 Loss: 0.0068298871628940105\n",
      "Epoch 319/500 Loss: 0.006823163013905287\n",
      "Epoch 320/500 Loss: 0.006817174144089222\n",
      "Epoch 321/500 Loss: 0.0068117002956569195\n",
      "Epoch 322/500 Loss: 0.006806273013353348\n",
      "Epoch 323/500 Loss: 0.006800319068133831\n",
      "Epoch 324/500 Loss: 0.006793801672756672\n",
      "Epoch 325/500 Loss: 0.00678676413372159\n",
      "Epoch 326/500 Loss: 0.006779552437365055\n",
      "Epoch 327/500 Loss: 0.00677220756188035\n",
      "Epoch 328/500 Loss: 0.006764886435121298\n",
      "Epoch 329/500 Loss: 0.006757514551281929\n",
      "Epoch 330/500 Loss: 0.006750079337507486\n",
      "Epoch 331/500 Loss: 0.006742498837411404\n",
      "Epoch 332/500 Loss: 0.0067347693257033825\n",
      "Epoch 333/500 Loss: 0.006726871244609356\n",
      "Epoch 334/500 Loss: 0.006718819495290518\n",
      "Epoch 335/500 Loss: 0.006710619665682316\n",
      "Epoch 336/500 Loss: 0.006702301558107138\n",
      "Epoch 337/500 Loss: 0.006693839095532894\n",
      "Epoch 338/500 Loss: 0.006685182917863131\n",
      "Epoch 339/500 Loss: 0.006676428951323032\n",
      "Epoch 340/500 Loss: 0.006667659152299166\n",
      "Epoch 341/500 Loss: 0.006659296806901693\n",
      "Epoch 342/500 Loss: 0.006652454845607281\n",
      "Epoch 343/500 Loss: 0.006650899071246386\n",
      "Epoch 344/500 Loss: 0.0066688465885818005\n",
      "Epoch 345/500 Loss: 0.006745229475200176\n",
      "Epoch 346/500 Loss: 0.006953830365091562\n",
      "Epoch 347/500 Loss: 0.007160001900047064\n",
      "Epoch 348/500 Loss: 0.006782688200473785\n",
      "Epoch 349/500 Loss: 0.0066153667867183685\n",
      "Epoch 350/500 Loss: 0.006855189800262451\n",
      "Epoch 351/500 Loss: 0.006705119740217924\n",
      "Epoch 352/500 Loss: 0.006593005266040564\n",
      "Epoch 353/500 Loss: 0.00673298817127943\n",
      "Epoch 354/500 Loss: 0.006613262463361025\n",
      "Epoch 355/500 Loss: 0.006592643912881613\n",
      "Epoch 356/500 Loss: 0.006661663763225079\n",
      "Epoch 357/500 Loss: 0.006560016889125109\n",
      "Epoch 358/500 Loss: 0.006593548692762852\n",
      "Epoch 359/500 Loss: 0.00660469476133585\n",
      "Epoch 360/500 Loss: 0.006535171531140804\n",
      "Epoch 361/500 Loss: 0.006584227085113525\n",
      "Epoch 362/500 Loss: 0.006557026877999306\n",
      "Epoch 363/500 Loss: 0.0065232617780566216\n",
      "Epoch 364/500 Loss: 0.006562342401593924\n",
      "Epoch 365/500 Loss: 0.006518791429698467\n",
      "Epoch 366/500 Loss: 0.006513300817459822\n",
      "Epoch 367/500 Loss: 0.00653016846626997\n",
      "Epoch 368/500 Loss: 0.006490499246865511\n",
      "Epoch 369/500 Loss: 0.006498199887573719\n",
      "Epoch 370/500 Loss: 0.006495294161140919\n",
      "Epoch 371/500 Loss: 0.006469469051808119\n",
      "Epoch 372/500 Loss: 0.006477599497884512\n",
      "Epoch 373/500 Loss: 0.006464786361902952\n",
      "Epoch 374/500 Loss: 0.006451474502682686\n",
      "Epoch 375/500 Loss: 0.0064545972272753716\n",
      "Epoch 376/500 Loss: 0.006439589895308018\n",
      "Epoch 377/500 Loss: 0.006433113943785429\n",
      "Epoch 378/500 Loss: 0.0064307451248168945\n",
      "Epoch 379/500 Loss: 0.00641705933958292\n",
      "Epoch 380/500 Loss: 0.006412997376173735\n",
      "Epoch 381/500 Loss: 0.006407064385712147\n",
      "Epoch 382/500 Loss: 0.0063958074897527695\n",
      "Epoch 383/500 Loss: 0.006392793729901314\n",
      "Epoch 384/500 Loss: 0.0063858176581561565\n",
      "Epoch 385/500 Loss: 0.006376690696924925\n",
      "Epoch 386/500 Loss: 0.0063729980029165745\n",
      "Epoch 387/500 Loss: 0.006364958360791206\n",
      "Epoch 388/500 Loss: 0.006355695892125368\n",
      "Epoch 389/500 Loss: 0.006350477691739798\n",
      "Epoch 390/500 Loss: 0.0063425227999687195\n",
      "Epoch 391/500 Loss: 0.006333831697702408\n",
      "Epoch 392/500 Loss: 0.006328163668513298\n",
      "Epoch 393/500 Loss: 0.006320509593933821\n",
      "Epoch 394/500 Loss: 0.0063111549243330956\n",
      "Epoch 395/500 Loss: 0.006303927395492792\n",
      "Epoch 396/500 Loss: 0.006296232808381319\n",
      "Epoch 397/500 Loss: 0.0062870667316019535\n",
      "Epoch 398/500 Loss: 0.0062794676050543785\n",
      "Epoch 399/500 Loss: 0.006272305268794298\n",
      "Epoch 400/500 Loss: 0.006263889838010073\n",
      "Epoch 401/500 Loss: 0.006256268359720707\n",
      "Epoch 402/500 Loss: 0.006249697413295507\n",
      "Epoch 403/500 Loss: 0.00624240655452013\n",
      "Epoch 404/500 Loss: 0.006234613712877035\n",
      "Epoch 405/500 Loss: 0.0062270695343613625\n",
      "Epoch 406/500 Loss: 0.006219291593879461\n",
      "Epoch 407/500 Loss: 0.006210373714566231\n",
      "Epoch 408/500 Loss: 0.006200908217579126\n",
      "Epoch 409/500 Loss: 0.006191649008542299\n",
      "Epoch 410/500 Loss: 0.006182573270052671\n",
      "Epoch 411/500 Loss: 0.0061732325702905655\n",
      "Epoch 412/500 Loss: 0.006163730286061764\n",
      "Epoch 413/500 Loss: 0.006154073402285576\n",
      "Epoch 414/500 Loss: 0.006144229788333178\n",
      "Epoch 415/500 Loss: 0.0061342460103333\n",
      "Epoch 416/500 Loss: 0.0061243753880262375\n",
      "Epoch 417/500 Loss: 0.006115207448601723\n",
      "Epoch 418/500 Loss: 0.006106976419687271\n",
      "Epoch 419/500 Loss: 0.00609953235834837\n",
      "Epoch 420/500 Loss: 0.006092511583119631\n",
      "Epoch 421/500 Loss: 0.006085644941776991\n",
      "Epoch 422/500 Loss: 0.006078876554965973\n",
      "Epoch 423/500 Loss: 0.006071949377655983\n",
      "Epoch 424/500 Loss: 0.006064961664378643\n",
      "Epoch 425/500 Loss: 0.00605793809518218\n",
      "Epoch 426/500 Loss: 0.006050717085599899\n",
      "Epoch 427/500 Loss: 0.006042781751602888\n",
      "Epoch 428/500 Loss: 0.006034068763256073\n",
      "Epoch 429/500 Loss: 0.006024795118719339\n",
      "Epoch 430/500 Loss: 0.006015101447701454\n",
      "Epoch 431/500 Loss: 0.006005456671118736\n",
      "Epoch 432/500 Loss: 0.005996305495500565\n",
      "Epoch 433/500 Loss: 0.0059883613139390945\n",
      "Epoch 434/500 Loss: 0.00598317151889205\n",
      "Epoch 435/500 Loss: 0.005986403673887253\n",
      "Epoch 436/500 Loss: 0.006020531058311462\n",
      "Epoch 437/500 Loss: 0.006150855217128992\n",
      "Epoch 438/500 Loss: 0.006507993210107088\n",
      "Epoch 439/500 Loss: 0.0070912037044763565\n",
      "Epoch 440/500 Loss: 0.006619811989367008\n",
      "Epoch 441/500 Loss: 0.005965548567473888\n",
      "Epoch 442/500 Loss: 0.0062175048515200615\n",
      "Epoch 443/500 Loss: 0.006402545142918825\n",
      "Epoch 444/500 Loss: 0.0060033611953258514\n",
      "Epoch 445/500 Loss: 0.006026810500770807\n",
      "Epoch 446/500 Loss: 0.006244496442377567\n",
      "Epoch 447/500 Loss: 0.005993114784359932\n",
      "Epoch 448/500 Loss: 0.0059555284678936005\n",
      "Epoch 449/500 Loss: 0.0061348192393779755\n",
      "Epoch 450/500 Loss: 0.00595898786559701\n",
      "Epoch 451/500 Loss: 0.005927545949816704\n",
      "Epoch 452/500 Loss: 0.0060547031462192535\n",
      "Epoch 453/500 Loss: 0.005916155409067869\n",
      "Epoch 454/500 Loss: 0.00590905686840415\n",
      "Epoch 455/500 Loss: 0.0059886351227760315\n",
      "Epoch 456/500 Loss: 0.00587946642190218\n",
      "Epoch 457/500 Loss: 0.005893558729439974\n",
      "Epoch 458/500 Loss: 0.005932928528636694\n",
      "Epoch 459/500 Loss: 0.005851097404956818\n",
      "Epoch 460/500 Loss: 0.005873245652765036\n",
      "Epoch 461/500 Loss: 0.0058891954831779\n",
      "Epoch 462/500 Loss: 0.005830232985317707\n",
      "Epoch 463/500 Loss: 0.005849029403179884\n",
      "Epoch 464/500 Loss: 0.005854962859302759\n",
      "Epoch 465/500 Loss: 0.005811885930597782\n",
      "Epoch 466/500 Loss: 0.005823514889925718\n",
      "Epoch 467/500 Loss: 0.005826335400342941\n",
      "Epoch 468/500 Loss: 0.005794016178697348\n",
      "Epoch 469/500 Loss: 0.005798392929136753\n",
      "Epoch 470/500 Loss: 0.005799959879368544\n",
      "Epoch 471/500 Loss: 0.005775731988251209\n",
      "Epoch 472/500 Loss: 0.005774074699729681\n",
      "Epoch 473/500 Loss: 0.005775935482233763\n",
      "Epoch 474/500 Loss: 0.005757634062319994\n",
      "Epoch 475/500 Loss: 0.005750627256929874\n",
      "Epoch 476/500 Loss: 0.005752309691160917\n",
      "Epoch 477/500 Loss: 0.00573925394564867\n",
      "Epoch 478/500 Loss: 0.005728115327656269\n",
      "Epoch 479/500 Loss: 0.005727819632738829\n",
      "Epoch 480/500 Loss: 0.005719538778066635\n",
      "Epoch 481/500 Loss: 0.005706906784325838\n",
      "Epoch 482/500 Loss: 0.005703138653188944\n",
      "Epoch 483/500 Loss: 0.005698227323591709\n",
      "Epoch 484/500 Loss: 0.005686354823410511\n",
      "Epoch 485/500 Loss: 0.005678011570125818\n",
      "Epoch 486/500 Loss: 0.005673713516443968\n",
      "Epoch 487/500 Loss: 0.0056654284708201885\n",
      "Epoch 488/500 Loss: 0.005654865875840187\n",
      "Epoch 489/500 Loss: 0.005647513549774885\n",
      "Epoch 490/500 Loss: 0.005641521420329809\n",
      "Epoch 491/500 Loss: 0.0056328242644667625\n",
      "Epoch 492/500 Loss: 0.005622838623821735\n",
      "Epoch 493/500 Loss: 0.00561483995988965\n",
      "Epoch 494/500 Loss: 0.005607869476079941\n",
      "Epoch 495/500 Loss: 0.005599341820925474\n",
      "Epoch 496/500 Loss: 0.005589643493294716\n",
      "Epoch 497/500 Loss: 0.0055805169977247715\n",
      "Epoch 498/500 Loss: 0.005572476889938116\n",
      "Epoch 499/500 Loss: 0.005564408376812935\n",
      "Epoch 500/500 Loss: 0.005555322859436274\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 500\n",
    "lr = 0.001\n",
    "\n",
    "print('training f')\n",
    "gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "PICNNtrain(model_init_f, gaussian_transport_dataloader, init_z_f, lr=lr, epochs=n_epoch)\n",
    "#PICNNtrain(model_init_f, gaussian_transport_dataloader, lr=0.0001, epochs=1, init_z = lambda x: x)\n",
    "\n",
    "print('training g')\n",
    "reversed_gaussian_dataset = MyDataset(gaussian_dataset.Y, gaussian_dataset.C, gaussian_dataset.X)\n",
    "gaussian_transport_dataset_reversed = get_gaussian_transport_dataset(reversed_gaussian_dataset)\n",
    "gaussian_transport_dataloader_reversed = DataLoader(gaussian_transport_dataset_reversed, batch_size=250, shuffle=True)\n",
    "#PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, lr=0.0001, epochs=25, init_z = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2)\n",
    "PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, init_z_g, lr=lr, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_init_f = model_init_f.state_dict()\n",
    "state_dict_init_g = model_init_g.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dorseuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('training f')\n",
    "# gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "# train_wasserstein(model_init_f, gaussian_transport_dataloader, lr=0.1, epochs=10, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y, C = gaussian_dataset.X, gaussian_dataset.Y, gaussian_dataset.C\n",
    "# #Calcul de la dérivée du PICNN\n",
    "\n",
    "# for test in range(20):\n",
    "#     x_i = X[test, :, :]\n",
    "#     y_i = Y[test, :, :]\n",
    "#     c_i = C[test, :, :]\n",
    "\n",
    "#     locs = c_i[:,0]\n",
    "#     #print(locs)\n",
    "\n",
    "#     scales = c_i[:,1]\n",
    "#     #print(scales)  \n",
    "\n",
    "\n",
    "#     y_i.requires_grad_(True)\n",
    "#     x_i.requires_grad_(True)\n",
    "#     #c_i.requires_grad_(True)    \n",
    "\n",
    "#     output_model_f = model_init_f(x_i, c_i)\n",
    "#     grad_model_f = torch.autograd.grad(outputs=output_model_f, inputs=x_i, grad_outputs=torch.ones_like(output_model_f), create_graph=True)[0].detach().numpy()\n",
    "\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True)\n",
    "#     plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True)\n",
    "#     plt.hist(grad_model_f[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     output_model_g = model_init_g(y_i, c_i)\n",
    "#     grad_model_g = torch.autograd.grad(outputs=output_model_g, inputs=y_i, grad_outputs=torch.ones_like(output_model_g), create_graph=True)[0].detach().numpy()\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True, color = 'red')\n",
    "#     #plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True, color = 'blue')\n",
    "#     plt.hist(grad_model_g[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     #plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Makkuva__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict_init_f = torch.load('trained_models/training14/models/model_f_0.pth')\n",
    "# state_dict_init_g = torch.load('trained_models/training14/models/model_g_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "old_ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "old_ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "# Load the state dictionary into ICNNf and ICNNg\n",
    "ICNNf.load_state_dict(state_dict_init_f)\n",
    "ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "old_ICNNf.load_state_dict(state_dict_init_f)\n",
    "old_ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "l_ICNNf = [ICNNf, old_ICNNf]\n",
    "l_ICNNg = [ICNNg, old_ICNNg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 2000\n",
    "test = 21\n",
    "\n",
    "filepath_pth_f = 'trained_models/training14/models/model_f_'\n",
    "filepath_pth_g = 'trained_models/training14/models/model_g_'\n",
    "\n",
    "filepath_plt_f = 'trained_models/training14/plots/model_f_'\n",
    "filepath_plt_g = 'trained_models/training14/plots/model_g_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pth_f = filepath_pth_f + str(0) + '.pth'\n",
    "filename_pth_g = filepath_pth_g + str(0) + '.pth'\n",
    "torch.save(ICNNf.state_dict(), filename_pth_f)\n",
    "torch.save(ICNNg.state_dict(), filename_pth_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename_plt_f = filepath_plt_f + str(0) + '.png'\n",
    "filename_plt_g = filepath_plt_g + str(0) + '.png'\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, init_z_f = init_z_f, init_z_g = init_z_g, filename_f = filename_plt_f, filename_g = filename_plt_g, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "R_f 1.964739931281656e-05\n",
      "proximal_term 0.5196471214294434\n",
      "loss_g: 5.789059162139893, loss_f: 0.47906094789505005\n",
      "epoch : 2\n",
      "R_f 3.323352575534955e-05\n",
      "proximal_term 0.5196450352668762\n",
      "loss_g: 5.805776119232178, loss_f: 0.5597701072692871\n",
      "epoch : 3\n",
      "R_f 4.9505921197123826e-05\n",
      "proximal_term 0.5196444988250732\n",
      "loss_g: 5.781257152557373, loss_f: 0.6664747595787048\n",
      "epoch : 4\n",
      "R_f 7.236286182887852e-05\n",
      "proximal_term 0.5196460485458374\n",
      "loss_g: 5.6972761154174805, loss_f: 0.7026309967041016\n",
      "epoch : 5\n",
      "R_f 9.092772961594164e-05\n",
      "proximal_term 0.5196494460105896\n",
      "loss_g: 5.572621822357178, loss_f: 0.5870129466056824\n",
      "epoch : 6\n",
      "R_f 0.00010683241998776793\n",
      "proximal_term 0.5196506381034851\n",
      "loss_g: 5.521111488342285, loss_f: 0.48737409710884094\n",
      "epoch : 7\n",
      "R_f 0.00012394830991979688\n",
      "proximal_term 0.5196495652198792\n",
      "loss_g: 5.535539627075195, loss_f: 0.4607309401035309\n",
      "epoch : 8\n",
      "R_f 0.00014457738143391907\n",
      "proximal_term 0.5196478366851807\n",
      "loss_g: 5.5677032470703125, loss_f: 0.4786405563354492\n",
      "epoch : 9\n",
      "R_f 0.00016883319767657667\n",
      "proximal_term 0.5196465849876404\n",
      "loss_g: 5.591264247894287, loss_f: 0.5137151479721069\n",
      "epoch : 10\n",
      "R_f 0.00019609634182415903\n",
      "proximal_term 0.5196462273597717\n",
      "loss_g: 5.597406387329102, loss_f: 0.5452399253845215\n",
      "epoch : 11\n",
      "R_f 0.00022540526697412133\n",
      "proximal_term 0.5196464657783508\n",
      "loss_g: 5.588339805603027, loss_f: 0.5607369542121887\n",
      "epoch : 12\n",
      "R_f 0.0002550701319705695\n",
      "proximal_term 0.5196471810340881\n",
      "loss_g: 5.571761131286621, loss_f: 0.5575056672096252\n",
      "epoch : 13\n",
      "R_f 0.0002834447950590402\n",
      "proximal_term 0.5196477174758911\n",
      "loss_g: 5.556400299072266, loss_f: 0.5411086678504944\n",
      "epoch : 14\n",
      "R_f 0.00030996656278148293\n",
      "proximal_term 0.5196480751037598\n",
      "loss_g: 5.548556804656982, loss_f: 0.5207536220550537\n",
      "epoch : 15\n",
      "R_f 0.0003347542369738221\n",
      "proximal_term 0.519648015499115\n",
      "loss_g: 5.550300598144531, loss_f: 0.5048202872276306\n",
      "epoch : 16\n",
      "R_f 0.0003586792154237628\n",
      "proximal_term 0.5196475386619568\n",
      "loss_g: 5.5595550537109375, loss_f: 0.4979252219200134\n",
      "epoch : 17\n",
      "R_f 0.0003826207830570638\n",
      "proximal_term 0.519646942615509\n",
      "loss_g: 5.571852684020996, loss_f: 0.5004613995552063\n",
      "epoch : 18\n",
      "R_f 0.00040715548675507307\n",
      "proximal_term 0.519646406173706\n",
      "loss_g: 5.5824875831604, loss_f: 0.5092443227767944\n",
      "epoch : 19\n",
      "R_f 0.00043259753147140145\n",
      "proximal_term 0.5196460485458374\n",
      "loss_g: 5.5884904861450195, loss_f: 0.519620954990387\n",
      "epoch : 20\n",
      "R_f 0.0004587918519973755\n",
      "proximal_term 0.5196460485458374\n",
      "loss_g: 5.589232921600342, loss_f: 0.527289092540741\n",
      "epoch : 21\n",
      "R_f 0.0004853543359786272\n",
      "proximal_term 0.519646167755127\n",
      "loss_g: 5.58615255355835, loss_f: 0.5299726724624634\n",
      "epoch : 22\n",
      "R_f 0.0005118692060932517\n",
      "proximal_term 0.519646406173706\n",
      "loss_g: 5.581676006317139, loss_f: 0.5277815461158752\n",
      "epoch : 23\n",
      "R_f 0.0005379999056458473\n",
      "proximal_term 0.5196465849876404\n",
      "loss_g: 5.578144550323486, loss_f: 0.5224924087524414\n",
      "epoch : 24\n",
      "R_f 0.0005635644774883986\n",
      "proximal_term 0.5196467041969299\n",
      "loss_g: 5.577024936676025, loss_f: 0.5166673064231873\n",
      "epoch : 25\n",
      "R_f 0.0005886240396648645\n",
      "proximal_term 0.5196467041969299\n",
      "loss_g: 5.578564643859863, loss_f: 0.5124892592430115\n",
      "epoch : 26\n",
      "R_f 0.0006134492577984929\n",
      "proximal_term 0.5196465849876404\n",
      "loss_g: 5.581928253173828, loss_f: 0.5109555721282959\n",
      "epoch : 27\n",
      "R_f 0.0006382387946359813\n",
      "proximal_term 0.5196465253829956\n",
      "loss_g: 5.585827350616455, loss_f: 0.5119240880012512\n",
      "epoch : 28\n",
      "R_f 0.0006631683208979666\n",
      "proximal_term 0.5196464657783508\n",
      "loss_g: 5.589025974273682, loss_f: 0.5143404603004456\n",
      "epoch : 29\n",
      "R_f 0.0006883128080517054\n",
      "proximal_term 0.5196463465690613\n",
      "loss_g: 5.590871810913086, loss_f: 0.5168768763542175\n",
      "epoch : 30\n",
      "R_f 0.0007136249332688749\n",
      "proximal_term 0.5196464657783508\n",
      "loss_g: 5.591346740722656, loss_f: 0.5185193419456482\n",
      "epoch : 31\n",
      "R_f 0.0007389663951471448\n",
      "proximal_term 0.5196465253829956\n",
      "loss_g: 5.590900421142578, loss_f: 0.518746018409729\n",
      "epoch : 32\n",
      "R_f 0.0010021079797297716\n",
      "proximal_term 0.5196466445922852\n",
      "loss_g: 5.590222358703613, loss_f: 0.51791912317276\n",
      "epoch : 33\n",
      "R_f 0.0014876080676913261\n",
      "proximal_term 0.5196467638015747\n",
      "loss_g: 5.589948654174805, loss_f: 0.5166527628898621\n",
      "epoch : 34\n",
      "R_f 0.001970796613022685\n",
      "proximal_term 0.5196468234062195\n",
      "loss_g: 5.590383529663086, loss_f: 0.5153273940086365\n",
      "epoch : 35\n",
      "R_f 0.0024523762986063957\n",
      "proximal_term 0.5196468830108643\n",
      "loss_g: 5.591528415679932, loss_f: 0.5145218968391418\n",
      "epoch : 36\n",
      "R_f 0.0029334998689591885\n",
      "proximal_term 0.5196470022201538\n",
      "loss_g: 5.593138694763184, loss_f: 0.5145267844200134\n",
      "epoch : 37\n",
      "R_f 0.003415255807340145\n",
      "proximal_term 0.5196470618247986\n",
      "loss_g: 5.594821453094482, loss_f: 0.5152259469032288\n",
      "epoch : 38\n",
      "R_f 0.003898231778293848\n",
      "proximal_term 0.5196471214294434\n",
      "loss_g: 5.596234321594238, loss_f: 0.5161707401275635\n",
      "epoch : 39\n",
      "R_f 0.004382588900625706\n",
      "proximal_term 0.5196472406387329\n",
      "loss_g: 5.597288608551025, loss_f: 0.5171650052070618\n",
      "epoch : 40\n",
      "R_f 0.004868054762482643\n",
      "proximal_term 0.5196473598480225\n",
      "loss_g: 5.597954273223877, loss_f: 0.5178537368774414\n",
      "epoch : 41\n",
      "R_f 0.0053541287779808044\n",
      "proximal_term 0.519647479057312\n",
      "loss_g: 5.598394393920898, loss_f: 0.5180957913398743\n",
      "epoch : 42\n",
      "R_f 0.005840380676090717\n",
      "proximal_term 0.5196476578712463\n",
      "loss_g: 5.598827838897705, loss_f: 0.5179923176765442\n",
      "epoch : 43\n",
      "R_f 0.006326654925942421\n",
      "proximal_term 0.5196478366851807\n",
      "loss_g: 5.599412441253662, loss_f: 0.5177527666091919\n",
      "epoch : 44\n",
      "R_f 0.006813040003180504\n",
      "proximal_term 0.5196479558944702\n",
      "loss_g: 5.600213527679443, loss_f: 0.5175285339355469\n",
      "epoch : 45\n",
      "R_f 0.007299875374883413\n",
      "proximal_term 0.5196481943130493\n",
      "loss_g: 5.601223468780518, loss_f: 0.5175098776817322\n",
      "epoch : 46\n",
      "R_f 0.007787536829710007\n",
      "proximal_term 0.5196483731269836\n",
      "loss_g: 5.602328300476074, loss_f: 0.5176981091499329\n",
      "epoch : 47\n",
      "R_f 0.008276390843093395\n",
      "proximal_term 0.5196484923362732\n",
      "loss_g: 5.6034345626831055, loss_f: 0.5180835723876953\n",
      "epoch : 48\n",
      "R_f 0.008766589686274529\n",
      "proximal_term 0.5196487307548523\n",
      "loss_g: 5.604458332061768, loss_f: 0.5185379385948181\n",
      "epoch : 49\n",
      "R_f 0.00925819668918848\n",
      "proximal_term 0.5196489095687866\n",
      "loss_g: 5.605360984802246, loss_f: 0.5189182758331299\n",
      "epoch : 50\n",
      "R_f 0.009751144796609879\n",
      "proximal_term 0.519649088382721\n",
      "loss_g: 5.606184482574463, loss_f: 0.5192452073097229\n",
      "epoch : 51\n",
      "R_f 0.010245311073958874\n",
      "proximal_term 0.5196493864059448\n",
      "loss_g: 5.606961250305176, loss_f: 0.5194300413131714\n",
      "epoch : 52\n",
      "R_f 0.010740620084106922\n",
      "proximal_term 0.5196495652198792\n",
      "loss_g: 5.607759475708008, loss_f: 0.5195114612579346\n",
      "epoch : 53\n",
      "R_f 0.01123707927763462\n",
      "proximal_term 0.519649863243103\n",
      "loss_g: 5.608630180358887, loss_f: 0.5195919871330261\n",
      "epoch : 54\n",
      "R_f 0.011734841391444206\n",
      "proximal_term 0.5196501016616821\n",
      "loss_g: 5.609575271606445, loss_f: 0.5197022557258606\n",
      "epoch : 55\n",
      "R_f 0.012234002351760864\n",
      "proximal_term 0.5196503400802612\n",
      "loss_g: 5.610589981079102, loss_f: 0.519895613193512\n",
      "epoch : 56\n",
      "R_f 0.01273476704955101\n",
      "proximal_term 0.5196506381034851\n",
      "loss_g: 5.611633777618408, loss_f: 0.5201514959335327\n",
      "epoch : 57\n",
      "R_f 0.013237135484814644\n",
      "proximal_term 0.5196508765220642\n",
      "loss_g: 5.612669467926025, loss_f: 0.5204088687896729\n",
      "epoch : 58\n",
      "R_f 0.013741171918809414\n",
      "proximal_term 0.5196512937545776\n",
      "loss_g: 5.61370325088501, loss_f: 0.5207088589668274\n",
      "epoch : 59\n",
      "R_f 0.014246885664761066\n",
      "proximal_term 0.5196515321731567\n",
      "loss_g: 5.614724159240723, loss_f: 0.5209862589836121\n",
      "epoch : 60\n",
      "R_f 0.0147542804479599\n",
      "proximal_term 0.5196518898010254\n",
      "loss_g: 5.615737438201904, loss_f: 0.5212735533714294\n",
      "epoch : 61\n",
      "R_f 0.015263240784406662\n",
      "proximal_term 0.5196521878242493\n",
      "loss_g: 5.616732120513916, loss_f: 0.5214632153511047\n",
      "epoch : 62\n",
      "R_f 0.01577378809452057\n",
      "proximal_term 0.5196525454521179\n",
      "loss_g: 5.617767333984375, loss_f: 0.5216803550720215\n",
      "epoch : 63\n",
      "R_f 0.01628587394952774\n",
      "proximal_term 0.5196528434753418\n",
      "loss_g: 5.618826866149902, loss_f: 0.5218704342842102\n",
      "epoch : 64\n",
      "R_f 0.01679946295917034\n",
      "proximal_term 0.5196532607078552\n",
      "loss_g: 5.6199212074279785, loss_f: 0.5220499038696289\n",
      "epoch : 65\n",
      "R_f 0.017314573749899864\n",
      "proximal_term 0.5196535587310791\n",
      "loss_g: 5.621054172515869, loss_f: 0.5222639441490173\n",
      "epoch : 66\n",
      "R_f 0.017836255952715874\n",
      "proximal_term 0.5196539759635925\n",
      "loss_g: 5.6222124099731445, loss_f: 0.5224781036376953\n",
      "epoch : 67\n",
      "R_f 0.018378423526883125\n",
      "proximal_term 0.5196545124053955\n",
      "loss_g: 5.623401641845703, loss_f: 0.5227488279342651\n",
      "epoch : 68\n",
      "R_f 0.018940741196274757\n",
      "proximal_term 0.5196548700332642\n",
      "loss_g: 5.624605178833008, loss_f: 0.5230628252029419\n",
      "epoch : 69\n",
      "R_f 0.019520025700330734\n",
      "proximal_term 0.5196552276611328\n",
      "loss_g: 5.6258039474487305, loss_f: 0.523362398147583\n",
      "epoch : 70\n",
      "R_f 0.020113470032811165\n",
      "proximal_term 0.5196555852890015\n",
      "loss_g: 5.627025604248047, loss_f: 0.5236214995384216\n",
      "epoch : 71\n",
      "R_f 0.020718256011605263\n",
      "proximal_term 0.5196559429168701\n",
      "loss_g: 5.628284931182861, loss_f: 0.5238767266273499\n",
      "epoch : 72\n",
      "R_f 0.02133200503885746\n",
      "proximal_term 0.5196564197540283\n",
      "loss_g: 5.629597187042236, loss_f: 0.5242087244987488\n",
      "epoch : 73\n",
      "R_f 0.021952591836452484\n",
      "proximal_term 0.5196567177772522\n",
      "loss_g: 5.63092041015625, loss_f: 0.5245534181594849\n",
      "epoch : 74\n",
      "R_f 0.022578883916139603\n",
      "proximal_term 0.5196571946144104\n",
      "loss_g: 5.6322503089904785, loss_f: 0.5248552560806274\n",
      "epoch : 75\n",
      "R_f 0.02320938929915428\n",
      "proximal_term 0.5196576118469238\n",
      "loss_g: 5.633626937866211, loss_f: 0.5252116322517395\n",
      "epoch : 76\n",
      "R_f 0.02384343557059765\n",
      "proximal_term 0.5196580290794373\n",
      "loss_g: 5.6350250244140625, loss_f: 0.5255977511405945\n",
      "epoch : 77\n",
      "R_f 0.024480381980538368\n",
      "proximal_term 0.5196585059165955\n",
      "loss_g: 5.636420726776123, loss_f: 0.5259537696838379\n",
      "epoch : 78\n",
      "R_f 0.025119619444012642\n",
      "proximal_term 0.5196589231491089\n",
      "loss_g: 5.63783073425293, loss_f: 0.5262346267700195\n",
      "epoch : 79\n",
      "R_f 0.025760838761925697\n",
      "proximal_term 0.5196593403816223\n",
      "loss_g: 5.639292240142822, loss_f: 0.5264801979064941\n",
      "epoch : 80\n",
      "R_f 0.026403700932860374\n",
      "proximal_term 0.5196598172187805\n",
      "loss_g: 5.640820503234863, loss_f: 0.5267310738563538\n",
      "epoch : 81\n",
      "R_f 0.027047932147979736\n",
      "proximal_term 0.5196601748466492\n",
      "loss_g: 5.642411708831787, loss_f: 0.5270006656646729\n",
      "epoch : 82\n",
      "R_f 0.027694182470440865\n",
      "proximal_term 0.5196605920791626\n",
      "loss_g: 5.64405632019043, loss_f: 0.5272727012634277\n",
      "epoch : 83\n",
      "R_f 0.028341641649603844\n",
      "proximal_term 0.5196610689163208\n",
      "loss_g: 5.6457624435424805, loss_f: 0.5276833772659302\n",
      "epoch : 84\n",
      "R_f 0.028990766033530235\n",
      "proximal_term 0.5196614861488342\n",
      "loss_g: 5.647441387176514, loss_f: 0.5280086994171143\n",
      "epoch : 85\n",
      "R_f 0.029641326516866684\n",
      "proximal_term 0.5196619629859924\n",
      "loss_g: 5.649145603179932, loss_f: 0.5283547043800354\n",
      "epoch : 86\n",
      "R_f 0.030293146148324013\n",
      "proximal_term 0.5196623802185059\n",
      "loss_g: 5.650864124298096, loss_f: 0.5287032127380371\n",
      "epoch : 87\n",
      "R_f 0.030946213752031326\n",
      "proximal_term 0.5196627974510193\n",
      "loss_g: 5.652598857879639, loss_f: 0.528992235660553\n",
      "epoch : 88\n",
      "R_f 0.031600307673215866\n",
      "proximal_term 0.5196633338928223\n",
      "loss_g: 5.654382228851318, loss_f: 0.5292220711708069\n",
      "epoch : 89\n",
      "R_f 0.03225535899400711\n",
      "proximal_term 0.5196638107299805\n",
      "loss_g: 5.65623140335083, loss_f: 0.5294509530067444\n",
      "epoch : 90\n",
      "R_f 0.03291149064898491\n",
      "proximal_term 0.5196643471717834\n",
      "loss_g: 5.658149242401123, loss_f: 0.5296863317489624\n",
      "epoch : 91\n",
      "R_f 0.03356825187802315\n",
      "proximal_term 0.5196648240089417\n",
      "loss_g: 5.660131931304932, loss_f: 0.5299434661865234\n",
      "epoch : 92\n",
      "R_f 0.03422576189041138\n",
      "proximal_term 0.5196653008460999\n",
      "loss_g: 5.662178039550781, loss_f: 0.5302801132202148\n",
      "epoch : 93\n",
      "R_f 0.034884363412857056\n",
      "proximal_term 0.5196658968925476\n",
      "loss_g: 5.66423225402832, loss_f: 0.5306276082992554\n",
      "epoch : 94\n",
      "R_f 0.03554527834057808\n",
      "proximal_term 0.5196664333343506\n",
      "loss_g: 5.6663031578063965, loss_f: 0.5308594107627869\n",
      "epoch : 95\n",
      "R_f 0.03620731830596924\n",
      "proximal_term 0.5196669101715088\n",
      "loss_g: 5.668437957763672, loss_f: 0.5311551690101624\n",
      "epoch : 96\n",
      "R_f 0.03687002882361412\n",
      "proximal_term 0.519667387008667\n",
      "loss_g: 5.670600891113281, loss_f: 0.5314928293228149\n",
      "epoch : 97\n",
      "R_f 0.03753350302577019\n",
      "proximal_term 0.51966792345047\n",
      "loss_g: 5.672780513763428, loss_f: 0.5318653583526611\n",
      "epoch : 98\n",
      "R_f 0.038198042660951614\n",
      "proximal_term 0.519668459892273\n",
      "loss_g: 5.674966335296631, loss_f: 0.5322607755661011\n",
      "epoch : 99\n",
      "R_f 0.038863543421030045\n",
      "proximal_term 0.5196690559387207\n",
      "loss_g: 5.677135467529297, loss_f: 0.5325872302055359\n",
      "epoch : 100\n",
      "R_f 0.03952968120574951\n",
      "proximal_term 0.5196695923805237\n",
      "loss_g: 5.679327964782715, loss_f: 0.5329593420028687\n",
      "epoch : 101\n",
      "R_f 0.040196795016527176\n",
      "proximal_term 0.5196701288223267\n",
      "loss_g: 5.681524753570557, loss_f: 0.5332476496696472\n",
      "epoch : 102\n",
      "R_f 0.040864791721105576\n",
      "proximal_term 0.5196706652641296\n",
      "loss_g: 5.683773517608643, loss_f: 0.5334547758102417\n",
      "epoch : 103\n",
      "R_f 0.04153357446193695\n",
      "proximal_term 0.5196712017059326\n",
      "loss_g: 5.686098575592041, loss_f: 0.5336887836456299\n",
      "epoch : 104\n",
      "R_f 0.04220343753695488\n",
      "proximal_term 0.5196717381477356\n",
      "loss_g: 5.688498020172119, loss_f: 0.5339751839637756\n",
      "epoch : 105\n",
      "R_f 0.042873628437519073\n",
      "proximal_term 0.5196720957756042\n",
      "loss_g: 5.690939903259277, loss_f: 0.5343217253684998\n",
      "epoch : 106\n",
      "R_f 0.04354412108659744\n",
      "proximal_term 0.519672691822052\n",
      "loss_g: 5.6934003829956055, loss_f: 0.5347553491592407\n",
      "epoch : 107\n",
      "R_f 0.0442153736948967\n",
      "proximal_term 0.519673228263855\n",
      "loss_g: 5.695846080780029, loss_f: 0.5352422595024109\n",
      "epoch : 108\n",
      "R_f 0.04488672316074371\n",
      "proximal_term 0.5196738839149475\n",
      "loss_g: 5.698246955871582, loss_f: 0.5355738997459412\n",
      "epoch : 109\n",
      "R_f 0.04555809125304222\n",
      "proximal_term 0.5196744203567505\n",
      "loss_g: 5.700675010681152, loss_f: 0.5360013246536255\n",
      "epoch : 110\n",
      "R_f 0.04622938483953476\n",
      "proximal_term 0.5196749567985535\n",
      "loss_g: 5.703091144561768, loss_f: 0.536445140838623\n",
      "epoch : 111\n",
      "R_f 0.04690052568912506\n",
      "proximal_term 0.519675612449646\n",
      "loss_g: 5.705488204956055, loss_f: 0.5367964506149292\n",
      "epoch : 112\n",
      "R_f 0.047571245580911636\n",
      "proximal_term 0.5196762681007385\n",
      "loss_g: 5.707921504974365, loss_f: 0.5370728969573975\n",
      "epoch : 113\n",
      "R_f 0.04824155941605568\n",
      "proximal_term 0.519676923751831\n",
      "loss_g: 5.710413932800293, loss_f: 0.5373187065124512\n",
      "epoch : 114\n",
      "R_f 0.048911094665527344\n",
      "proximal_term 0.5196774005889893\n",
      "loss_g: 5.712987899780273, loss_f: 0.5376967191696167\n",
      "epoch : 115\n",
      "R_f 0.04958100989460945\n",
      "proximal_term 0.519677996635437\n",
      "loss_g: 5.715569496154785, loss_f: 0.5380128622055054\n",
      "epoch : 116\n",
      "R_f 0.05025072395801544\n",
      "proximal_term 0.5196785926818848\n",
      "loss_g: 5.718203067779541, loss_f: 0.5383338332176208\n",
      "epoch : 117\n",
      "R_f 0.05092030018568039\n",
      "proximal_term 0.519679069519043\n",
      "loss_g: 5.720884323120117, loss_f: 0.5386122465133667\n",
      "epoch : 118\n",
      "R_f 0.05158970132470131\n",
      "proximal_term 0.519679605960846\n",
      "loss_g: 5.723623275756836, loss_f: 0.5388739109039307\n",
      "epoch : 119\n",
      "R_f 0.05225834622979164\n",
      "proximal_term 0.5196802020072937\n",
      "loss_g: 5.726408958435059, loss_f: 0.5391966104507446\n",
      "epoch : 120\n",
      "R_f 0.05292608588933945\n",
      "proximal_term 0.5196807980537415\n",
      "loss_g: 5.729236602783203, loss_f: 0.5396921038627625\n",
      "epoch : 121\n",
      "R_f 0.05359329655766487\n",
      "proximal_term 0.5196813941001892\n",
      "loss_g: 5.732006549835205, loss_f: 0.5402345657348633\n",
      "epoch : 122\n",
      "R_f 0.05426010116934776\n",
      "proximal_term 0.519681990146637\n",
      "loss_g: 5.734713077545166, loss_f: 0.5406002998352051\n",
      "epoch : 123\n",
      "R_f 0.05492711067199707\n",
      "proximal_term 0.5196825861930847\n",
      "loss_g: 5.737438201904297, loss_f: 0.5407331585884094\n",
      "epoch : 124\n",
      "R_f 0.0555945560336113\n",
      "proximal_term 0.5196832418441772\n",
      "loss_g: 5.740303993225098, loss_f: 0.5409263372421265\n",
      "epoch : 125\n",
      "R_f 0.05626298859715462\n",
      "proximal_term 0.519683837890625\n",
      "loss_g: 5.743273735046387, loss_f: 0.5410665273666382\n",
      "epoch : 126\n",
      "R_f 0.05693182349205017\n",
      "proximal_term 0.5196844339370728\n",
      "loss_g: 5.746379852294922, loss_f: 0.5413405895233154\n",
      "epoch : 127\n",
      "R_f 0.05760142579674721\n",
      "proximal_term 0.5196852684020996\n",
      "loss_g: 5.749550819396973, loss_f: 0.5417954921722412\n",
      "epoch : 128\n",
      "R_f 0.05827183648943901\n",
      "proximal_term 0.5196858644485474\n",
      "loss_g: 5.752695560455322, loss_f: 0.5424987077713013\n",
      "epoch : 129\n",
      "R_f 0.05894258990883827\n",
      "proximal_term 0.5196865200996399\n",
      "loss_g: 5.75570821762085, loss_f: 0.543186366558075\n",
      "epoch : 130\n",
      "R_f 0.05961478501558304\n",
      "proximal_term 0.5196871757507324\n",
      "loss_g: 5.758608818054199, loss_f: 0.5436772704124451\n",
      "epoch : 131\n",
      "R_f 0.060287389904260635\n",
      "proximal_term 0.5196878910064697\n",
      "loss_g: 5.761487007141113, loss_f: 0.5440804958343506\n",
      "epoch : 132\n",
      "R_f 0.0609610378742218\n",
      "proximal_term 0.5196885466575623\n",
      "loss_g: 5.7643961906433105, loss_f: 0.544284999370575\n",
      "epoch : 133\n",
      "R_f 0.061636269092559814\n",
      "proximal_term 0.5196892023086548\n",
      "loss_g: 5.76743221282959, loss_f: 0.5445579290390015\n",
      "epoch : 134\n",
      "R_f 0.062312670052051544\n",
      "proximal_term 0.5196897983551025\n",
      "loss_g: 5.770556449890137, loss_f: 0.5448576211929321\n",
      "epoch : 135\n",
      "R_f 0.06298859417438507\n",
      "proximal_term 0.5196905136108398\n",
      "loss_g: 5.773751258850098, loss_f: 0.545473039150238\n",
      "epoch : 136\n",
      "R_f 0.063663549721241\n",
      "proximal_term 0.5196911096572876\n",
      "loss_g: 5.776866436004639, loss_f: 0.5463182330131531\n",
      "epoch : 137\n",
      "R_f 0.06433837860822678\n",
      "proximal_term 0.5196917653083801\n",
      "loss_g: 5.779774188995361, loss_f: 0.5470247268676758\n",
      "epoch : 138\n",
      "R_f 0.06501422077417374\n",
      "proximal_term 0.5196924209594727\n",
      "loss_g: 5.7825446128845215, loss_f: 0.5473262667655945\n",
      "epoch : 139\n",
      "R_f 0.0656912550330162\n",
      "proximal_term 0.5196930766105652\n",
      "loss_g: 5.785398960113525, loss_f: 0.5474599599838257\n",
      "epoch : 140\n",
      "R_f 0.06636973470449448\n",
      "proximal_term 0.5196937918663025\n",
      "loss_g: 5.788423538208008, loss_f: 0.5475624799728394\n",
      "epoch : 141\n",
      "R_f 0.06704987585544586\n",
      "proximal_term 0.5196945667266846\n",
      "loss_g: 5.791619300842285, loss_f: 0.5477324724197388\n",
      "epoch : 142\n",
      "R_f 0.06773002445697784\n",
      "proximal_term 0.5196953415870667\n",
      "loss_g: 5.794949531555176, loss_f: 0.5482527017593384\n",
      "epoch : 143\n",
      "R_f 0.06841248273849487\n",
      "proximal_term 0.519696056842804\n",
      "loss_g: 5.798236846923828, loss_f: 0.5488711595535278\n",
      "epoch : 144\n",
      "R_f 0.06909677386283875\n",
      "proximal_term 0.5196967124938965\n",
      "loss_g: 5.801451683044434, loss_f: 0.549328088760376\n",
      "epoch : 145\n",
      "R_f 0.06977961957454681\n",
      "proximal_term 0.5196974277496338\n",
      "loss_g: 5.804685115814209, loss_f: 0.5501657128334045\n",
      "epoch : 146\n",
      "R_f 0.07046203315258026\n",
      "proximal_term 0.5196980834007263\n",
      "loss_g: 5.8077497482299805, loss_f: 0.5509631037712097\n",
      "epoch : 147\n",
      "R_f 0.07114657759666443\n",
      "proximal_term 0.5196987390518188\n",
      "loss_g: 5.810664176940918, loss_f: 0.5512881875038147\n",
      "epoch : 148\n",
      "R_f 0.07183893024921417\n",
      "proximal_term 0.5196994543075562\n",
      "loss_g: 5.813675880432129, loss_f: 0.5507196187973022\n",
      "epoch : 149\n",
      "R_f 0.07253825664520264\n",
      "proximal_term 0.5197002291679382\n",
      "loss_g: 5.817251205444336, loss_f: 0.5499656796455383\n",
      "epoch : 150\n",
      "R_f 0.0732443705201149\n",
      "proximal_term 0.5197008848190308\n",
      "loss_g: 5.821453094482422, loss_f: 0.5497533679008484\n",
      "epoch : 151\n",
      "R_f 0.07395517081022263\n",
      "proximal_term 0.5197016596794128\n",
      "loss_g: 5.825953006744385, loss_f: 0.5504944920539856\n",
      "epoch : 152\n",
      "R_f 0.07466816902160645\n",
      "proximal_term 0.5197023749351501\n",
      "loss_g: 5.830287933349609, loss_f: 0.5518941879272461\n",
      "epoch : 153\n",
      "R_f 0.0753813236951828\n",
      "proximal_term 0.5197031497955322\n",
      "loss_g: 5.8341145515441895, loss_f: 0.5537045001983643\n",
      "epoch : 154\n",
      "R_f 0.07609400153160095\n",
      "proximal_term 0.5197041034698486\n",
      "loss_g: 5.8372626304626465, loss_f: 0.5553819537162781\n",
      "epoch : 155\n",
      "R_f 0.07680787146091461\n",
      "proximal_term 0.5197051167488098\n",
      "loss_g: 5.839828014373779, loss_f: 0.5558980703353882\n",
      "epoch : 156\n",
      "R_f 0.07752291113138199\n",
      "proximal_term 0.5197059512138367\n",
      "loss_g: 5.842435359954834, loss_f: 0.5555781722068787\n",
      "epoch : 157\n",
      "R_f 0.07823861390352249\n",
      "proximal_term 0.5197067260742188\n",
      "loss_g: 5.845447540283203, loss_f: 0.5551525354385376\n",
      "epoch : 158\n",
      "R_f 0.07895426452159882\n",
      "proximal_term 0.5197075605392456\n",
      "loss_g: 5.848917007446289, loss_f: 0.5549288392066956\n",
      "epoch : 159\n",
      "R_f 0.07966940104961395\n",
      "proximal_term 0.5197083353996277\n",
      "loss_g: 5.852673053741455, loss_f: 0.5555212497711182\n",
      "epoch : 160\n",
      "R_f 0.0803840309381485\n",
      "proximal_term 0.5197091698646545\n",
      "loss_g: 5.8563551902771, loss_f: 0.5562148690223694\n",
      "epoch : 161\n",
      "R_f 0.08109772205352783\n",
      "proximal_term 0.5197099447250366\n",
      "loss_g: 5.859865665435791, loss_f: 0.5569700598716736\n",
      "epoch : 162\n",
      "R_f 0.08180974423885345\n",
      "proximal_term 0.5197108387947083\n",
      "loss_g: 5.863228797912598, loss_f: 0.5576224327087402\n",
      "epoch : 163\n",
      "R_f 0.08252141624689102\n",
      "proximal_term 0.5197116732597351\n",
      "loss_g: 5.866468906402588, loss_f: 0.5577781796455383\n",
      "epoch : 164\n",
      "R_f 0.08323267102241516\n",
      "proximal_term 0.519712507724762\n",
      "loss_g: 5.869848728179932, loss_f: 0.5577073097229004\n",
      "epoch : 165\n",
      "R_f 0.08394388854503632\n",
      "proximal_term 0.5197134613990784\n",
      "loss_g: 5.87346076965332, loss_f: 0.5575905442237854\n",
      "epoch : 166\n",
      "R_f 0.08465499430894852\n",
      "proximal_term 0.51971435546875\n",
      "loss_g: 5.877341270446777, loss_f: 0.557942807674408\n",
      "epoch : 167\n",
      "R_f 0.08536544442176819\n",
      "proximal_term 0.5197151899337769\n",
      "loss_g: 5.881258487701416, loss_f: 0.5586481094360352\n",
      "epoch : 168\n",
      "R_f 0.08607550710439682\n",
      "proximal_term 0.5197159647941589\n",
      "loss_g: 5.885026931762695, loss_f: 0.5594800710678101\n",
      "epoch : 169\n",
      "R_f 0.08678451180458069\n",
      "proximal_term 0.5197168588638306\n",
      "loss_g: 5.888619422912598, loss_f: 0.5600368976593018\n",
      "epoch : 170\n",
      "R_f 0.0874931588768959\n",
      "proximal_term 0.5197176933288574\n",
      "loss_g: 5.892180442810059, loss_f: 0.5602392554283142\n",
      "epoch : 171\n",
      "R_f 0.08820117264986038\n",
      "proximal_term 0.5197185277938843\n",
      "loss_g: 5.895891189575195, loss_f: 0.5604323148727417\n",
      "epoch : 172\n",
      "R_f 0.08890828490257263\n",
      "proximal_term 0.5197193026542664\n",
      "loss_g: 5.8997483253479, loss_f: 0.5606386661529541\n",
      "epoch : 173\n",
      "R_f 0.08961433172225952\n",
      "proximal_term 0.5197201371192932\n",
      "loss_g: 5.903773307800293, loss_f: 0.5610056519508362\n",
      "epoch : 174\n",
      "R_f 0.09031945466995239\n",
      "proximal_term 0.5197209715843201\n",
      "loss_g: 5.907872676849365, loss_f: 0.5614537596702576\n",
      "epoch : 175\n",
      "R_f 0.09102385491132736\n",
      "proximal_term 0.5197219848632812\n",
      "loss_g: 5.911993026733398, loss_f: 0.5620453953742981\n",
      "epoch : 176\n",
      "R_f 0.09172679483890533\n",
      "proximal_term 0.5197228193283081\n",
      "loss_g: 5.916078090667725, loss_f: 0.5626858472824097\n",
      "epoch : 177\n",
      "R_f 0.0924287736415863\n",
      "proximal_term 0.5197238326072693\n",
      "loss_g: 5.920126914978027, loss_f: 0.5633191466331482\n",
      "epoch : 178\n",
      "R_f 0.09313016384840012\n",
      "proximal_term 0.5197247862815857\n",
      "loss_g: 5.924140930175781, loss_f: 0.5636247396469116\n",
      "epoch : 179\n",
      "R_f 0.09383077919483185\n",
      "proximal_term 0.5197255611419678\n",
      "loss_g: 5.928287982940674, loss_f: 0.5637714266777039\n",
      "epoch : 180\n",
      "R_f 0.0945298820734024\n",
      "proximal_term 0.5197265148162842\n",
      "loss_g: 5.932673454284668, loss_f: 0.5641399621963501\n",
      "epoch : 181\n",
      "R_f 0.0952271893620491\n",
      "proximal_term 0.5197274088859558\n",
      "loss_g: 5.937134742736816, loss_f: 0.5648701786994934\n",
      "epoch : 182\n",
      "R_f 0.09592310339212418\n",
      "proximal_term 0.5197281837463379\n",
      "loss_g: 5.941529273986816, loss_f: 0.5658011436462402\n",
      "epoch : 183\n",
      "R_f 0.09661773592233658\n",
      "proximal_term 0.5197290778160095\n",
      "loss_g: 5.945754051208496, loss_f: 0.5662502646446228\n",
      "epoch : 184\n",
      "R_f 0.0973106399178505\n",
      "proximal_term 0.5197298526763916\n",
      "loss_g: 5.950084686279297, loss_f: 0.5665885210037231\n",
      "epoch : 185\n",
      "R_f 0.09800232946872711\n",
      "proximal_term 0.519730806350708\n",
      "loss_g: 5.954566955566406, loss_f: 0.5669389367103577\n",
      "epoch : 186\n",
      "R_f 0.09869249910116196\n",
      "proximal_term 0.5197316408157349\n",
      "loss_g: 5.959200859069824, loss_f: 0.5672767162322998\n",
      "epoch : 187\n",
      "R_f 0.09938159584999084\n",
      "proximal_term 0.5197325944900513\n",
      "loss_g: 5.963954925537109, loss_f: 0.5677293539047241\n",
      "epoch : 188\n",
      "R_f 0.100069560110569\n",
      "proximal_term 0.5197334885597229\n",
      "loss_g: 5.968808174133301, loss_f: 0.5684269070625305\n",
      "epoch : 189\n",
      "R_f 0.10075703263282776\n",
      "proximal_term 0.5197343826293945\n",
      "loss_g: 5.973628044128418, loss_f: 0.5692293643951416\n",
      "epoch : 190\n",
      "R_f 0.10144481062889099\n",
      "proximal_term 0.5197352766990662\n",
      "loss_g: 5.978377819061279, loss_f: 0.5696839690208435\n",
      "epoch : 191\n",
      "R_f 0.10213307291269302\n",
      "proximal_term 0.519736111164093\n",
      "loss_g: 5.983242988586426, loss_f: 0.5699818134307861\n",
      "epoch : 192\n",
      "R_f 0.10282372683286667\n",
      "proximal_term 0.5197371244430542\n",
      "loss_g: 5.988290786743164, loss_f: 0.5702139139175415\n",
      "epoch : 193\n",
      "R_f 0.10351694375276566\n",
      "proximal_term 0.519737958908081\n",
      "loss_g: 5.993580341339111, loss_f: 0.5705150961875916\n",
      "epoch : 194\n",
      "R_f 0.10421473532915115\n",
      "proximal_term 0.5197388529777527\n",
      "loss_g: 5.999061584472656, loss_f: 0.5711236000061035\n",
      "epoch : 195\n",
      "R_f 0.10491606593132019\n",
      "proximal_term 0.5197397470474243\n",
      "loss_g: 6.004580497741699, loss_f: 0.5718467235565186\n",
      "epoch : 196\n",
      "R_f 0.10562026500701904\n",
      "proximal_term 0.5197407007217407\n",
      "loss_g: 6.0100812911987305, loss_f: 0.5732381939888\n",
      "epoch : 197\n",
      "R_f 0.1063278391957283\n",
      "proximal_term 0.5197416543960571\n",
      "loss_g: 6.015247344970703, loss_f: 0.5744268894195557\n",
      "epoch : 198\n",
      "R_f 0.10703606903553009\n",
      "proximal_term 0.5197427868843079\n",
      "loss_g: 6.020170211791992, loss_f: 0.5754031538963318\n",
      "epoch : 199\n",
      "R_f 0.1077435240149498\n",
      "proximal_term 0.5197436809539795\n",
      "loss_g: 6.024930953979492, loss_f: 0.5762577652931213\n",
      "epoch : 200\n",
      "R_f 0.10844416171312332\n",
      "proximal_term 0.5197446346282959\n",
      "loss_g: 6.029601097106934, loss_f: 0.5777316093444824\n",
      "epoch : 201\n",
      "R_f 0.10913938283920288\n",
      "proximal_term 0.5197456479072571\n",
      "loss_g: 6.033799648284912, loss_f: 0.57879638671875\n",
      "epoch : 202\n",
      "R_f 0.10983094573020935\n",
      "proximal_term 0.5197465419769287\n",
      "loss_g: 6.037789344787598, loss_f: 0.5785375237464905\n",
      "epoch : 203\n",
      "R_f 0.11051906645298004\n",
      "proximal_term 0.5197474360466003\n",
      "loss_g: 6.042255878448486, loss_f: 0.5778981447219849\n",
      "epoch : 204\n",
      "R_f 0.11120288819074631\n",
      "proximal_term 0.5197482705116272\n",
      "loss_g: 6.047390937805176, loss_f: 0.5781573057174683\n",
      "epoch : 205\n",
      "R_f 0.11188153922557831\n",
      "proximal_term 0.5197492837905884\n",
      "loss_g: 6.052662372589111, loss_f: 0.5793966054916382\n",
      "epoch : 206\n",
      "R_f 0.11255660653114319\n",
      "proximal_term 0.5197502374649048\n",
      "loss_g: 6.057607173919678, loss_f: 0.5804600119590759\n",
      "epoch : 207\n",
      "R_f 0.1132270023226738\n",
      "proximal_term 0.5197511911392212\n",
      "loss_g: 6.062355995178223, loss_f: 0.580907940864563\n",
      "epoch : 208\n",
      "R_f 0.11389435082674026\n",
      "proximal_term 0.5197523832321167\n",
      "loss_g: 6.067173004150391, loss_f: 0.5807775855064392\n",
      "epoch : 209\n",
      "R_f 0.11455859243869781\n",
      "proximal_term 0.5197533965110779\n",
      "loss_g: 6.072415351867676, loss_f: 0.5807242393493652\n",
      "epoch : 210\n",
      "R_f 0.11521884799003601\n",
      "proximal_term 0.5197542905807495\n",
      "loss_g: 6.077995300292969, loss_f: 0.5815478563308716\n",
      "epoch : 211\n",
      "R_f 0.11587552726268768\n",
      "proximal_term 0.5197552442550659\n",
      "loss_g: 6.083458423614502, loss_f: 0.5827206969261169\n",
      "epoch : 212\n",
      "R_f 0.11652892082929611\n",
      "proximal_term 0.5197561979293823\n",
      "loss_g: 6.08862829208374, loss_f: 0.5833340883255005\n",
      "epoch : 213\n",
      "R_f 0.11717821657657623\n",
      "proximal_term 0.519757091999054\n",
      "loss_g: 6.093838214874268, loss_f: 0.5837442278862\n",
      "epoch : 214\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=501, shuffle=True)\n",
    "\n",
    "loss_f = list()\n",
    "loss_g = list()\n",
    "\n",
    "prev_param_f = [param.clone().detach() for param in ICNNf.parameters()]\n",
    "prev_param_g = [param.clone().detach() for param in ICNNg.parameters()]\n",
    "\n",
    "for epoch in range(1, 501) :\n",
    "    print('epoch :', epoch)\n",
    "    mean_loss_f, mean_loss_g, prev_param_f, prev_param_g = train_makkuva_epoch(ICNNf=ICNNf, ICNNg=ICNNg, prev_param_f=prev_param_f, prev_param_g=prev_param_g, dataloader = dataloader, init_z_f = init_z_f, init_z_g = init_z_g, lr=0.001, train_freq_g=10, train_freq_f=1, regularize_f = False, regularize_g = True, lambda_proximal=0.0001)\n",
    "    #mean_loss_f, mean_loss_g = train_makkuva_epoch(ICNNf, ICNNg, None, None, dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2, lr=0.0001, train_freq_g=10, train_freq_f=1, gaussian_transport=False)\n",
    "\n",
    "    loss_f.append(mean_loss_f)\n",
    "    loss_g.append(mean_loss_g)\n",
    "\n",
    "    filename_pth_f = filepath_pth_f + str(epoch) + '.pth'\n",
    "    filename_pth_g = filepath_pth_g + str(epoch) + '.pth'\n",
    "    torch.save(l_ICNNf[epoch%2].state_dict(), filename_pth_f)\n",
    "    torch.save(l_ICNNg[epoch%2].state_dict(), filename_pth_g)\n",
    "\n",
    "    filename_plt_f = filepath_plt_f + str(epoch) + '.png'\n",
    "    filename_plt_g = filepath_plt_g + str(epoch) + '.png'\n",
    "    plot_transport(dataset, test, ICNNf, ICNNg, init_z_f=init_z_f, init_z_g=init_z_g, filename_f = filename_plt_f, filename_g = filename_plt_g, n_points=n_points)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ele in loss_f:\n",
    "#     print(ele)\n",
    "\n",
    "# print('stop')\n",
    "\n",
    "# for ele in loss_g:\n",
    "#     print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_transport() missing 2 required positional arguments: 'init_z_f' and 'init_z_g'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[285], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m filename_plt_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models/training13/plots/model_f_test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m filename_plt_g \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models/training13/plots/model_g_test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mplot_transport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mICNNf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mICNNg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_f\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilename_plt_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_g\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilename_plt_g\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: plot_transport() missing 2 required positional arguments: 'init_z_f' and 'init_z_g'"
     ]
    }
   ],
   "source": [
    "ICNNf.load_state_dict(torch.load(filepath_pth_f + '0.pth'))\n",
    "ICNNg.load_state_dict(torch.load(filepath_pth_g + '0.pth'))\n",
    "\n",
    "filename_plt_f = 'trained_models/training14/plots/model_f_test'\n",
    "filename_plt_g = 'trained_models/training14/plots/model_g_test'\n",
    "\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, filename_f = filename_plt_f, filename_g = filename_plt_g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
