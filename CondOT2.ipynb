{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icnnet import ICNNet\n",
    "from mydataset import MyDataset, get_gaussian_dataset, get_gaussian_transport_dataset\n",
    "from toy_data_dataloader_gaussian import generate_gaussian_dataset, get_dataset, generate_dataset\n",
    "from train_picnn import PICNNtrain\n",
    "from train_wasserstein import train_wasserstein\n",
    "from train_makkuva import train_makkuva, train_makkuva_epoch\n",
    "from visualization import plot_transport\n",
    "from gaussian_transport import get_gaussian_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Generate dataset__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = get_dataset(d=2, r=100, N=500) #valou\n",
    "#dataset = generate_gaussian_dataset(d=2, r=400, N=10000) #thomas\n",
    "dataset = generate_dataset(d=2, r=1000, N=50)\n",
    "gaussian_dataset = get_gaussian_dataset(dataset)\n",
    "gaussian_transport_dataset = get_gaussian_transport_dataset(gaussian_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(batch):\n",
    "    means = torch.mean(batch, dim=1)\n",
    "    average_mean = torch.mean(means, dim=0)\n",
    "    return(average_mean)\n",
    "\n",
    "def get_covariance(batch):\n",
    "    n = batch.size(1) - 1\n",
    "    mean = torch.mean(batch, dim=1, keepdim=True)\n",
    "    batch = batch - mean  # Centering the data\n",
    "    cov = torch.matmul(batch.transpose(1, 2), batch) / n\n",
    "    return(torch.mean(cov, dim=0))\n",
    "\n",
    "mean1 = get_mean(dataset.X)\n",
    "cov1 = get_covariance(dataset.X)\n",
    "mean2 = get_mean(dataset.Y)\n",
    "cov2 = get_covariance(dataset.Y)\n",
    "\n",
    "def init_z_f(x):\n",
    "    #return (1/2) * torch.norm(x, dim=-1, keepdim=True)**2\n",
    "    return(get_gaussian_transport(u=x, cov1 = cov1, cov2 = cov2, m1=mean1, m2=mean2))\n",
    "\n",
    "def init_z_g(x) :\n",
    "    #return (1/2) * torch.norm(x, dim=-1, keepdim=True)**2\n",
    "    return(get_gaussian_transport(u=x, cov1 = cov2, cov2 = cov2, m1=mean2, m2=mean1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Initialization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __PICNN training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "layer_sizes = [input_size,64, 64, 64, 64, 64, 64, 1]\n",
    "n_layers = len(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# def get_embedding(C, c):\n",
    "#     scalar_product = torch.matmul(c.float(), C.t().float())\n",
    "#     embedding = F.softmax(scalar_product, dim=1)\n",
    "#     return(embedding)\n",
    "\n",
    "context_layer_sizes = [12] * n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init_f = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "model_init_g = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training f\n",
      "Epoch 1/100 Loss: 0.11959782242774963\n",
      "Epoch 2/100 Loss: 104.4300537109375\n",
      "Epoch 3/100 Loss: 4.114497184753418\n",
      "Epoch 4/100 Loss: 6.26355504989624\n",
      "Epoch 5/100 Loss: 22.026565551757812\n",
      "Epoch 6/100 Loss: 33.934425354003906\n",
      "Epoch 7/100 Loss: 41.351898193359375\n",
      "Epoch 8/100 Loss: 45.47114562988281\n",
      "Epoch 9/100 Loss: 47.20033645629883\n",
      "Epoch 10/100 Loss: 47.06255340576172\n",
      "Epoch 11/100 Loss: 45.302486419677734\n",
      "Epoch 12/100 Loss: 41.990604400634766\n",
      "Epoch 13/100 Loss: 37.09465408325195\n",
      "Epoch 14/100 Loss: 30.568370819091797\n",
      "Epoch 15/100 Loss: 22.4964599609375\n",
      "Epoch 16/100 Loss: 13.364995956420898\n",
      "Epoch 17/100 Loss: 4.845413684844971\n",
      "Epoch 18/100 Loss: 0.23548443615436554\n",
      "Epoch 19/100 Loss: 3.6036977767944336\n",
      "Epoch 20/100 Loss: 13.103581428527832\n",
      "Epoch 21/100 Loss: 17.63178062438965\n",
      "Epoch 22/100 Loss: 12.13708782196045\n",
      "Epoch 23/100 Loss: 4.162672519683838\n",
      "Epoch 24/100 Loss: 0.33997035026550293\n",
      "Epoch 25/100 Loss: 0.9143361449241638\n",
      "Epoch 26/100 Loss: 3.4462215900421143\n",
      "Epoch 27/100 Loss: 5.916441440582275\n",
      "Epoch 28/100 Loss: 7.338350772857666\n",
      "Epoch 29/100 Loss: 7.40334939956665\n",
      "Epoch 30/100 Loss: 6.207294940948486\n",
      "Epoch 31/100 Loss: 4.146835803985596\n",
      "Epoch 32/100 Loss: 1.9009125232696533\n",
      "Epoch 33/100 Loss: 0.3637476861476898\n",
      "Epoch 34/100 Loss: 0.31290221214294434\n",
      "Epoch 35/100 Loss: 1.7081701755523682\n",
      "Epoch 36/100 Loss: 3.234501838684082\n",
      "Epoch 37/100 Loss: 3.401745557785034\n",
      "Epoch 38/100 Loss: 2.157550573348999\n",
      "Epoch 39/100 Loss: 0.7314606308937073\n",
      "Epoch 40/100 Loss: 0.12736530601978302\n",
      "Epoch 41/100 Loss: 0.41378337144851685\n",
      "Epoch 42/100 Loss: 1.090710163116455\n",
      "Epoch 43/100 Loss: 1.6295545101165771\n",
      "Epoch 44/100 Loss: 1.7359222173690796\n",
      "Epoch 45/100 Loss: 1.390336036682129\n",
      "Epoch 46/100 Loss: 0.7971562743186951\n",
      "Epoch 47/100 Loss: 0.2767837941646576\n",
      "Epoch 48/100 Loss: 0.12885726988315582\n",
      "Epoch 49/100 Loss: 0.3957386016845703\n",
      "Epoch 50/100 Loss: 0.7867183685302734\n",
      "Epoch 51/100 Loss: 0.9200871586799622\n",
      "Epoch 52/100 Loss: 0.6921815872192383\n",
      "Epoch 53/100 Loss: 0.3345053195953369\n",
      "Epoch 54/100 Loss: 0.1321985125541687\n",
      "Epoch 55/100 Loss: 0.1723002940416336\n",
      "Epoch 56/100 Loss: 0.34627988934516907\n",
      "Epoch 57/100 Loss: 0.4897221624851227\n",
      "Epoch 58/100 Loss: 0.49895918369293213\n",
      "Epoch 59/100 Loss: 0.3779345750808716\n",
      "Epoch 60/100 Loss: 0.21669074892997742\n",
      "Epoch 61/100 Loss: 0.12464424967765808\n",
      "Epoch 62/100 Loss: 0.15206961333751678\n",
      "Epoch 63/100 Loss: 0.2512141764163971\n",
      "Epoch 64/100 Loss: 0.31909728050231934\n",
      "Epoch 65/100 Loss: 0.29332348704338074\n",
      "Epoch 66/100 Loss: 0.2043703943490982\n",
      "Epoch 67/100 Loss: 0.13212841749191284\n",
      "Epoch 68/100 Loss: 0.12569300830364227\n",
      "Epoch 69/100 Loss: 0.1703677922487259\n",
      "Epoch 70/100 Loss: 0.21550855040550232\n",
      "Epoch 71/100 Loss: 0.22142070531845093\n",
      "Epoch 72/100 Loss: 0.1862497478723526\n",
      "Epoch 73/100 Loss: 0.14066292345523834\n",
      "Epoch 74/100 Loss: 0.11976072192192078\n",
      "Epoch 75/100 Loss: 0.13406704366207123\n",
      "Epoch 76/100 Loss: 0.16268377006053925\n",
      "Epoch 77/100 Loss: 0.17504124343395233\n",
      "Epoch 78/100 Loss: 0.16020196676254272\n",
      "Epoch 79/100 Loss: 0.134059339761734\n",
      "Epoch 80/100 Loss: 0.11986478418111801\n",
      "Epoch 81/100 Loss: 0.12578731775283813\n",
      "Epoch 82/100 Loss: 0.14101892709732056\n",
      "Epoch 83/100 Loss: 0.1491186022758484\n",
      "Epoch 84/100 Loss: 0.14301422238349915\n",
      "Epoch 85/100 Loss: 0.12921826541423798\n",
      "Epoch 86/100 Loss: 0.12004198133945465\n",
      "Epoch 87/100 Loss: 0.12192010134458542\n",
      "Epoch 88/100 Loss: 0.13031716644763947\n",
      "Epoch 89/100 Loss: 0.13538998365402222\n",
      "Epoch 90/100 Loss: 0.13210910558700562\n",
      "Epoch 91/100 Loss: 0.12433671206235886\n",
      "Epoch 92/100 Loss: 0.1195550411939621\n",
      "Epoch 93/100 Loss: 0.12106668204069138\n",
      "Epoch 94/100 Loss: 0.12571662664413452\n",
      "Epoch 95/100 Loss: 0.12809203565120697\n",
      "Epoch 96/100 Loss: 0.12594041228294373\n",
      "Epoch 97/100 Loss: 0.12167935818433762\n",
      "Epoch 98/100 Loss: 0.11935447156429291\n",
      "Epoch 99/100 Loss: 0.12053167819976807\n",
      "Epoch 100/100 Loss: 0.12314829230308533\n",
      "training g\n",
      "Epoch 1/100 Loss: 0.00012834763037972152\n",
      "Epoch 2/100 Loss: 0.2374546080827713\n",
      "Epoch 3/100 Loss: 0.010738886892795563\n",
      "Epoch 4/100 Loss: 0.03073800727725029\n",
      "Epoch 5/100 Loss: 0.09479215741157532\n",
      "Epoch 6/100 Loss: 0.11113879829645157\n",
      "Epoch 7/100 Loss: 0.08725738525390625\n",
      "Epoch 8/100 Loss: 0.04898083582520485\n",
      "Epoch 9/100 Loss: 0.01738700084388256\n",
      "Epoch 10/100 Loss: 0.003848304972052574\n",
      "Epoch 11/100 Loss: 0.009449838660657406\n",
      "Epoch 12/100 Loss: 0.02532736398279667\n",
      "Epoch 13/100 Loss: 0.03878526762127876\n",
      "Epoch 14/100 Loss: 0.04167945310473442\n",
      "Epoch 15/100 Loss: 0.034259598702192307\n",
      "Epoch 16/100 Loss: 0.022190116345882416\n",
      "Epoch 17/100 Loss: 0.011566624976694584\n",
      "Epoch 18/100 Loss: 0.005929206497967243\n",
      "Epoch 19/100 Loss: 0.005736019462347031\n",
      "Epoch 20/100 Loss: 0.009255626238882542\n",
      "Epoch 21/100 Loss: 0.013879313133656979\n",
      "Epoch 22/100 Loss: 0.017297862097620964\n",
      "Epoch 23/100 Loss: 0.01821093074977398\n",
      "Epoch 24/100 Loss: 0.016513751819729805\n",
      "Epoch 25/100 Loss: 0.013045338913798332\n",
      "Epoch 26/100 Loss: 0.009104730561375618\n",
      "Epoch 27/100 Loss: 0.005953326355665922\n",
      "Epoch 28/100 Loss: 0.004420172888785601\n",
      "Epoch 29/100 Loss: 0.00468038022518158\n",
      "Epoch 30/100 Loss: 0.006266697775572538\n",
      "Epoch 31/100 Loss: 0.008141415193676949\n",
      "Epoch 32/100 Loss: 0.00876606721431017\n",
      "Epoch 33/100 Loss: 0.007621955592185259\n",
      "Epoch 34/100 Loss: 0.005496239755302668\n",
      "Epoch 35/100 Loss: 0.00352209503762424\n",
      "Epoch 36/100 Loss: 0.002402477664873004\n",
      "Epoch 37/100 Loss: 0.0021914448589086533\n",
      "Epoch 38/100 Loss: 0.0024697131011635065\n",
      "Epoch 39/100 Loss: 0.0027133116964250803\n",
      "Epoch 40/100 Loss: 0.0025487374514341354\n",
      "Epoch 41/100 Loss: 0.001983834197744727\n",
      "Epoch 42/100 Loss: 0.001520368386991322\n",
      "Epoch 43/100 Loss: 0.0017394816968590021\n",
      "Epoch 44/100 Loss: 0.0021622066851705313\n",
      "Epoch 45/100 Loss: 0.0019265224691480398\n",
      "Epoch 46/100 Loss: 0.0012660670327022672\n",
      "Epoch 47/100 Loss: 0.0007953468593768775\n",
      "Epoch 48/100 Loss: 0.0007184825371950865\n",
      "Epoch 49/100 Loss: 0.0008143477025441825\n",
      "Epoch 50/100 Loss: 0.0008342217770405114\n",
      "Epoch 51/100 Loss: 0.0007160334498621523\n",
      "Epoch 52/100 Loss: 0.0005661520408466458\n",
      "Epoch 53/100 Loss: 0.0005199050065129995\n",
      "Epoch 54/100 Loss: 0.0006039670552127063\n",
      "Epoch 55/100 Loss: 0.000706081569660455\n",
      "Epoch 56/100 Loss: 0.0006960672908462584\n",
      "Epoch 57/100 Loss: 0.0005798697820864618\n",
      "Epoch 58/100 Loss: 0.00047194494982250035\n",
      "Epoch 59/100 Loss: 0.0004497645713854581\n",
      "Epoch 60/100 Loss: 0.00048053203499875963\n",
      "Epoch 61/100 Loss: 0.00048073832294903696\n",
      "Epoch 62/100 Loss: 0.0004144013219047338\n",
      "Epoch 63/100 Loss: 0.00032569639733992517\n",
      "Epoch 64/100 Loss: 0.000285412825178355\n",
      "Epoch 65/100 Loss: 0.000307765876641497\n",
      "Epoch 66/100 Loss: 0.0003325569268781692\n",
      "Epoch 67/100 Loss: 0.0003081620379816741\n",
      "Epoch 68/100 Loss: 0.00026151418569497764\n",
      "Epoch 69/100 Loss: 0.0002492750936653465\n",
      "Epoch 70/100 Loss: 0.00027519933064468205\n",
      "Epoch 71/100 Loss: 0.00029343290952965617\n",
      "Epoch 72/100 Loss: 0.00027613696875050664\n",
      "Epoch 73/100 Loss: 0.00024606610531918705\n",
      "Epoch 74/100 Loss: 0.00023843732196837664\n",
      "Epoch 75/100 Loss: 0.00025022224872373044\n",
      "Epoch 76/100 Loss: 0.00024917550035752356\n",
      "Epoch 77/100 Loss: 0.00022616340720560402\n",
      "Epoch 78/100 Loss: 0.00020569453772623092\n",
      "Epoch 79/100 Loss: 0.00020463606051634997\n",
      "Epoch 80/100 Loss: 0.0002102634753100574\n",
      "Epoch 81/100 Loss: 0.00020455362391658127\n",
      "Epoch 82/100 Loss: 0.00019045613589696586\n",
      "Epoch 83/100 Loss: 0.00018387915042694658\n",
      "Epoch 84/100 Loss: 0.0001885546516859904\n",
      "Epoch 85/100 Loss: 0.00019202088878955692\n",
      "Epoch 86/100 Loss: 0.00018624156655278057\n",
      "Epoch 87/100 Loss: 0.00017857737839221954\n",
      "Epoch 88/100 Loss: 0.0001777094294084236\n",
      "Epoch 89/100 Loss: 0.0001803053164621815\n",
      "Epoch 90/100 Loss: 0.00017813133308663964\n",
      "Epoch 91/100 Loss: 0.00017117465904448181\n",
      "Epoch 92/100 Loss: 0.00016641660477034748\n",
      "Epoch 93/100 Loss: 0.00016629911260679364\n",
      "Epoch 94/100 Loss: 0.00016574212349951267\n",
      "Epoch 95/100 Loss: 0.00016139663057401776\n",
      "Epoch 96/100 Loss: 0.00015683412493672222\n",
      "Epoch 97/100 Loss: 0.00015576861915178597\n",
      "Epoch 98/100 Loss: 0.0001561229000799358\n",
      "Epoch 99/100 Loss: 0.0001543280086480081\n",
      "Epoch 100/100 Loss: 0.00015123747289180756\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "lr = 0.001\n",
    "\n",
    "print('training f')\n",
    "gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "PICNNtrain(model_init_f, gaussian_transport_dataloader, init_z_f, lr=lr, epochs=n_epoch)\n",
    "#PICNNtrain(model_init_f, gaussian_transport_dataloader, lr=0.0001, epochs=1, init_z = lambda x: x)\n",
    "\n",
    "print('training g')\n",
    "reversed_gaussian_dataset = MyDataset(gaussian_dataset.Y, gaussian_dataset.C, gaussian_dataset.X)\n",
    "gaussian_transport_dataset_reversed = get_gaussian_transport_dataset(reversed_gaussian_dataset)\n",
    "gaussian_transport_dataloader_reversed = DataLoader(gaussian_transport_dataset_reversed, batch_size=250, shuffle=True)\n",
    "#PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, lr=0.0001, epochs=25, init_z = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2)\n",
    "PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, init_z_g, lr=lr, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_init_f = model_init_f.state_dict()\n",
    "state_dict_init_g = model_init_g.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dorseuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('training f')\n",
    "# gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "# train_wasserstein(model_init_f, gaussian_transport_dataloader, lr=0.1, epochs=10, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y, C = gaussian_dataset.X, gaussian_dataset.Y, gaussian_dataset.C\n",
    "# #Calcul de la dérivée du PICNN\n",
    "\n",
    "# for test in range(20):\n",
    "#     x_i = X[test, :, :]\n",
    "#     y_i = Y[test, :, :]\n",
    "#     c_i = C[test, :, :]\n",
    "\n",
    "#     locs = c_i[:,0]\n",
    "#     #print(locs)\n",
    "\n",
    "#     scales = c_i[:,1]\n",
    "#     #print(scales)  \n",
    "\n",
    "\n",
    "#     y_i.requires_grad_(True)\n",
    "#     x_i.requires_grad_(True)\n",
    "#     #c_i.requires_grad_(True)    \n",
    "\n",
    "#     output_model_f = model_init_f(x_i, c_i)\n",
    "#     grad_model_f = torch.autograd.grad(outputs=output_model_f, inputs=x_i, grad_outputs=torch.ones_like(output_model_f), create_graph=True)[0].detach().numpy()\n",
    "\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True)\n",
    "#     plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True)\n",
    "#     plt.hist(grad_model_f[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     output_model_g = model_init_g(y_i, c_i)\n",
    "#     grad_model_g = torch.autograd.grad(outputs=output_model_g, inputs=y_i, grad_outputs=torch.ones_like(output_model_g), create_graph=True)[0].detach().numpy()\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True, color = 'red')\n",
    "#     #plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True, color = 'blue')\n",
    "#     plt.hist(grad_model_g[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     #plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Makkuva__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict_init_f = torch.load('trained_models/training16/models/model_f_0.pth')\n",
    "# state_dict_init_g = torch.load('trained_models/training16/models/model_g_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "old_ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "old_ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "# Load the state dictionary into ICNNf and ICNNg\n",
    "ICNNf.load_state_dict(state_dict_init_f)\n",
    "ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "old_ICNNf.load_state_dict(state_dict_init_f)\n",
    "old_ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "l_ICNNf = [ICNNf, old_ICNNf]\n",
    "l_ICNNg = [ICNNg, old_ICNNg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 2000\n",
    "test = 21\n",
    "\n",
    "filepath_pth_f = 'trained_models/training16/models/model_f_'\n",
    "filepath_pth_g = 'trained_models/training16/models/model_g_'\n",
    "\n",
    "filepath_plt_f = 'trained_models/training16/plots/model_f_'\n",
    "filepath_plt_g = 'trained_models/training16/plots/model_g_'\n",
    "\n",
    "import os\n",
    "os.makedirs(filepath_pth_f, exist_ok=True)\n",
    "os.makedirs(filepath_pth_g, exist_ok=True)\n",
    "\n",
    "os.makedirs(filepath_plt_f, exist_ok=True)\n",
    "os.makedirs(filepath_plt_g, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pth_f = filepath_pth_f + str(0) + '.pth'\n",
    "filename_pth_g = filepath_pth_g + str(0) + '.pth'\n",
    "torch.save(ICNNf.state_dict(), filename_pth_f)\n",
    "torch.save(ICNNg.state_dict(), filename_pth_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename_plt_f = filepath_plt_f + str(0) + '.png'\n",
    "filename_plt_g = filepath_plt_g + str(0) + '.png'\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, init_z_f = init_z_f, init_z_g = init_z_g, filename_f = filename_plt_f, filename_g = filename_plt_g, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "R_f 6.080854655010626e-05\n",
      "proximal_term 10.694985389709473\n",
      "loss_g: 20.003807067871094, loss_f: 10.744110107421875\n",
      "epoch : 2\n",
      "R_f 0.00017087519518099725\n",
      "proximal_term 10.706409454345703\n",
      "loss_g: 17.71279525756836, loss_f: 7.049393177032471\n",
      "epoch : 3\n",
      "name layers_z.0.weight\n",
      "name layers_z.1.weight\n",
      "name layers_z.2.weight\n",
      "name layers_z.3.weight\n",
      "name layers_z.4.weight\n",
      "name layers_z.5.weight\n",
      "name layers_z.6.weight\n",
      "name layers_zu.0.0.weight\n",
      "name layers_zu.0.0.bias\n",
      "name layers_zu.1.0.weight\n",
      "name layers_zu.1.0.bias\n",
      "name layers_zu.2.0.weight\n",
      "name layers_zu.2.0.bias\n",
      "name layers_zu.3.0.weight\n",
      "name layers_zu.3.0.bias\n",
      "name layers_zu.4.0.weight\n",
      "name layers_zu.4.0.bias\n",
      "name layers_zu.5.0.weight\n",
      "name layers_zu.5.0.bias\n",
      "name layers_zu.6.0.weight\n",
      "name layers_zu.6.0.bias\n",
      "name layers_x.0.weight\n",
      "name layers_x.1.weight\n",
      "name layers_x.2.weight\n",
      "name layers_x.3.weight\n",
      "name layers_x.4.weight\n",
      "name layers_x.5.weight\n",
      "name layers_x.6.weight\n",
      "name layers_xu.0.weight\n",
      "name layers_xu.0.bias\n",
      "name layers_xu.1.weight\n",
      "name layers_xu.1.bias\n",
      "name layers_xu.2.weight\n",
      "name layers_xu.2.bias\n",
      "name layers_xu.3.weight\n",
      "name layers_xu.3.bias\n",
      "name layers_xu.4.weight\n",
      "name layers_xu.4.bias\n",
      "name layers_xu.5.weight\n",
      "name layers_xu.5.bias\n",
      "name layers_xu.6.weight\n",
      "name layers_xu.6.bias\n",
      "name layers_v.0.0.weight\n",
      "name layers_v.0.0.bias\n",
      "name layers_v.1.0.weight\n",
      "name layers_v.1.0.bias\n",
      "name layers_v.2.0.weight\n",
      "name layers_v.2.0.bias\n",
      "name layers_v.3.0.weight\n",
      "name layers_v.3.0.bias\n",
      "name layers_v.4.0.weight\n",
      "name layers_v.4.0.bias\n",
      "name layers_v.5.0.weight\n",
      "name layers_v.5.0.bias\n",
      "name layers_z.0.weight\n",
      "name layers_z.1.weight\n",
      "name layers_z.2.weight\n",
      "name layers_z.3.weight\n",
      "name layers_z.4.weight\n",
      "name layers_z.5.weight\n",
      "name layers_z.6.weight\n",
      "name layers_zu.0.0.weight\n",
      "name layers_zu.0.0.bias\n",
      "name layers_zu.1.0.weight\n",
      "name layers_zu.1.0.bias\n",
      "name layers_zu.2.0.weight\n",
      "name layers_zu.2.0.bias\n",
      "name layers_zu.3.0.weight\n",
      "name layers_zu.3.0.bias\n",
      "name layers_zu.4.0.weight\n",
      "name layers_zu.4.0.bias\n",
      "name layers_zu.5.0.weight\n",
      "name layers_zu.5.0.bias\n",
      "name layers_zu.6.0.weight\n",
      "name layers_zu.6.0.bias\n",
      "name layers_x.0.weight\n",
      "name layers_x.1.weight\n",
      "name layers_x.2.weight\n",
      "name layers_x.3.weight\n",
      "name layers_x.4.weight\n",
      "name layers_x.5.weight\n",
      "name layers_x.6.weight\n",
      "name layers_xu.0.weight\n",
      "name layers_xu.0.bias\n",
      "name layers_xu.1.weight\n",
      "name layers_xu.1.bias\n",
      "name layers_xu.2.weight\n",
      "name layers_xu.2.bias\n",
      "name layers_xu.3.weight\n",
      "name layers_xu.3.bias\n",
      "name layers_xu.4.weight\n",
      "name layers_xu.4.bias\n",
      "name layers_xu.5.weight\n",
      "name layers_xu.5.bias\n",
      "name layers_xu.6.weight\n",
      "name layers_xu.6.bias\n",
      "name layers_v.0.0.weight\n",
      "name layers_v.0.0.bias\n",
      "name layers_v.1.0.weight\n",
      "name layers_v.1.0.bias\n",
      "name layers_v.2.0.weight\n",
      "name layers_v.2.0.bias\n",
      "name layers_v.3.0.weight\n",
      "name layers_v.3.0.bias\n",
      "name layers_v.4.0.weight\n",
      "name layers_v.4.0.bias\n",
      "name layers_v.5.0.weight\n",
      "name layers_v.5.0.bias\n",
      "name layers_z.0.weight\n",
      "name layers_z.1.weight\n",
      "name layers_z.2.weight\n",
      "name layers_z.3.weight\n",
      "name layers_z.4.weight\n",
      "name layers_z.5.weight\n",
      "name layers_z.6.weight\n",
      "name layers_zu.0.0.weight\n",
      "name layers_zu.0.0.bias\n",
      "name layers_zu.1.0.weight\n",
      "name layers_zu.1.0.bias\n",
      "name layers_zu.2.0.weight\n",
      "name layers_zu.2.0.bias\n",
      "name layers_zu.3.0.weight\n",
      "name layers_zu.3.0.bias\n",
      "name layers_zu.4.0.weight\n",
      "name layers_zu.4.0.bias\n",
      "name layers_zu.5.0.weight\n",
      "name layers_zu.5.0.bias\n",
      "name layers_zu.6.0.weight\n",
      "name layers_zu.6.0.bias\n",
      "name layers_x.0.weight\n",
      "name layers_x.1.weight\n",
      "name layers_x.2.weight\n",
      "name layers_x.3.weight\n",
      "name layers_x.4.weight\n",
      "name layers_x.5.weight\n",
      "name layers_x.6.weight\n",
      "name layers_xu.0.weight\n",
      "name layers_xu.0.bias\n",
      "name layers_xu.1.weight\n",
      "name layers_xu.1.bias\n",
      "name layers_xu.2.weight\n",
      "name layers_xu.2.bias\n",
      "name layers_xu.3.weight\n",
      "name layers_xu.3.bias\n",
      "name layers_xu.4.weight\n",
      "name layers_xu.4.bias\n",
      "name layers_xu.5.weight\n",
      "name layers_xu.5.bias\n",
      "name layers_xu.6.weight\n",
      "name layers_xu.6.bias\n",
      "name layers_v.0.0.weight\n",
      "name layers_v.0.0.bias\n",
      "name layers_v.1.0.weight\n",
      "name layers_v.1.0.bias\n",
      "name layers_v.2.0.weight\n",
      "name layers_v.2.0.bias\n",
      "name layers_v.3.0.weight\n",
      "name layers_v.3.0.bias\n",
      "name layers_v.4.0.weight\n",
      "name layers_v.4.0.bias\n",
      "name layers_v.5.0.weight\n",
      "name layers_v.5.0.bias\n",
      "name layers_z.0.weight\n",
      "name layers_z.1.weight\n",
      "name layers_z.2.weight\n",
      "name layers_z.3.weight\n",
      "name layers_z.4.weight\n",
      "name layers_z.5.weight\n",
      "name layers_z.6.weight\n",
      "name layers_zu.0.0.weight\n",
      "name layers_zu.0.0.bias\n",
      "name layers_zu.1.0.weight\n",
      "name layers_zu.1.0.bias\n",
      "name layers_zu.2.0.weight\n",
      "name layers_zu.2.0.bias\n",
      "name layers_zu.3.0.weight\n",
      "name layers_zu.3.0.bias\n",
      "name layers_zu.4.0.weight\n",
      "name layers_zu.4.0.bias\n",
      "name layers_zu.5.0.weight\n",
      "name layers_zu.5.0.bias\n",
      "name layers_zu.6.0.weight\n",
      "name layers_zu.6.0.bias\n",
      "name layers_x.0.weight\n",
      "name layers_x.1.weight\n",
      "name layers_x.2.weight\n",
      "name layers_x.3.weight\n",
      "name layers_x.4.weight\n",
      "name layers_x.5.weight\n",
      "name layers_x.6.weight\n",
      "name layers_xu.0.weight\n",
      "name layers_xu.0.bias\n",
      "name layers_xu.1.weight\n",
      "name layers_xu.1.bias\n",
      "name layers_xu.2.weight\n",
      "name layers_xu.2.bias\n",
      "name layers_xu.3.weight\n",
      "name layers_xu.3.bias\n",
      "name layers_xu.4.weight\n",
      "name layers_xu.4.bias\n",
      "name layers_xu.5.weight\n",
      "name layers_xu.5.bias\n",
      "name layers_xu.6.weight\n",
      "name layers_xu.6.bias\n",
      "name layers_v.0.0.weight\n",
      "name layers_v.0.0.bias\n",
      "name layers_v.1.0.weight\n",
      "name layers_v.1.0.bias\n",
      "name layers_v.2.0.weight\n",
      "name layers_v.2.0.bias\n",
      "name layers_v.3.0.weight\n",
      "name layers_v.3.0.bias\n",
      "name layers_v.4.0.weight\n",
      "name layers_v.4.0.bias\n",
      "name layers_v.5.0.weight\n",
      "name layers_v.5.0.bias\n",
      "name layers_z.0.weight\n",
      "name layers_z.1.weight\n",
      "name layers_z.2.weight\n",
      "name layers_z.3.weight\n",
      "name layers_z.4.weight\n",
      "name layers_z.5.weight\n",
      "name layers_z.6.weight\n",
      "name layers_zu.0.0.weight\n",
      "name layers_zu.0.0.bias\n",
      "name layers_zu.1.0.weight\n",
      "name layers_zu.1.0.bias\n",
      "name layers_zu.2.0.weight\n",
      "name layers_zu.2.0.bias\n",
      "name layers_zu.3.0.weight\n",
      "name layers_zu.3.0.bias\n",
      "name layers_zu.4.0.weight\n",
      "name layers_zu.4.0.bias\n",
      "name layers_zu.5.0.weight\n",
      "name layers_zu.5.0.bias\n",
      "name layers_zu.6.0.weight\n",
      "name layers_zu.6.0.bias\n",
      "name layers_x.0.weight\n",
      "name layers_x.1.weight\n",
      "name layers_x.2.weight\n",
      "name layers_x.3.weight\n",
      "name layers_x.4.weight\n",
      "name layers_x.5.weight\n",
      "name layers_x.6.weight\n",
      "name layers_xu.0.weight\n",
      "name layers_xu.0.bias\n",
      "name layers_xu.1.weight\n",
      "name layers_xu.1.bias\n",
      "name layers_xu.2.weight\n",
      "name layers_xu.2.bias\n",
      "name layers_xu.3.weight\n",
      "name layers_xu.3.bias\n",
      "name layers_xu.4.weight\n",
      "name layers_xu.4.bias\n",
      "name layers_xu.5.weight\n",
      "name layers_xu.5.bias\n",
      "name layers_xu.6.weight\n",
      "name layers_xu.6.bias\n",
      "name layers_v.0.0.weight\n",
      "name layers_v.0.0.bias\n",
      "name layers_v.1.0.weight\n",
      "name layers_v.1.0.bias\n",
      "name layers_v.2.0.weight\n",
      "name layers_v.2.0.bias\n",
      "name layers_v.3.0.weight\n",
      "name layers_v.3.0.bias\n",
      "name layers_v.4.0.weight\n",
      "name layers_v.4.0.bias\n",
      "name layers_v.5.0.weight\n",
      "name layers_v.5.0.bias\n",
      "name layers_z.0.weight\n",
      "name layers_z.1.weight\n",
      "name layers_z.2.weight\n",
      "name layers_z.3.weight\n",
      "name layers_z.4.weight\n",
      "name layers_z.5.weight\n",
      "name layers_z.6.weight\n",
      "name layers_zu.0.0.weight\n",
      "name layers_zu.0.0.bias\n",
      "name layers_zu.1.0.weight\n",
      "name layers_zu.1.0.bias\n",
      "name layers_zu.2.0.weight\n",
      "name layers_zu.2.0.bias\n",
      "name layers_zu.3.0.weight\n",
      "name layers_zu.3.0.bias\n",
      "name layers_zu.4.0.weight\n",
      "name layers_zu.4.0.bias\n",
      "name layers_zu.5.0.weight\n",
      "name layers_zu.5.0.bias\n",
      "name layers_zu.6.0.weight\n",
      "name layers_zu.6.0.bias\n",
      "name layers_x.0.weight\n",
      "name layers_x.1.weight\n",
      "name layers_x.2.weight\n",
      "name layers_x.3.weight\n",
      "name layers_x.4.weight\n",
      "name layers_x.5.weight\n",
      "name layers_x.6.weight\n",
      "name layers_xu.0.weight\n",
      "name layers_xu.0.bias\n",
      "name layers_xu.1.weight\n",
      "name layers_xu.1.bias\n",
      "name layers_xu.2.weight\n",
      "name layers_xu.2.bias\n",
      "name layers_xu.3.weight\n",
      "name layers_xu.3.bias\n",
      "name layers_xu.4.weight\n",
      "name layers_xu.4.bias\n",
      "name layers_xu.5.weight\n",
      "name layers_xu.5.bias\n",
      "name layers_xu.6.weight\n",
      "name layers_xu.6.bias\n",
      "name layers_v.0.0.weight\n",
      "name layers_v.0.0.bias\n",
      "name layers_v.1.0.weight\n",
      "name layers_v.1.0.bias\n",
      "name layers_v.2.0.weight\n",
      "name layers_v.2.0.bias\n",
      "name layers_v.3.0.weight\n",
      "name layers_v.3.0.bias\n",
      "name layers_v.4.0.weight\n",
      "name layers_v.4.0.bias\n",
      "name layers_v.5.0.weight\n",
      "name layers_v.5.0.bias\n",
      "name layers_z.0.weight\n",
      "name layers_z.1.weight\n",
      "name layers_z.2.weight\n",
      "name layers_z.3.weight\n",
      "name layers_z.4.weight\n",
      "name layers_z.5.weight\n",
      "name layers_z.6.weight\n",
      "name layers_zu.0.0.weight\n",
      "name layers_zu.0.0.bias\n",
      "name layers_zu.1.0.weight\n",
      "name layers_zu.1.0.bias\n",
      "name layers_zu.2.0.weight\n",
      "name layers_zu.2.0.bias\n",
      "name layers_zu.3.0.weight\n",
      "name layers_zu.3.0.bias\n",
      "name layers_zu.4.0.weight\n",
      "name layers_zu.4.0.bias\n",
      "name layers_zu.5.0.weight\n",
      "name layers_zu.5.0.bias\n",
      "name layers_zu.6.0.weight\n",
      "name layers_zu.6.0.bias\n",
      "name layers_x.0.weight\n",
      "name layers_x.1.weight\n",
      "name layers_x.2.weight\n",
      "name layers_x.3.weight\n",
      "name layers_x.4.weight\n",
      "name layers_x.5.weight\n",
      "name layers_x.6.weight\n",
      "name layers_xu.0.weight\n",
      "name layers_xu.0.bias\n",
      "name layers_xu.1.weight\n",
      "name layers_xu.1.bias\n",
      "name layers_xu.2.weight\n",
      "name layers_xu.2.bias\n",
      "name layers_xu.3.weight\n",
      "name layers_xu.3.bias\n",
      "name layers_xu.4.weight\n",
      "name layers_xu.4.bias\n",
      "name layers_xu.5.weight\n",
      "name layers_xu.5.bias\n",
      "name layers_xu.6.weight\n",
      "name layers_xu.6.bias\n",
      "name layers_v.0.0.weight\n",
      "name layers_v.0.0.bias\n",
      "name layers_v.1.0.weight\n",
      "name layers_v.1.0.bias\n",
      "name layers_v.2.0.weight\n",
      "name layers_v.2.0.bias\n",
      "name layers_v.3.0.weight\n",
      "name layers_v.3.0.bias\n",
      "name layers_v.4.0.weight\n",
      "name layers_v.4.0.bias\n",
      "name layers_v.5.0.weight\n",
      "name layers_v.5.0.bias\n",
      "name layers_z.0.weight\n",
      "name layers_z.1.weight\n",
      "name layers_z.2.weight\n",
      "name layers_z.3.weight\n",
      "name layers_z.4.weight\n",
      "name layers_z.5.weight\n",
      "name layers_z.6.weight\n",
      "name layers_zu.0.0.weight\n",
      "name layers_zu.0.0.bias\n",
      "name layers_zu.1.0.weight\n",
      "name layers_zu.1.0.bias\n",
      "name layers_zu.2.0.weight\n",
      "name layers_zu.2.0.bias\n",
      "name layers_zu.3.0.weight\n",
      "name layers_zu.3.0.bias\n",
      "name layers_zu.4.0.weight\n",
      "name layers_zu.4.0.bias\n",
      "name layers_zu.5.0.weight\n",
      "name layers_zu.5.0.bias\n",
      "name layers_zu.6.0.weight\n",
      "name layers_zu.6.0.bias\n",
      "name layers_x.0.weight\n",
      "name layers_x.1.weight\n",
      "name layers_x.2.weight\n",
      "name layers_x.3.weight\n",
      "name layers_x.4.weight\n",
      "name layers_x.5.weight\n",
      "name layers_x.6.weight\n",
      "name layers_xu.0.weight\n",
      "name layers_xu.0.bias\n",
      "name layers_xu.1.weight\n",
      "name layers_xu.1.bias\n",
      "name layers_xu.2.weight\n",
      "name layers_xu.2.bias\n",
      "name layers_xu.3.weight\n",
      "name layers_xu.3.bias\n",
      "name layers_xu.4.weight\n",
      "name layers_xu.4.bias\n",
      "name layers_xu.5.weight\n",
      "name layers_xu.5.bias\n",
      "name layers_xu.6.weight\n",
      "name layers_xu.6.bias\n",
      "name layers_v.0.0.weight\n",
      "name layers_v.0.0.bias\n",
      "name layers_v.1.0.weight\n",
      "name layers_v.1.0.bias\n",
      "name layers_v.2.0.weight\n",
      "name layers_v.2.0.bias\n",
      "name layers_v.3.0.weight\n",
      "name layers_v.3.0.bias\n",
      "name layers_v.4.0.weight\n",
      "name layers_v.4.0.bias\n",
      "name layers_v.5.0.weight\n",
      "name layers_v.5.0.bias\n",
      "name f  layers_z.0.weight\n",
      "name f  layers_z.1.weight\n",
      "name f  layers_z.2.weight\n",
      "name f  layers_z.3.weight\n",
      "name f  layers_z.4.weight\n",
      "name f  layers_z.5.weight\n",
      "name f  layers_z.6.weight\n",
      "name f  layers_zu.0.0.weight\n",
      "name f  layers_zu.0.0.bias\n",
      "name f  layers_zu.1.0.weight\n",
      "name f  layers_zu.1.0.bias\n",
      "name f  layers_zu.2.0.weight\n",
      "name f  layers_zu.2.0.bias\n",
      "name f  layers_zu.3.0.weight\n",
      "name f  layers_zu.3.0.bias\n",
      "name f  layers_zu.4.0.weight\n",
      "name f  layers_zu.4.0.bias\n",
      "name f  layers_zu.5.0.weight\n",
      "name f  layers_zu.5.0.bias\n",
      "name f  layers_zu.6.0.weight\n",
      "name f  layers_zu.6.0.bias\n",
      "name f  layers_x.0.weight\n",
      "name f  layers_x.1.weight\n",
      "name f  layers_x.2.weight\n",
      "name f  layers_x.3.weight\n",
      "name f  layers_x.4.weight\n",
      "name f  layers_x.5.weight\n",
      "name f  layers_x.6.weight\n",
      "name f  layers_xu.0.weight\n",
      "name f  layers_xu.0.bias\n",
      "name f  layers_xu.1.weight\n",
      "name f  layers_xu.1.bias\n",
      "name f  layers_xu.2.weight\n",
      "name f  layers_xu.2.bias\n",
      "name f  layers_xu.3.weight\n",
      "name f  layers_xu.3.bias\n",
      "name f  layers_xu.4.weight\n",
      "name f  layers_xu.4.bias\n",
      "name f  layers_xu.5.weight\n",
      "name f  layers_xu.5.bias\n",
      "name f  layers_xu.6.weight\n",
      "name f  layers_xu.6.bias\n",
      "name f  layers_v.0.0.weight\n",
      "name f  layers_v.0.0.bias\n",
      "name f  layers_v.1.0.weight\n",
      "name f  layers_v.1.0.bias\n",
      "name f  layers_v.2.0.weight\n",
      "name f  layers_v.2.0.bias\n",
      "name f  layers_v.3.0.weight\n",
      "name f  layers_v.3.0.bias\n",
      "name f  layers_v.4.0.weight\n",
      "name f  layers_v.4.0.bias\n",
      "name f  layers_v.5.0.weight\n",
      "name f  layers_v.5.0.bias\n",
      "R_f 0.00020039870287291706\n",
      "proximal_term inf\n",
      "loss_g: nan, loss_f: nan\n",
      "epoch : 4\n",
      "name layers_z.0.weight\n",
      "name layers_z.1.weight\n",
      "name layers_z.2.weight\n",
      "name layers_z.3.weight\n",
      "name layers_z.4.weight\n",
      "name layers_z.5.weight\n",
      "name layers_z.6.weight\n",
      "name layers_zu.0.0.weight\n",
      "name layers_zu.0.0.bias\n",
      "name layers_zu.1.0.weight\n",
      "name layers_zu.1.0.bias\n",
      "name layers_zu.2.0.weight\n",
      "name layers_zu.2.0.bias\n",
      "name layers_zu.3.0.weight\n",
      "name layers_zu.3.0.bias\n",
      "name layers_zu.4.0.weight\n",
      "name layers_zu.4.0.bias\n",
      "name layers_zu.5.0.weight\n",
      "name layers_zu.5.0.bias\n",
      "name layers_zu.6.0.weight\n",
      "name layers_zu.6.0.bias\n",
      "name layers_x.0.weight\n",
      "name layers_x.1.weight\n",
      "name layers_x.2.weight\n",
      "name layers_x.3.weight\n",
      "name layers_x.4.weight\n",
      "name layers_x.5.weight\n",
      "name layers_x.6.weight\n",
      "name layers_xu.0.weight\n",
      "name layers_xu.0.bias\n",
      "name layers_xu.1.weight\n",
      "name layers_xu.1.bias\n",
      "name layers_xu.2.weight\n",
      "name layers_xu.2.bias\n",
      "name layers_xu.3.weight\n",
      "name layers_xu.3.bias\n",
      "name layers_xu.4.weight\n",
      "name layers_xu.4.bias\n",
      "name layers_xu.5.weight\n",
      "name layers_xu.5.bias\n",
      "name layers_xu.6.weight\n",
      "name layers_xu.6.bias\n",
      "name layers_v.0.0.weight\n",
      "name layers_v.0.0.bias\n",
      "name layers_v.1.0.weight\n",
      "name layers_v.1.0.bias\n",
      "name layers_v.2.0.weight\n",
      "name layers_v.2.0.bias\n",
      "name layers_v.3.0.weight\n",
      "name layers_v.3.0.bias\n",
      "name layers_v.4.0.weight\n",
      "name layers_v.4.0.bias\n",
      "name layers_v.5.0.weight\n",
      "name layers_v.5.0.bias\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'param_simulated' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m501\u001b[39m) :\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch :\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch)\n\u001b[0;32m---> 11\u001b[0m     mean_loss_f, mean_loss_g, prev_param_f, prev_param_g \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_makkuva_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mICNNf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mICNNf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mICNNg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mICNNg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_param_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_param_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_param_g\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_param_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_z_f\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minit_z_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_z_g\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minit_z_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_freq_g\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_freq_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularize_f\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularize_g\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_proximal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#mean_loss_f, mean_loss_g = train_makkuva_epoch(ICNNf, ICNNg, None, None, dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2, lr=0.0001, train_freq_g=10, train_freq_f=1, gaussian_transport=False)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     loss_f\u001b[38;5;241m.\u001b[39mappend(mean_loss_f)\n",
      "File \u001b[0;32m~/Desktop/CondOT/CondOT-1/train_makkuva.py:144\u001b[0m, in \u001b[0;36mtrain_makkuva_epoch\u001b[0;34m(ICNNf, ICNNg, prev_param_f, prev_param_g, dataloader, init_z_f, init_z_g, lr, train_freq_g, train_freq_f, regularize_g, regularize_f, lambda_proximal)\u001b[0m\n\u001b[1;32m    142\u001b[0m proximal_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lambda_proximal \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m prev_param_f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sim_param, prev_param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mparam_simulated\u001b[49m, prev_param_g):\n\u001b[1;32m    145\u001b[0m             proximal_term \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (lambda_proximal \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(sim_param \u001b[38;5;241m-\u001b[39m prev_param)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m#print('proximal_term', proximal_term.item())\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'param_simulated' referenced before assignment"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=501, shuffle=True)\n",
    "\n",
    "loss_f = list()\n",
    "loss_g = list()\n",
    "\n",
    "prev_param_f = [param.clone().detach() for param in ICNNf.parameters()]\n",
    "prev_param_g = [param.clone().detach() for param in ICNNg.parameters()]\n",
    "\n",
    "for epoch in range(1, 501) :\n",
    "    print('epoch :', epoch)\n",
    "    mean_loss_f, mean_loss_g, prev_param_f, prev_param_g = train_makkuva_epoch(ICNNf=ICNNf, ICNNg=ICNNg, prev_param_f=prev_param_f, prev_param_g=prev_param_g, dataloader = dataloader, init_z_f = init_z_f, init_z_g = init_z_g, lr=0.01, train_freq_g=10, train_freq_f=1, regularize_f = True, regularize_g = True, lambda_proximal=0.001)\n",
    "    #mean_loss_f, mean_loss_g = train_makkuva_epoch(ICNNf, ICNNg, None, None, dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2, lr=0.0001, train_freq_g=10, train_freq_f=1, gaussian_transport=False)\n",
    "\n",
    "    loss_f.append(mean_loss_f)\n",
    "    loss_g.append(mean_loss_g)\n",
    "\n",
    "    filename_pth_f = filepath_pth_f + str(epoch) + '.pth'\n",
    "    filename_pth_g = filepath_pth_g + str(epoch) + '.pth'\n",
    "    torch.save(l_ICNNf[epoch%2].state_dict(), filename_pth_f)\n",
    "    torch.save(l_ICNNg[epoch%2].state_dict(), filename_pth_g)\n",
    "\n",
    "    filename_plt_f = filepath_plt_f + str(epoch) + '.png'\n",
    "    filename_plt_g = filepath_plt_g + str(epoch) + '.png'\n",
    "    plot_transport(dataset, test, ICNNf, ICNNg, init_z_f=init_z_f, init_z_g=init_z_g, filename_f = filename_plt_f, filename_g = filename_plt_g, n_points=n_points)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ele in loss_f:\n",
    "#     print(ele)\n",
    "\n",
    "# print('stop')\n",
    "\n",
    "# for ele in loss_g:\n",
    "#     print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICNNf.load_state_dict(torch.load(filepath_pth_f + '0.pth'))\n",
    "ICNNg.load_state_dict(torch.load(filepath_pth_g + '0.pth'))\n",
    "\n",
    "filename_plt_f = 'trained_models/training16/plots/model_f_test'\n",
    "filename_plt_g = 'trained_models/training16/plots/model_g_test'\n",
    "\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, filename_f = filename_plt_f, filename_g = filename_plt_g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
