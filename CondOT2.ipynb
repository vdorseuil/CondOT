{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icnnet import ICNNet\n",
    "from mydataset import MyDataset, get_gaussian_dataset, get_gaussian_transport_dataset\n",
    "from toy_data_dataloader_gaussian import generate_gaussian_dataset, get_dataset, generate_dataset\n",
    "from train_picnn import PICNNtrain\n",
    "from train_wasserstein import train_wasserstein\n",
    "from train_makkuva import train_makkuva, train_makkuva_epoch\n",
    "from visualization import plot_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Generate dataset__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dmgtr\\OneDrive - Ecole Polytechnique\\3A\\P2\\MAP588 - EA CondOT\\CondOT\\gaussian_transport.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  u = torch.tensor(u)\n"
     ]
    }
   ],
   "source": [
    "#dataset = get_dataset(d=2, r=100, N=500) #valou\n",
    "#dataset = generate_gaussian_dataset(d=2, r=400, N=10000) #thomas\n",
    "dataset = generate_dataset(d=2, r=1000, N=50)\n",
    "gaussian_dataset = get_gaussian_dataset(dataset)\n",
    "gaussian_transport_dataset = get_gaussian_transport_dataset(gaussian_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Initialization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __PICNN training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "layer_sizes = [input_size,64,64,64, 1]\n",
    "n_layers = len(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_embedding(C, c):\n",
    "    scalar_product = torch.matmul(c.float(), C.t().float())\n",
    "    embedding = F.softmax(scalar_product, dim=1)\n",
    "    return(embedding)\n",
    "\n",
    "context_layer_sizes = [12] * n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init_f = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "model_init_g = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training f\n",
      "Epoch 1/250 Loss: 0.057322945445775986\n",
      "Epoch 2/250 Loss: 0.056910786777734756\n",
      "Epoch 3/250 Loss: 0.056491900235414505\n",
      "Epoch 4/250 Loss: 0.05607828125357628\n",
      "Epoch 5/250 Loss: 0.05567314475774765\n",
      "Epoch 6/250 Loss: 0.05527341738343239\n",
      "Epoch 7/250 Loss: 0.054877109825611115\n",
      "Epoch 8/250 Loss: 0.054483067244291306\n",
      "Epoch 9/250 Loss: 0.054088570177555084\n",
      "Epoch 10/250 Loss: 0.053700536489486694\n",
      "Epoch 11/250 Loss: 0.05331666022539139\n",
      "Epoch 12/250 Loss: 0.05293552204966545\n",
      "Epoch 13/250 Loss: 0.052555982023477554\n",
      "Epoch 14/250 Loss: 0.05217800661921501\n",
      "Epoch 15/250 Loss: 0.051803626120090485\n",
      "Epoch 16/250 Loss: 0.05143744498491287\n",
      "Epoch 17/250 Loss: 0.05107295885682106\n",
      "Epoch 18/250 Loss: 0.05070872604846954\n",
      "Epoch 19/250 Loss: 0.05034996196627617\n",
      "Epoch 20/250 Loss: 0.0499957799911499\n",
      "Epoch 21/250 Loss: 0.04964367300271988\n",
      "Epoch 22/250 Loss: 0.049294330179691315\n",
      "Epoch 23/250 Loss: 0.048947401344776154\n",
      "Epoch 24/250 Loss: 0.0486033633351326\n",
      "Epoch 25/250 Loss: 0.0482613667845726\n",
      "Epoch 26/250 Loss: 0.04791969805955887\n",
      "Epoch 27/250 Loss: 0.04758261889219284\n",
      "Epoch 28/250 Loss: 0.047248370945453644\n",
      "Epoch 29/250 Loss: 0.046916406601667404\n",
      "Epoch 30/250 Loss: 0.0465860553085804\n",
      "Epoch 31/250 Loss: 0.04625731334090233\n",
      "Epoch 32/250 Loss: 0.04593159258365631\n",
      "Epoch 33/250 Loss: 0.04560708627104759\n",
      "Epoch 34/250 Loss: 0.045283906161785126\n",
      "Epoch 35/250 Loss: 0.04496372863650322\n",
      "Epoch 36/250 Loss: 0.04464507848024368\n",
      "Epoch 37/250 Loss: 0.04432634264230728\n",
      "Epoch 38/250 Loss: 0.04401054605841637\n",
      "Epoch 39/250 Loss: 0.043696243315935135\n",
      "Epoch 40/250 Loss: 0.04338477551937103\n",
      "Epoch 41/250 Loss: 0.04307368025183678\n",
      "Epoch 42/250 Loss: 0.04276353865861893\n",
      "Epoch 43/250 Loss: 0.04245590791106224\n",
      "Epoch 44/250 Loss: 0.04215031862258911\n",
      "Epoch 45/250 Loss: 0.04184604436159134\n",
      "Epoch 46/250 Loss: 0.041541825979948044\n",
      "Epoch 47/250 Loss: 0.04123925790190697\n",
      "Epoch 48/250 Loss: 0.04093870893120766\n",
      "Epoch 49/250 Loss: 0.04063946381211281\n",
      "Epoch 50/250 Loss: 0.04034212976694107\n",
      "Epoch 51/250 Loss: 0.0400448814034462\n",
      "Epoch 52/250 Loss: 0.039749957621097565\n",
      "Epoch 53/250 Loss: 0.0394560880959034\n",
      "Epoch 54/250 Loss: 0.039164356887340546\n",
      "Epoch 55/250 Loss: 0.03887474909424782\n",
      "Epoch 56/250 Loss: 0.03858648240566254\n",
      "Epoch 57/250 Loss: 0.03829874470829964\n",
      "Epoch 58/250 Loss: 0.038011666387319565\n",
      "Epoch 59/250 Loss: 0.03772677481174469\n",
      "Epoch 60/250 Loss: 0.03744320943951607\n",
      "Epoch 61/250 Loss: 0.0371597558259964\n",
      "Epoch 62/250 Loss: 0.03687857836484909\n",
      "Epoch 63/250 Loss: 0.03660067543387413\n",
      "Epoch 64/250 Loss: 0.036326732486486435\n",
      "Epoch 65/250 Loss: 0.03605356812477112\n",
      "Epoch 66/250 Loss: 0.03578091785311699\n",
      "Epoch 67/250 Loss: 0.0355108417570591\n",
      "Epoch 68/250 Loss: 0.03524230793118477\n",
      "Epoch 69/250 Loss: 0.034974370151758194\n",
      "Epoch 70/250 Loss: 0.03470630943775177\n",
      "Epoch 71/250 Loss: 0.034442655742168427\n",
      "Epoch 72/250 Loss: 0.034179940819740295\n",
      "Epoch 73/250 Loss: 0.033918123692274094\n",
      "Epoch 74/250 Loss: 0.033656954765319824\n",
      "Epoch 75/250 Loss: 0.03340036794543266\n",
      "Epoch 76/250 Loss: 0.03314552083611488\n",
      "Epoch 77/250 Loss: 0.032891832292079926\n",
      "Epoch 78/250 Loss: 0.03264050930738449\n",
      "Epoch 79/250 Loss: 0.0323888324201107\n",
      "Epoch 80/250 Loss: 0.032136425375938416\n",
      "Epoch 81/250 Loss: 0.031884852796792984\n",
      "Epoch 82/250 Loss: 0.031633757054805756\n",
      "Epoch 83/250 Loss: 0.03138849511742592\n",
      "Epoch 84/250 Loss: 0.031150223687291145\n",
      "Epoch 85/250 Loss: 0.030914194881916046\n",
      "Epoch 86/250 Loss: 0.03068104200065136\n",
      "Epoch 87/250 Loss: 0.03044882044196129\n",
      "Epoch 88/250 Loss: 0.03021952323615551\n",
      "Epoch 89/250 Loss: 0.029990840703248978\n",
      "Epoch 90/250 Loss: 0.02976328134536743\n",
      "Epoch 91/250 Loss: 0.029538627713918686\n",
      "Epoch 92/250 Loss: 0.029316242784261703\n",
      "Epoch 93/250 Loss: 0.029095802456140518\n",
      "Epoch 94/250 Loss: 0.028876682743430138\n",
      "Epoch 95/250 Loss: 0.02866014651954174\n",
      "Epoch 96/250 Loss: 0.028444379568099976\n",
      "Epoch 97/250 Loss: 0.02823205105960369\n",
      "Epoch 98/250 Loss: 0.02802187204360962\n",
      "Epoch 99/250 Loss: 0.027814341709017754\n",
      "Epoch 100/250 Loss: 0.02760874107480049\n",
      "Epoch 101/250 Loss: 0.027404872700572014\n",
      "Epoch 102/250 Loss: 0.02720250003039837\n",
      "Epoch 103/250 Loss: 0.02700110897421837\n",
      "Epoch 104/250 Loss: 0.026801394298672676\n",
      "Epoch 105/250 Loss: 0.02660571224987507\n",
      "Epoch 106/250 Loss: 0.02641092799603939\n",
      "Epoch 107/250 Loss: 0.026215894147753716\n",
      "Epoch 108/250 Loss: 0.02602429687976837\n",
      "Epoch 109/250 Loss: 0.025834262371063232\n",
      "Epoch 110/250 Loss: 0.025645142421126366\n",
      "Epoch 111/250 Loss: 0.025457600131630898\n",
      "Epoch 112/250 Loss: 0.025273118168115616\n",
      "Epoch 113/250 Loss: 0.02509027160704136\n",
      "Epoch 114/250 Loss: 0.024908345192670822\n",
      "Epoch 115/250 Loss: 0.024727486073970795\n",
      "Epoch 116/250 Loss: 0.024550415575504303\n",
      "Epoch 117/250 Loss: 0.02437463402748108\n",
      "Epoch 118/250 Loss: 0.02419976145029068\n",
      "Epoch 119/250 Loss: 0.024025702849030495\n",
      "Epoch 120/250 Loss: 0.0238528810441494\n",
      "Epoch 121/250 Loss: 0.02368171326816082\n",
      "Epoch 122/250 Loss: 0.023514648899435997\n",
      "Epoch 123/250 Loss: 0.02334894984960556\n",
      "Epoch 124/250 Loss: 0.023182015866041183\n",
      "Epoch 125/250 Loss: 0.02301882579922676\n",
      "Epoch 126/250 Loss: 0.0228560883551836\n",
      "Epoch 127/250 Loss: 0.022692959755659103\n",
      "Epoch 128/250 Loss: 0.02253088168799877\n",
      "Epoch 129/250 Loss: 0.022369980812072754\n",
      "Epoch 130/250 Loss: 0.0222090482711792\n",
      "Epoch 131/250 Loss: 0.02205091156065464\n",
      "Epoch 132/250 Loss: 0.0218922421336174\n",
      "Epoch 133/250 Loss: 0.021734682843089104\n",
      "Epoch 134/250 Loss: 0.021578440442681313\n",
      "Epoch 135/250 Loss: 0.021424029022455215\n",
      "Epoch 136/250 Loss: 0.021270405501127243\n",
      "Epoch 137/250 Loss: 0.02111697755753994\n",
      "Epoch 138/250 Loss: 0.02096610888838768\n",
      "Epoch 139/250 Loss: 0.0208175927400589\n",
      "Epoch 140/250 Loss: 0.020670227706432343\n",
      "Epoch 141/250 Loss: 0.020525161176919937\n",
      "Epoch 142/250 Loss: 0.02038257010281086\n",
      "Epoch 143/250 Loss: 0.02024068869650364\n",
      "Epoch 144/250 Loss: 0.02009960263967514\n",
      "Epoch 145/250 Loss: 0.019958550110459328\n",
      "Epoch 146/250 Loss: 0.019818967208266258\n",
      "Epoch 147/250 Loss: 0.019680358469486237\n",
      "Epoch 148/250 Loss: 0.019542695954442024\n",
      "Epoch 149/250 Loss: 0.019406644627451897\n",
      "Epoch 150/250 Loss: 0.019271206110715866\n",
      "Epoch 151/250 Loss: 0.01914028637111187\n",
      "Epoch 152/250 Loss: 0.019009027630090714\n",
      "Epoch 153/250 Loss: 0.018877925351262093\n",
      "Epoch 154/250 Loss: 0.018747583031654358\n",
      "Epoch 155/250 Loss: 0.01861903816461563\n",
      "Epoch 156/250 Loss: 0.018490955233573914\n",
      "Epoch 157/250 Loss: 0.01836279220879078\n",
      "Epoch 158/250 Loss: 0.018236925825476646\n",
      "Epoch 159/250 Loss: 0.018112080171704292\n",
      "Epoch 160/250 Loss: 0.017988502979278564\n",
      "Epoch 161/250 Loss: 0.01786685921251774\n",
      "Epoch 162/250 Loss: 0.01774594746530056\n",
      "Epoch 163/250 Loss: 0.01762527786195278\n",
      "Epoch 164/250 Loss: 0.017505628988146782\n",
      "Epoch 165/250 Loss: 0.01738605834543705\n",
      "Epoch 166/250 Loss: 0.017269594594836235\n",
      "Epoch 167/250 Loss: 0.01715460792183876\n",
      "Epoch 168/250 Loss: 0.017040204256772995\n",
      "Epoch 169/250 Loss: 0.016926253214478493\n",
      "Epoch 170/250 Loss: 0.016813848167657852\n",
      "Epoch 171/250 Loss: 0.01670227013528347\n",
      "Epoch 172/250 Loss: 0.016592612490057945\n",
      "Epoch 173/250 Loss: 0.016482515260577202\n",
      "Epoch 174/250 Loss: 0.01637456566095352\n",
      "Epoch 175/250 Loss: 0.016269128769636154\n",
      "Epoch 176/250 Loss: 0.016164088621735573\n",
      "Epoch 177/250 Loss: 0.016062159091234207\n",
      "Epoch 178/250 Loss: 0.015963299199938774\n",
      "Epoch 179/250 Loss: 0.01586468331515789\n",
      "Epoch 180/250 Loss: 0.015772441402077675\n",
      "Epoch 181/250 Loss: 0.015676850453019142\n",
      "Epoch 182/250 Loss: 0.015580379404127598\n",
      "Epoch 183/250 Loss: 0.015482308343052864\n",
      "Epoch 184/250 Loss: 0.015385670587420464\n",
      "Epoch 185/250 Loss: 0.015293820761144161\n",
      "Epoch 186/250 Loss: 0.01520538330078125\n",
      "Epoch 187/250 Loss: 0.01511518843472004\n",
      "Epoch 188/250 Loss: 0.015025397762656212\n",
      "Epoch 189/250 Loss: 0.014936622232198715\n",
      "Epoch 190/250 Loss: 0.014850317500531673\n",
      "Epoch 191/250 Loss: 0.014765973202884197\n",
      "Epoch 192/250 Loss: 0.014680534601211548\n",
      "Epoch 193/250 Loss: 0.014594241976737976\n",
      "Epoch 194/250 Loss: 0.014511815272271633\n",
      "Epoch 195/250 Loss: 0.014428606256842613\n",
      "Epoch 196/250 Loss: 0.014346356503665447\n",
      "Epoch 197/250 Loss: 0.01426732912659645\n",
      "Epoch 198/250 Loss: 0.01418793573975563\n",
      "Epoch 199/250 Loss: 0.014108450151979923\n",
      "Epoch 200/250 Loss: 0.01402897946536541\n",
      "Epoch 201/250 Loss: 0.013952193781733513\n",
      "Epoch 202/250 Loss: 0.01387530006468296\n",
      "Epoch 203/250 Loss: 0.013799183070659637\n",
      "Epoch 204/250 Loss: 0.013724387623369694\n",
      "Epoch 205/250 Loss: 0.013649874366819859\n",
      "Epoch 206/250 Loss: 0.013576033525168896\n",
      "Epoch 207/250 Loss: 0.013502121903002262\n",
      "Epoch 208/250 Loss: 0.01342956442385912\n",
      "Epoch 209/250 Loss: 0.013357361778616905\n",
      "Epoch 210/250 Loss: 0.013286862522363663\n",
      "Epoch 211/250 Loss: 0.01321705337613821\n",
      "Epoch 212/250 Loss: 0.013146842829883099\n",
      "Epoch 213/250 Loss: 0.013077792711555958\n",
      "Epoch 214/250 Loss: 0.013009236194193363\n",
      "Epoch 215/250 Loss: 0.012941506691277027\n",
      "Epoch 216/250 Loss: 0.012873872183263302\n",
      "Epoch 217/250 Loss: 0.012807812541723251\n",
      "Epoch 218/250 Loss: 0.012742077931761742\n",
      "Epoch 219/250 Loss: 0.012676656246185303\n",
      "Epoch 220/250 Loss: 0.012611117213964462\n",
      "Epoch 221/250 Loss: 0.012546049430966377\n",
      "Epoch 222/250 Loss: 0.01248156651854515\n",
      "Epoch 223/250 Loss: 0.012417665682733059\n",
      "Epoch 224/250 Loss: 0.012354530394077301\n",
      "Epoch 225/250 Loss: 0.012291893362998962\n",
      "Epoch 226/250 Loss: 0.012230230495333672\n",
      "Epoch 227/250 Loss: 0.012170109897851944\n",
      "Epoch 228/250 Loss: 0.012110551819205284\n",
      "Epoch 229/250 Loss: 0.012051374651491642\n",
      "Epoch 230/250 Loss: 0.011993585154414177\n",
      "Epoch 231/250 Loss: 0.011936289258301258\n",
      "Epoch 232/250 Loss: 0.011878542602062225\n",
      "Epoch 233/250 Loss: 0.011820359155535698\n",
      "Epoch 234/250 Loss: 0.011760739609599113\n",
      "Epoch 235/250 Loss: 0.011701248586177826\n",
      "Epoch 236/250 Loss: 0.011644188314676285\n",
      "Epoch 237/250 Loss: 0.01158927008509636\n",
      "Epoch 238/250 Loss: 0.011535415425896645\n",
      "Epoch 239/250 Loss: 0.01148141361773014\n",
      "Epoch 240/250 Loss: 0.011426900513470173\n",
      "Epoch 241/250 Loss: 0.011372484266757965\n",
      "Epoch 242/250 Loss: 0.011319073848426342\n",
      "Epoch 243/250 Loss: 0.01126624271273613\n",
      "Epoch 244/250 Loss: 0.01121531706303358\n",
      "Epoch 245/250 Loss: 0.01116457860916853\n",
      "Epoch 246/250 Loss: 0.01111391931772232\n",
      "Epoch 247/250 Loss: 0.011062919162213802\n",
      "Epoch 248/250 Loss: 0.011012124828994274\n",
      "Epoch 249/250 Loss: 0.010961986146867275\n",
      "Epoch 250/250 Loss: 0.010912717320024967\n",
      "training g\n",
      "Epoch 1/250 Loss: 0.0733400210738182\n",
      "Epoch 2/250 Loss: 0.07262112945318222\n",
      "Epoch 3/250 Loss: 0.07192372530698776\n",
      "Epoch 4/250 Loss: 0.0712335854768753\n",
      "Epoch 5/250 Loss: 0.0705544501543045\n",
      "Epoch 6/250 Loss: 0.06987641751766205\n",
      "Epoch 7/250 Loss: 0.0692075565457344\n",
      "Epoch 8/250 Loss: 0.06854072213172913\n",
      "Epoch 9/250 Loss: 0.0678805559873581\n",
      "Epoch 10/250 Loss: 0.06722553074359894\n",
      "Epoch 11/250 Loss: 0.06657715141773224\n",
      "Epoch 12/250 Loss: 0.06593503057956696\n",
      "Epoch 13/250 Loss: 0.065297432243824\n",
      "Epoch 14/250 Loss: 0.06466677039861679\n",
      "Epoch 15/250 Loss: 0.0640425980091095\n",
      "Epoch 16/250 Loss: 0.06342455744743347\n",
      "Epoch 17/250 Loss: 0.0628134161233902\n",
      "Epoch 18/250 Loss: 0.0622086226940155\n",
      "Epoch 19/250 Loss: 0.06161181628704071\n",
      "Epoch 20/250 Loss: 0.06102227047085762\n",
      "Epoch 21/250 Loss: 0.06043966859579086\n",
      "Epoch 22/250 Loss: 0.059864383190870285\n",
      "Epoch 23/250 Loss: 0.05929584801197052\n",
      "Epoch 24/250 Loss: 0.05873483791947365\n",
      "Epoch 25/250 Loss: 0.05818215385079384\n",
      "Epoch 26/250 Loss: 0.05763615667819977\n",
      "Epoch 27/250 Loss: 0.057096630334854126\n",
      "Epoch 28/250 Loss: 0.056562572717666626\n",
      "Epoch 29/250 Loss: 0.056034721434116364\n",
      "Epoch 30/250 Loss: 0.05551300197839737\n",
      "Epoch 31/250 Loss: 0.05499771982431412\n",
      "Epoch 32/250 Loss: 0.05448922887444496\n",
      "Epoch 33/250 Loss: 0.0539870411157608\n",
      "Epoch 34/250 Loss: 0.05349377542734146\n",
      "Epoch 35/250 Loss: 0.05300717428326607\n",
      "Epoch 36/250 Loss: 0.052526649087667465\n",
      "Epoch 37/250 Loss: 0.05205255746841431\n",
      "Epoch 38/250 Loss: 0.051585108041763306\n",
      "Epoch 39/250 Loss: 0.051124803721904755\n",
      "Epoch 40/250 Loss: 0.05067203938961029\n",
      "Epoch 41/250 Loss: 0.05022571235895157\n",
      "Epoch 42/250 Loss: 0.04978629946708679\n",
      "Epoch 43/250 Loss: 0.04935538023710251\n",
      "Epoch 44/250 Loss: 0.04893091693520546\n",
      "Epoch 45/250 Loss: 0.048512957990169525\n",
      "Epoch 46/250 Loss: 0.04810138791799545\n",
      "Epoch 47/250 Loss: 0.04769632965326309\n",
      "Epoch 48/250 Loss: 0.047297246754169464\n",
      "Epoch 49/250 Loss: 0.04690433666110039\n",
      "Epoch 50/250 Loss: 0.04651721194386482\n",
      "Epoch 51/250 Loss: 0.04613550752401352\n",
      "Epoch 52/250 Loss: 0.04575882479548454\n",
      "Epoch 53/250 Loss: 0.045387107878923416\n",
      "Epoch 54/250 Loss: 0.04502032324671745\n",
      "Epoch 55/250 Loss: 0.044658031314611435\n",
      "Epoch 56/250 Loss: 0.04430118203163147\n",
      "Epoch 57/250 Loss: 0.043950702995061874\n",
      "Epoch 58/250 Loss: 0.043605655431747437\n",
      "Epoch 59/250 Loss: 0.043265704065561295\n",
      "Epoch 60/250 Loss: 0.0429302342236042\n",
      "Epoch 61/250 Loss: 0.042599063366651535\n",
      "Epoch 62/250 Loss: 0.04227250814437866\n",
      "Epoch 63/250 Loss: 0.04195024445652962\n",
      "Epoch 64/250 Loss: 0.04163217172026634\n",
      "Epoch 65/250 Loss: 0.041318539530038834\n",
      "Epoch 66/250 Loss: 0.04100889340043068\n",
      "Epoch 67/250 Loss: 0.04070302098989487\n",
      "Epoch 68/250 Loss: 0.040400657802820206\n",
      "Epoch 69/250 Loss: 0.040101807564496994\n",
      "Epoch 70/250 Loss: 0.03980694338679314\n",
      "Epoch 71/250 Loss: 0.03951633721590042\n",
      "Epoch 72/250 Loss: 0.03922922536730766\n",
      "Epoch 73/250 Loss: 0.038945067673921585\n",
      "Epoch 74/250 Loss: 0.0386640764772892\n",
      "Epoch 75/250 Loss: 0.0383862629532814\n",
      "Epoch 76/250 Loss: 0.038111597299575806\n",
      "Epoch 77/250 Loss: 0.03783978149294853\n",
      "Epoch 78/250 Loss: 0.03757069259881973\n",
      "Epoch 79/250 Loss: 0.037304386496543884\n",
      "Epoch 80/250 Loss: 0.037040721625089645\n",
      "Epoch 81/250 Loss: 0.03677978366613388\n",
      "Epoch 82/250 Loss: 0.03652225062251091\n",
      "Epoch 83/250 Loss: 0.03626734018325806\n",
      "Epoch 84/250 Loss: 0.03601470962166786\n",
      "Epoch 85/250 Loss: 0.03576447442173958\n",
      "Epoch 86/250 Loss: 0.035516493022441864\n",
      "Epoch 87/250 Loss: 0.0352708138525486\n",
      "Epoch 88/250 Loss: 0.03502773120999336\n",
      "Epoch 89/250 Loss: 0.034786906093358994\n",
      "Epoch 90/250 Loss: 0.03454846516251564\n",
      "Epoch 91/250 Loss: 0.03431221470236778\n",
      "Epoch 92/250 Loss: 0.0340779572725296\n",
      "Epoch 93/250 Loss: 0.033845800906419754\n",
      "Epoch 94/250 Loss: 0.03361577168107033\n",
      "Epoch 95/250 Loss: 0.03338783606886864\n",
      "Epoch 96/250 Loss: 0.03316153213381767\n",
      "Epoch 97/250 Loss: 0.032937221229076385\n",
      "Epoch 98/250 Loss: 0.032715704292058945\n",
      "Epoch 99/250 Loss: 0.03249606490135193\n",
      "Epoch 100/250 Loss: 0.03227837011218071\n",
      "Epoch 101/250 Loss: 0.032062631100416183\n",
      "Epoch 102/250 Loss: 0.03184882178902626\n",
      "Epoch 103/250 Loss: 0.03163691982626915\n",
      "Epoch 104/250 Loss: 0.03142702579498291\n",
      "Epoch 105/250 Loss: 0.031218823045492172\n",
      "Epoch 106/250 Loss: 0.03101244382560253\n",
      "Epoch 107/250 Loss: 0.03080756776034832\n",
      "Epoch 108/250 Loss: 0.030604297295212746\n",
      "Epoch 109/250 Loss: 0.030402904376387596\n",
      "Epoch 110/250 Loss: 0.030203193426132202\n",
      "Epoch 111/250 Loss: 0.030005261301994324\n",
      "Epoch 112/250 Loss: 0.02980899251997471\n",
      "Epoch 113/250 Loss: 0.029614463448524475\n",
      "Epoch 114/250 Loss: 0.02942183054983616\n",
      "Epoch 115/250 Loss: 0.02923138253390789\n",
      "Epoch 116/250 Loss: 0.02904314361512661\n",
      "Epoch 117/250 Loss: 0.028856979683041573\n",
      "Epoch 118/250 Loss: 0.02867286652326584\n",
      "Epoch 119/250 Loss: 0.028490781784057617\n",
      "Epoch 120/250 Loss: 0.028311535716056824\n",
      "Epoch 121/250 Loss: 0.028134042397141457\n",
      "Epoch 122/250 Loss: 0.02795824408531189\n",
      "Epoch 123/250 Loss: 0.02778448536992073\n",
      "Epoch 124/250 Loss: 0.027612434700131416\n",
      "Epoch 125/250 Loss: 0.027441874146461487\n",
      "Epoch 126/250 Loss: 0.02727307565510273\n",
      "Epoch 127/250 Loss: 0.027106011286377907\n",
      "Epoch 128/250 Loss: 0.026940565556287766\n",
      "Epoch 129/250 Loss: 0.026776853948831558\n",
      "Epoch 130/250 Loss: 0.026614854112267494\n",
      "Epoch 131/250 Loss: 0.026454387232661247\n",
      "Epoch 132/250 Loss: 0.026295209303498268\n",
      "Epoch 133/250 Loss: 0.02613731287419796\n",
      "Epoch 134/250 Loss: 0.025980781763792038\n",
      "Epoch 135/250 Loss: 0.025825513526797295\n",
      "Epoch 136/250 Loss: 0.025671599432826042\n",
      "Epoch 137/250 Loss: 0.025518808513879776\n",
      "Epoch 138/250 Loss: 0.02536742016673088\n",
      "Epoch 139/250 Loss: 0.0252176932990551\n",
      "Epoch 140/250 Loss: 0.025069452822208405\n",
      "Epoch 141/250 Loss: 0.024922529235482216\n",
      "Epoch 142/250 Loss: 0.024776674807071686\n",
      "Epoch 143/250 Loss: 0.024632226675748825\n",
      "Epoch 144/250 Loss: 0.024489091709256172\n",
      "Epoch 145/250 Loss: 0.024347158148884773\n",
      "Epoch 146/250 Loss: 0.02420654520392418\n",
      "Epoch 147/250 Loss: 0.024066977202892303\n",
      "Epoch 148/250 Loss: 0.023928020149469376\n",
      "Epoch 149/250 Loss: 0.02379000559449196\n",
      "Epoch 150/250 Loss: 0.02365303784608841\n",
      "Epoch 151/250 Loss: 0.02351725660264492\n",
      "Epoch 152/250 Loss: 0.02338232658803463\n",
      "Epoch 153/250 Loss: 0.023248428478837013\n",
      "Epoch 154/250 Loss: 0.023115720599889755\n",
      "Epoch 155/250 Loss: 0.022983910515904427\n",
      "Epoch 156/250 Loss: 0.02285296842455864\n",
      "Epoch 157/250 Loss: 0.022723305970430374\n",
      "Epoch 158/250 Loss: 0.022594809532165527\n",
      "Epoch 159/250 Loss: 0.022467270493507385\n",
      "Epoch 160/250 Loss: 0.022340860217809677\n",
      "Epoch 161/250 Loss: 0.022215722128748894\n",
      "Epoch 162/250 Loss: 0.022091777995228767\n",
      "Epoch 163/250 Loss: 0.02196878381073475\n",
      "Epoch 164/250 Loss: 0.02184683084487915\n",
      "Epoch 165/250 Loss: 0.021725796163082123\n",
      "Epoch 166/250 Loss: 0.02160579338669777\n",
      "Epoch 167/250 Loss: 0.021486660465598106\n",
      "Epoch 168/250 Loss: 0.021368462592363358\n",
      "Epoch 169/250 Loss: 0.021251169964671135\n",
      "Epoch 170/250 Loss: 0.021134579554200172\n",
      "Epoch 171/250 Loss: 0.021018793806433678\n",
      "Epoch 172/250 Loss: 0.020903998985886574\n",
      "Epoch 173/250 Loss: 0.020790059119462967\n",
      "Epoch 174/250 Loss: 0.02067684568464756\n",
      "Epoch 175/250 Loss: 0.020564571022987366\n",
      "Epoch 176/250 Loss: 0.020453045144677162\n",
      "Epoch 177/250 Loss: 0.020342322066426277\n",
      "Epoch 178/250 Loss: 0.02023247629404068\n",
      "Epoch 179/250 Loss: 0.020124293863773346\n",
      "Epoch 180/250 Loss: 0.020017173141241074\n",
      "Epoch 181/250 Loss: 0.01991083472967148\n",
      "Epoch 182/250 Loss: 0.019805027171969414\n",
      "Epoch 183/250 Loss: 0.019699888303875923\n",
      "Epoch 184/250 Loss: 0.019595332443714142\n",
      "Epoch 185/250 Loss: 0.019491277635097504\n",
      "Epoch 186/250 Loss: 0.01938808150589466\n",
      "Epoch 187/250 Loss: 0.019285697489976883\n",
      "Epoch 188/250 Loss: 0.019184019416570663\n",
      "Epoch 189/250 Loss: 0.019082888960838318\n",
      "Epoch 190/250 Loss: 0.018982436507940292\n",
      "Epoch 191/250 Loss: 0.018882550299167633\n",
      "Epoch 192/250 Loss: 0.018783338367938995\n",
      "Epoch 193/250 Loss: 0.01868477091193199\n",
      "Epoch 194/250 Loss: 0.0185867790132761\n",
      "Epoch 195/250 Loss: 0.01848926581442356\n",
      "Epoch 196/250 Loss: 0.01839224062860012\n",
      "Epoch 197/250 Loss: 0.018295850604772568\n",
      "Epoch 198/250 Loss: 0.018199991434812546\n",
      "Epoch 199/250 Loss: 0.01810462586581707\n",
      "Epoch 200/250 Loss: 0.018009886145591736\n",
      "Epoch 201/250 Loss: 0.017915721982717514\n",
      "Epoch 202/250 Loss: 0.017822200432419777\n",
      "Epoch 203/250 Loss: 0.017729327082633972\n",
      "Epoch 204/250 Loss: 0.017637046054005623\n",
      "Epoch 205/250 Loss: 0.01754561997950077\n",
      "Epoch 206/250 Loss: 0.017454948276281357\n",
      "Epoch 207/250 Loss: 0.017364908009767532\n",
      "Epoch 208/250 Loss: 0.017275476828217506\n",
      "Epoch 209/250 Loss: 0.017186686396598816\n",
      "Epoch 210/250 Loss: 0.01709846220910549\n",
      "Epoch 211/250 Loss: 0.017010698094964027\n",
      "Epoch 212/250 Loss: 0.0169235672801733\n",
      "Epoch 213/250 Loss: 0.016836896538734436\n",
      "Epoch 214/250 Loss: 0.01675068773329258\n",
      "Epoch 215/250 Loss: 0.016664965078234673\n",
      "Epoch 216/250 Loss: 0.016579745337367058\n",
      "Epoch 217/250 Loss: 0.016495009884238243\n",
      "Epoch 218/250 Loss: 0.016410857439041138\n",
      "Epoch 219/250 Loss: 0.016327178105711937\n",
      "Epoch 220/250 Loss: 0.016243992373347282\n",
      "Epoch 221/250 Loss: 0.01616126298904419\n",
      "Epoch 222/250 Loss: 0.016079021617770195\n",
      "Epoch 223/250 Loss: 0.01599719747900963\n",
      "Epoch 224/250 Loss: 0.015915775671601295\n",
      "Epoch 225/250 Loss: 0.01583479531109333\n",
      "Epoch 226/250 Loss: 0.015754321590065956\n",
      "Epoch 227/250 Loss: 0.015674369409680367\n",
      "Epoch 228/250 Loss: 0.015594926662743092\n",
      "Epoch 229/250 Loss: 0.015516089275479317\n",
      "Epoch 230/250 Loss: 0.015437879599630833\n",
      "Epoch 231/250 Loss: 0.015360003337264061\n",
      "Epoch 232/250 Loss: 0.015282480046153069\n",
      "Epoch 233/250 Loss: 0.01520542148500681\n",
      "Epoch 234/250 Loss: 0.015128854662179947\n",
      "Epoch 235/250 Loss: 0.015052847564220428\n",
      "Epoch 236/250 Loss: 0.014977343380451202\n",
      "Epoch 237/250 Loss: 0.014902270399034023\n",
      "Epoch 238/250 Loss: 0.014827661216259003\n",
      "Epoch 239/250 Loss: 0.01475342083722353\n",
      "Epoch 240/250 Loss: 0.014679687097668648\n",
      "Epoch 241/250 Loss: 0.014606641605496407\n",
      "Epoch 242/250 Loss: 0.014534047804772854\n",
      "Epoch 243/250 Loss: 0.014461785554885864\n",
      "Epoch 244/250 Loss: 0.014390086755156517\n",
      "Epoch 245/250 Loss: 0.014318966306746006\n",
      "Epoch 246/250 Loss: 0.01424840185791254\n",
      "Epoch 247/250 Loss: 0.014178326353430748\n",
      "Epoch 248/250 Loss: 0.014108607545495033\n",
      "Epoch 249/250 Loss: 0.014039182104170322\n",
      "Epoch 250/250 Loss: 0.01397004909813404\n"
     ]
    }
   ],
   "source": [
    "print('training f')\n",
    "gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "PICNNtrain(model_init_f, gaussian_transport_dataloader, lr=0.0001, epochs=250, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)\n",
    "#PICNNtrain(model_init_f, gaussian_transport_dataloader, lr=0.0001, epochs=1, init_z = lambda x: x)\n",
    "\n",
    "\n",
    "print('training g')\n",
    "reversed_gaussian_dataset = MyDataset(gaussian_dataset.Y, gaussian_dataset.C, gaussian_dataset.X)\n",
    "gaussian_transport_dataset_reversed = get_gaussian_transport_dataset(reversed_gaussian_dataset)\n",
    "gaussian_transport_dataloader_reversed = DataLoader(gaussian_transport_dataset_reversed, batch_size=250, shuffle=True)\n",
    "#PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, lr=0.0001, epochs=25, init_z = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2)\n",
    "PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, lr=0.0001, epochs=250, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_init_f = model_init_f.state_dict()\n",
    "state_dict_init_g = model_init_g.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('training f')\n",
    "# gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "# train_wasserstein(model_init_f, gaussian_transport_dataloader, lr=0.1, epochs=10, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y, C = gaussian_dataset.X, gaussian_dataset.Y, gaussian_dataset.C\n",
    "# #Calcul de la dérivée du PICNN\n",
    "\n",
    "# for test in range(20):\n",
    "#     x_i = X[test, :, :]\n",
    "#     y_i = Y[test, :, :]\n",
    "#     c_i = C[test, :, :]\n",
    "\n",
    "#     locs = c_i[:,0]\n",
    "#     #print(locs)\n",
    "\n",
    "#     scales = c_i[:,1]\n",
    "#     #print(scales)  \n",
    "\n",
    "\n",
    "#     y_i.requires_grad_(True)\n",
    "#     x_i.requires_grad_(True)\n",
    "#     #c_i.requires_grad_(True)    \n",
    "\n",
    "#     output_model_f = model_init_f(x_i, c_i)\n",
    "#     grad_model_f = torch.autograd.grad(outputs=output_model_f, inputs=x_i, grad_outputs=torch.ones_like(output_model_f), create_graph=True)[0].detach().numpy()\n",
    "\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True)\n",
    "#     plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True)\n",
    "#     plt.hist(grad_model_f[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     output_model_g = model_init_g(y_i, c_i)\n",
    "#     grad_model_g = torch.autograd.grad(outputs=output_model_g, inputs=y_i, grad_outputs=torch.ones_like(output_model_g), create_graph=True)[0].detach().numpy()\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True, color = 'red')\n",
    "#     #plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True, color = 'blue')\n",
    "#     plt.hist(grad_model_g[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     #plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Makkuva__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict_init_f = torch.load('trained_models/training9/models/model_f_0.pth')\n",
    "# state_dict_init_g = torch.load('trained_models/training9/models/model_g_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "old_ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "old_ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "# Load the state dictionary into ICNNf and ICNNg\n",
    "ICNNf.load_state_dict(state_dict_init_f)\n",
    "ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "old_ICNNf.load_state_dict(state_dict_init_f)\n",
    "old_ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "l_ICNNf = [ICNNf, old_ICNNf]\n",
    "l_ICNNg = [ICNNg, old_ICNNg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 2000\n",
    "test = 16\n",
    "\n",
    "filepath_pth_f = 'trained_models/training9/models/model_f_'\n",
    "filepath_pth_g = 'trained_models/training9/models/model_g_'\n",
    "\n",
    "filepath_plt_f = 'trained_models/training9/plots/model_f_'\n",
    "filepath_plt_g = 'trained_models/training9/plots/model_g_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pth_f = filepath_pth_f + str(0) + '.pth'\n",
    "filename_pth_g = filepath_pth_g + str(0) + '.pth'\n",
    "torch.save(ICNNf.state_dict(), filename_pth_f)\n",
    "torch.save(ICNNg.state_dict(), filename_pth_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename_plt_f = filepath_plt_f + str(0) + '.png'\n",
    "filename_plt_g = filepath_plt_g + str(0) + '.png'\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, filename_f = filename_plt_f, filename_g = filename_plt_g, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_g: 0.7187504172325134, loss_f: -0.12365482747554779\n",
      "loss_g: 0.7187504172325134, loss_f: -0.12365474551916122\n",
      "loss_g: 0.9674621820449829, loss_f: 0.3908659815788269\n",
      "loss_g: 0.9674623012542725, loss_f: 0.3908659517765045\n",
      "loss_g: 0.6865757703781128, loss_f: -0.22369220852851868\n",
      "loss_g: 0.6865753531455994, loss_f: -0.22369146347045898\n",
      "loss_g: 1.0721560716629028, loss_f: -0.006209456827491522\n",
      "loss_g: 1.072157621383667, loss_f: -0.006239877548068762\n",
      "loss_g: 0.9000243544578552, loss_f: -0.07690823078155518\n",
      "loss_g: 0.9001421332359314, loss_f: -0.07452258467674255\n",
      "loss_g: 1.0317668914794922, loss_f: 0.3227865695953369\n",
      "loss_g: 1.0298584699630737, loss_f: 0.3249962627887726\n",
      "loss_g: 0.677795946598053, loss_f: -0.25361230969429016\n",
      "loss_g: 0.6760386824607849, loss_f: -0.2901286780834198\n",
      "loss_g: 0.9672154784202576, loss_f: 0.14428408443927765\n",
      "loss_g: 0.9963656067848206, loss_f: 0.05697343870997429\n",
      "loss_g: 0.7823339104652405, loss_f: 0.0631287470459938\n",
      "loss_g: 0.8589330315589905, loss_f: 0.10293494910001755\n",
      "loss_g: 0.7298372983932495, loss_f: -0.48913148045539856\n",
      "loss_g: 0.7701810598373413, loss_f: -0.427307665348053\n",
      "loss_g: 0.974276065826416, loss_f: -0.19256390631198883\n",
      "loss_g: 1.0110619068145752, loss_f: -0.1373508721590042\n",
      "loss_g: 1.1698319911956787, loss_f: 0.17665371298789978\n",
      "loss_g: 1.1203454732894897, loss_f: 0.26322028040885925\n",
      "loss_g: 1.0078374147415161, loss_f: 0.21020302176475525\n",
      "loss_g: 0.9123573899269104, loss_f: 0.18850177526474\n",
      "loss_g: 0.7749132513999939, loss_f: -0.37759947776794434\n",
      "loss_g: 0.7420716881752014, loss_f: -0.32093435525894165\n",
      "loss_g: 1.0157448053359985, loss_f: -0.11903637647628784\n",
      "loss_g: 0.9306675791740417, loss_f: 0.09963107109069824\n",
      "loss_g: 1.1240154504776, loss_f: 0.27328163385391235\n",
      "loss_g: 0.798865795135498, loss_f: -0.2610546350479126\n",
      "loss_g: 0.9318543076515198, loss_f: 0.13910473883152008\n",
      "loss_g: 0.9531792998313904, loss_f: 0.2099733054637909\n",
      "loss_g: 0.8039084076881409, loss_f: -0.33517348766326904\n",
      "loss_g: 0.7647939920425415, loss_f: -0.2982564866542816\n",
      "loss_g: 1.0003578662872314, loss_f: 0.0634002536535263\n",
      "loss_g: 0.9466201066970825, loss_f: 0.15982581675052643\n",
      "loss_g: 0.8737090229988098, loss_f: -0.3097561001777649\n",
      "loss_g: 0.7862406373023987, loss_f: -0.25407856702804565\n",
      "loss_g: 1.0430043935775757, loss_f: 0.06755858659744263\n",
      "loss_g: 0.948014497756958, loss_f: 0.20790742337703705\n",
      "loss_g: 0.9106289744377136, loss_f: -0.26513582468032837\n",
      "loss_g: 0.770982563495636, loss_f: -0.3541554808616638\n",
      "loss_g: 1.0666111707687378, loss_f: 0.12276806682348251\n",
      "loss_g: 0.9764741063117981, loss_f: 0.04318671301007271\n",
      "loss_g: 0.8910759091377258, loss_f: -0.20344124734401703\n",
      "loss_g: 0.8987432718276978, loss_f: -0.18702898919582367\n",
      "loss_g: 1.0310741662979126, loss_f: 0.23239712417125702\n",
      "loss_g: 1.0293543338775635, loss_f: 0.2555934488773346\n",
      "loss_g: 0.8258714079856873, loss_f: -0.38407203555107117\n",
      "loss_g: 0.8240236639976501, loss_f: -0.2984069585800171\n",
      "loss_g: 1.0446584224700928, loss_f: -0.06665365397930145\n",
      "loss_g: 1.014104962348938, loss_f: 0.062428414821624756\n",
      "loss_g: 1.1195082664489746, loss_f: 0.21459287405014038\n",
      "loss_g: 0.9313330054283142, loss_f: -0.14736837148666382\n",
      "loss_g: 0.9618122577667236, loss_f: 0.06553477793931961\n",
      "loss_g: 1.0468263626098633, loss_f: 0.27068397402763367\n",
      "loss_g: 0.9095419645309448, loss_f: -0.31348511576652527\n",
      "loss_g: 0.8483145236968994, loss_f: -0.31046926975250244\n",
      "loss_g: 1.0950000286102295, loss_f: 0.0215395949780941\n",
      "loss_g: 1.0502259731292725, loss_f: 0.011731418780982494\n",
      "loss_g: 0.9791110754013062, loss_f: -0.07367560267448425\n",
      "loss_g: 1.0247037410736084, loss_f: 0.004047174472361803\n",
      "loss_g: 1.0659663677215576, loss_f: 0.1669977307319641\n",
      "loss_g: 1.0079724788665771, loss_f: 0.02421843819320202\n",
      "loss_g: 0.8767552971839905, loss_f: -0.26911112666130066\n",
      "loss_g: 0.9907991886138916, loss_f: 0.025808604434132576\n",
      "loss_g: 1.0465987920761108, loss_f: 0.09802814573049545\n",
      "loss_g: 0.9606480002403259, loss_f: -0.09506656974554062\n",
      "loss_g: 0.9105462431907654, loss_f: -0.23597140610218048\n",
      "loss_g: 1.0665725469589233, loss_f: 0.12374264001846313\n",
      "loss_g: 1.0630881786346436, loss_f: 0.1400458663702011\n",
      "loss_g: 0.9451801180839539, loss_f: -0.02497217245399952\n",
      "loss_g: 0.9059399962425232, loss_f: -0.29287680983543396\n",
      "loss_g: 1.0125112533569336, loss_f: -0.04878521338105202\n",
      "loss_g: 1.0870633125305176, loss_f: 0.016932900995016098\n",
      "loss_g: 1.0542041063308716, loss_f: 0.07190548628568649\n",
      "loss_g: 1.0176817178726196, loss_f: 0.027253637090325356\n",
      "loss_g: 0.9922444820404053, loss_f: -0.19725827872753143\n",
      "loss_g: 1.0037496089935303, loss_f: -0.09686703234910965\n",
      "loss_g: 1.127654790878296, loss_f: 0.12577737867832184\n",
      "loss_g: 1.118530035018921, loss_f: 0.11397134512662888\n",
      "loss_g: 0.9870967864990234, loss_f: -0.13357731699943542\n",
      "loss_g: 1.0020169019699097, loss_f: -0.019878264516592026\n",
      "loss_g: 1.100191354751587, loss_f: 0.2227005809545517\n",
      "loss_g: 1.0554685592651367, loss_f: 0.011391235515475273\n",
      "loss_g: 0.922339141368866, loss_f: -0.2746666371822357\n",
      "loss_g: 1.018235445022583, loss_f: -0.02721608243882656\n",
      "loss_g: 1.1117005348205566, loss_f: -0.012946650385856628\n",
      "loss_g: 1.077268123626709, loss_f: -0.05431004986166954\n",
      "loss_g: 1.1236445903778076, loss_f: 0.12657932937145233\n",
      "loss_g: 1.1018788814544678, loss_f: 0.14455030858516693\n",
      "loss_g: 1.0426108837127686, loss_f: 0.01949610747396946\n",
      "loss_g: 0.9524978399276733, loss_f: -0.22759172320365906\n",
      "loss_g: 1.041841745376587, loss_f: 0.05208296701312065\n",
      "loss_g: 1.1041876077651978, loss_f: 0.12977705895900726\n",
      "loss_g: 1.0032105445861816, loss_f: -0.06115815043449402\n",
      "loss_g: 0.9544987082481384, loss_f: -0.22528983652591705\n",
      "loss_g: 1.0857510566711426, loss_f: 0.0841880738735199\n",
      "loss_g: 1.1105371713638306, loss_f: 0.10655245929956436\n",
      "loss_g: 1.006502389907837, loss_f: -0.055099815130233765\n",
      "loss_g: 0.9764738082885742, loss_f: -0.19799500703811646\n",
      "loss_g: 1.0874963998794556, loss_f: 0.08543365448713303\n",
      "loss_g: 1.1197632551193237, loss_f: 0.12468384951353073\n",
      "loss_g: 1.004813313484192, loss_f: -0.04377332702279091\n",
      "loss_g: 0.980661153793335, loss_f: -0.22570660710334778\n",
      "loss_g: 1.0753437280654907, loss_f: 0.05262373387813568\n",
      "loss_g: 1.1382255554199219, loss_f: 0.061978790909051895\n",
      "loss_g: 1.0148414373397827, loss_f: -0.03384216129779816\n",
      "loss_g: 1.049168586730957, loss_f: -0.10682334005832672\n",
      "loss_g: 1.0776888132095337, loss_f: 0.05847983807325363\n",
      "loss_g: 1.1458195447921753, loss_f: 0.16101866960525513\n",
      "loss_g: 1.011602759361267, loss_f: -0.040698446333408356\n",
      "loss_g: 0.975295901298523, loss_f: -0.277193158864975\n",
      "loss_g: 1.0792933702468872, loss_f: 0.09450624138116837\n",
      "loss_g: 1.1661652326583862, loss_f: -0.05289090424776077\n",
      "loss_g: 0.9929834008216858, loss_f: -0.008214634843170643\n",
      "loss_g: 1.2012232542037964, loss_f: 0.21765992045402527\n",
      "epoch : 120\r"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=500, shuffle=True)\n",
    "\n",
    "loss_f = list()\n",
    "loss_g = list()\n",
    "\n",
    "for epoch in range(1, 201) :\n",
    "    print('epoch :', epoch, end=('\\r'))\n",
    "    mean_loss_f, mean_loss_g = train_makkuva_epoch(l_ICNNf[epoch%2], l_ICNNg[epoch%2], l_ICNNf[1 - epoch%2], l_ICNNg[1 - epoch%2], dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, lr=0.001, train_freq_g=10, train_freq_f=1, gaussian_transport=False, regularize_f = False, regularize_g = True)\n",
    "    #mean_loss_f, mean_loss_g = train_makkuva_epoch(ICNNf, ICNNg, None, None, dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2, lr=0.0001, train_freq_g=10, train_freq_f=1, gaussian_transport=False)\n",
    "\n",
    "    loss_f.append(mean_loss_f)\n",
    "    loss_g.append(mean_loss_g)\n",
    "\n",
    "    filename_pth_f = filepath_pth_f + str(epoch) + '.pth'\n",
    "    filename_pth_g = filepath_pth_g + str(epoch) + '.pth'\n",
    "    torch.save(l_ICNNf[epoch%2].state_dict(), filename_pth_f)\n",
    "    torch.save(l_ICNNg[epoch%2].state_dict(), filename_pth_g)\n",
    "\n",
    "    filename_plt_f = filepath_plt_f + str(epoch) + '.png'\n",
    "    filename_plt_g = filepath_plt_g + str(epoch) + '.png'\n",
    "    plot_transport(dataset, test, l_ICNNf[epoch%2], l_ICNNg[epoch%2], filename_f = filename_plt_f, filename_g = filename_plt_g, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in loss_f:\n",
    "    print(ele)\n",
    "\n",
    "print('stop')\n",
    "\n",
    "for ele in loss_g:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICNNf.load_state_dict(torch.load(filepath_pth_f + '0.pth'))\n",
    "ICNNg.load_state_dict(torch.load(filepath_pth_g + '0.pth'))\n",
    "\n",
    "filename_plt_f = 'trained_models/training9/plots/model_f_test'\n",
    "filename_plt_g = 'trained_models/training9/plots/model_g_test'\n",
    "\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, filename_f = filename_plt_f, filename_g = filename_plt_g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
