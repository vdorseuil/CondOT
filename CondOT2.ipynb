{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icnnet import ICNNet\n",
    "from mydataset import MyDataset, get_gaussian_dataset, get_gaussian_transport_dataset\n",
    "from toy_data_dataloader_gaussian import generate_gaussian_dataset, get_dataset, generate_dataset\n",
    "from train_picnn import PICNNtrain\n",
    "from train_wasserstein import train_wasserstein\n",
    "from train_makkuva import train_makkuva, train_makkuva_epoch\n",
    "from visualization import plot_transport\n",
    "from gaussian_transport import get_gaussian_transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Generate dataset__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = get_dataset(d=2, r=100, N=500) #valou\n",
    "#dataset = generate_gaussian_dataset(d=2, r=400, N=10000) #thomas\n",
    "dataset = generate_dataset(d=2, r=500, N=50)\n",
    "gaussian_dataset = get_gaussian_dataset(dataset)\n",
    "gaussian_transport_dataset = get_gaussian_transport_dataset(gaussian_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(batch):\n",
    "    means = torch.mean(batch, dim=1)\n",
    "    average_mean = torch.mean(means, dim=0)\n",
    "    return(average_mean)\n",
    "\n",
    "def get_covariance(batch):\n",
    "    n = batch.size(1) - 1\n",
    "    mean = torch.mean(batch, dim=1, keepdim=True)\n",
    "    batch = batch - mean  # Centering the data\n",
    "    cov = torch.matmul(batch.transpose(1, 2), batch) / n\n",
    "    return(torch.mean(cov, dim=0))\n",
    "\n",
    "mean1 = get_mean(dataset.X)\n",
    "cov1 = get_covariance(dataset.X)\n",
    "mean2 = get_mean(dataset.Y)\n",
    "cov2 = get_covariance(dataset.Y)\n",
    "\n",
    "def init_z_f(x):\n",
    "    return (1/2) * torch.norm(x, dim=-1, keepdim=True)**2\n",
    "    return(get_gaussian_transport(u=x, cov1 = cov1, cov2 = cov2, m1=mean1, m2=mean2))\n",
    "\n",
    "def init_z_g(x) :\n",
    "    return (1/2) * torch.norm(x, dim=-1, keepdim=True)**2\n",
    "    return(get_gaussian_transport(u=x, cov1 = cov2, cov2 = cov1, m1=mean2, m2=mean1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Initialization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __PICNN training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "layer_sizes = [input_size,64, 64, 64, 1]\n",
    "n_layers = len(layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# def get_embedding(C, c):\n",
    "#     scalar_product = torch.matmul(c.float(), C.t().float())\n",
    "#     embedding = F.softmax(scalar_product, dim=1)\n",
    "#     return(embedding)\n",
    "\n",
    "context_layer_sizes = [12] + [64] * (n_layers-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init_f = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "model_init_g = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training f\n",
      "training g\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 0\n",
    "lr = 0.001\n",
    "\n",
    "print('training f')\n",
    "gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "PICNNtrain(model_init_f, gaussian_transport_dataloader, init_z_f, lr=lr, epochs=n_epoch)\n",
    "PICNNtrain(model_init_f, gaussian_transport_dataloader, init_z_f, lr=lr, epochs=n_epoch)\n",
    "#PICNNtrain(model_init_f, gaussian_transport_dataloader, lr=0.0001, epochs=1, init_z = lambda x: x)\n",
    "\n",
    "print('training g')\n",
    "reversed_gaussian_dataset = MyDataset(gaussian_dataset.Y, gaussian_dataset.C, gaussian_dataset.X)\n",
    "gaussian_transport_dataset_reversed = get_gaussian_transport_dataset(reversed_gaussian_dataset)\n",
    "gaussian_transport_dataloader_reversed = DataLoader(gaussian_transport_dataset_reversed, batch_size=250, shuffle=True)\n",
    "#PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, lr=0.0001, epochs=25, init_z = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2)\n",
    "PICNNtrain(model_init_g, gaussian_transport_dataloader_reversed, init_z_g, lr=lr, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_init_f = model_init_f.state_dict()\n",
    "state_dict_init_g = model_init_g.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dorseuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('training f')\n",
    "# gaussian_transport_dataloader = DataLoader(gaussian_transport_dataset, batch_size=250, shuffle=True)\n",
    "# train_wasserstein(model_init_f, gaussian_transport_dataloader, lr=0.1, epochs=10, init_z = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y, C = gaussian_dataset.X, gaussian_dataset.Y, gaussian_dataset.C\n",
    "# #Calcul de la dérivée du PICNN\n",
    "\n",
    "# for test in range(20):\n",
    "#     x_i = X[test, :, :]\n",
    "#     y_i = Y[test, :, :]\n",
    "#     c_i = C[test, :, :]\n",
    "\n",
    "#     locs = c_i[:,0]\n",
    "#     #print(locs)\n",
    "\n",
    "#     scales = c_i[:,1]\n",
    "#     #print(scales)  \n",
    "\n",
    "\n",
    "#     y_i.requires_grad_(True)\n",
    "#     x_i.requires_grad_(True)\n",
    "#     #c_i.requires_grad_(True)    \n",
    "\n",
    "#     output_model_f = model_init_f(x_i, c_i)\n",
    "#     grad_model_f = torch.autograd.grad(outputs=output_model_f, inputs=x_i, grad_outputs=torch.ones_like(output_model_f), create_graph=True)[0].detach().numpy()\n",
    "\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True)\n",
    "#     plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True)\n",
    "#     plt.hist(grad_model_f[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     output_model_g = model_init_g(y_i, c_i)\n",
    "#     grad_model_g = torch.autograd.grad(outputs=output_model_g, inputs=y_i, grad_outputs=torch.ones_like(output_model_g), create_graph=True)[0].detach().numpy()\n",
    "#     plt.hist(X[test, :, 0],  bins=15, label = 'X', density = True, color = 'red')\n",
    "#     #plt.hist(Y[test, :, 0],  bins=15, label = 'Y', density = True, color = 'blue')\n",
    "#     plt.hist(grad_model_g[:, 0],  bins=15, label = 'grad_model', density = True, alpha = 0.5)\n",
    "#     # plt.hist(X_pred,  bins=15, label = 'X_pred', density = True, alpha = 0.5)\n",
    "#     interval_x = np.linspace(-3, 3, 300)\n",
    "#     interval_y = np.linspace(-3*scales[0] + locs[0], 3*scales[0] + locs[0], 300)\n",
    "\n",
    "#     plt.plot(interval_x, stats.norm.pdf(interval_x, loc=0, scale=1), label = 'X_distrib', color = 'blue')\n",
    "#     #plt.plot(interval_y, stats.norm.pdf(interval_y, loc = locs[0], scale = scales[0]), label = 'Y_distrib', color = 'orange')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Makkuva__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict_init_f = torch.load('trained_models/training22/models/model_f_0.pth')\n",
    "# state_dict_init_g = torch.load('trained_models/training22/models/model_g_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ICNNf = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "ICNNg = ICNNet(layer_sizes = layer_sizes, context_layer_sizes=context_layer_sizes, init_bunne = 'TR')\n",
    "\n",
    "# Load the state dictionary into ICNNf and ICNNg\n",
    "ICNNf.load_state_dict(state_dict_init_f)\n",
    "ICNNg.load_state_dict(state_dict_init_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 2000\n",
    "test = 14\n",
    "\n",
    "filepath_pth_f = 'trained_models/training22/models/model_f_'\n",
    "filepath_pth_g = 'trained_models/training22/models/model_g_'\n",
    "\n",
    "filepath_plt_f = 'trained_models/training22/plots/model_f_'\n",
    "filepath_plt_g = 'trained_models/training22/plots/model_g_'\n",
    "\n",
    "import os\n",
    "os.makedirs(filepath_pth_f, exist_ok=True)\n",
    "os.makedirs(filepath_pth_g, exist_ok=True)\n",
    "\n",
    "os.makedirs(filepath_plt_f, exist_ok=True)\n",
    "os.makedirs(filepath_plt_g, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pth_f = filepath_pth_f + str(0) + '.pth'\n",
    "filename_pth_g = filepath_pth_g + str(0) + '.pth'\n",
    "torch.save(ICNNf.state_dict(), filename_pth_f)\n",
    "torch.save(ICNNg.state_dict(), filename_pth_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename_plt_f = filepath_plt_f + str(0) + '.png'\n",
    "filename_plt_g = filepath_plt_g + str(0) + '.png'\n",
    "plot_transport(dataset, test, ICNNf, ICNNg, init_z_f = init_z_f, init_z_g = init_z_g, filename_f = filename_plt_f, filename_g = filename_plt_g, n_points=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "200.1241455078125\n",
      "318.9075927734375\n",
      "151.32456970214844\n",
      "86.82632446289062\n",
      "7.3665361404418945\n",
      "loss_g: 7.3665361404418945, loss_f: -6.9537858963012695\n",
      "epoch : 2\n",
      "10.699885368347168\n",
      "7.648858070373535\n",
      "6.402629375457764\n",
      "5.745697975158691\n",
      "5.356654167175293\n",
      "loss_g: 5.356654167175293, loss_f: -2.4423844814300537\n",
      "epoch : 3\n",
      "5.5365166664123535\n",
      "5.083315372467041\n",
      "158.7531280517578\n",
      "30.1740779876709\n",
      "5.434520244598389\n",
      "loss_g: 5.434520244598389, loss_f: 0.9087734222412109\n",
      "epoch : 4\n",
      "5.348333835601807\n",
      "5.301485061645508\n",
      "5.263774871826172\n",
      "5.230041980743408\n",
      "5.19821310043335\n",
      "loss_g: 5.19821310043335, loss_f: 0.985832691192627\n",
      "epoch : 5\n",
      "5.160715579986572\n",
      "5.129481315612793\n",
      "5.098044395446777\n",
      "5.066792011260986\n",
      "5.035608291625977\n",
      "loss_g: 5.035608291625977, loss_f: 0.9391283392906189\n",
      "epoch : 6\n",
      "4.998515605926514\n",
      "4.968017578125\n",
      "4.93822717666626\n",
      "4.908973217010498\n",
      "4.880425930023193\n",
      "loss_g: 4.880425930023193, loss_f: 0.85956871509552\n",
      "epoch : 7\n",
      "4.845444202423096\n",
      "4.817580699920654\n",
      "4.7904815673828125\n",
      "4.764032363891602\n",
      "4.738250255584717\n",
      "loss_g: 4.738250255584717, loss_f: 0.7627017498016357\n",
      "epoch : 8\n",
      "4.705856800079346\n",
      "4.680822372436523\n",
      "4.656555652618408\n",
      "4.63309907913208\n",
      "4.6103949546813965\n",
      "loss_g: 4.6103949546813965, loss_f: 0.6554206609725952\n",
      "epoch : 9\n",
      "4.5818400382995605\n",
      "4.560323238372803\n",
      "4.5396881103515625\n",
      "4.5201592445373535\n",
      "4.5022783279418945\n",
      "loss_g: 4.5022783279418945, loss_f: 0.5420802235603333\n",
      "epoch : 10\n",
      "4.480319499969482\n",
      "4.464061260223389\n",
      "4.44907283782959\n",
      "4.4355292320251465\n",
      "4.42317533493042\n",
      "loss_g: 4.42317533493042, loss_f: 0.4281238615512848\n",
      "epoch : 11\n",
      "4.4099650382995605\n",
      "4.3992085456848145\n",
      "4.389114856719971\n",
      "4.379708766937256\n",
      "4.370909214019775\n",
      "loss_g: 4.370909214019775, loss_f: 0.31635943055152893\n",
      "epoch : 12\n",
      "4.364739894866943\n",
      "4.357109546661377\n",
      "4.350047588348389\n",
      "4.343505859375\n",
      "4.337441921234131\n",
      "loss_g: 4.337441921234131, loss_f: 0.2097770869731903\n",
      "epoch : 13\n",
      "4.338618278503418\n",
      "4.333634853363037\n",
      "4.3290486335754395\n",
      "4.3248467445373535\n",
      "4.320989608764648\n",
      "loss_g: 4.320989608764648, loss_f: 0.11472517997026443\n",
      "epoch : 14\n",
      "4.329213619232178\n",
      "4.326272010803223\n",
      "4.323576927185059\n",
      "4.321150302886963\n",
      "4.318935871124268\n",
      "loss_g: 4.318935871124268, loss_f: 0.03457707166671753\n",
      "epoch : 15\n",
      "4.333395957946777\n",
      "4.3318610191345215\n",
      "4.330480098724365\n",
      "4.329226493835449\n",
      "4.32810115814209\n",
      "loss_g: 4.32810115814209, loss_f: -0.02898656576871872\n",
      "epoch : 16\n",
      "4.347506046295166\n",
      "4.346842288970947\n",
      "4.346250057220459\n",
      "4.345726013183594\n",
      "4.345264434814453\n",
      "loss_g: 4.345264434814453, loss_f: -0.07579106837511063\n",
      "epoch : 17\n",
      "4.368134021759033\n",
      "4.367925643920898\n",
      "4.367743015289307\n",
      "4.367588996887207\n",
      "4.367446422576904\n",
      "loss_g: 4.367446422576904, loss_f: -0.10671073198318481\n",
      "epoch : 18\n",
      "4.3923563957214355\n",
      "4.392315864562988\n",
      "4.392288684844971\n",
      "4.392265796661377\n",
      "4.392244338989258\n",
      "loss_g: 4.392244338989258, loss_f: -0.12365163862705231\n",
      "epoch : 19\n",
      "4.417975425720215\n",
      "4.417943954467773\n",
      "4.417923450469971\n",
      "4.41790771484375\n",
      "4.4178972244262695\n",
      "loss_g: 4.4178972244262695, loss_f: -0.12907403707504272\n",
      "epoch : 20\n",
      "4.443512916564941\n",
      "4.443419933319092\n",
      "4.44334602355957\n",
      "4.4432830810546875\n",
      "4.443227291107178\n",
      "loss_g: 4.443227291107178, loss_f: -0.12552854418754578\n",
      "epoch : 21\n",
      "4.468013286590576\n",
      "4.467843532562256\n",
      "4.467700004577637\n",
      "4.46757173538208\n",
      "4.46746301651001\n",
      "loss_g: 4.46746301651001, loss_f: -0.11527883261442184\n",
      "epoch : 22\n",
      "4.490889549255371\n",
      "4.490655422210693\n",
      "4.490450859069824\n",
      "4.490276336669922\n",
      "4.4901251792907715\n",
      "loss_g: 4.4901251792907715, loss_f: -0.10057242214679718\n",
      "epoch : 23\n",
      "4.5121026039123535\n",
      "4.511834621429443\n",
      "4.511597156524658\n",
      "4.511387348175049\n",
      "4.511207580566406\n",
      "loss_g: 4.511207580566406, loss_f: -0.08301720768213272\n",
      "epoch : 24\n",
      "4.531502723693848\n",
      "4.531211853027344\n",
      "4.530956745147705\n",
      "4.530735015869141\n",
      "4.530536651611328\n",
      "loss_g: 4.530536651611328, loss_f: -0.06402072310447693\n",
      "epoch : 25\n",
      "4.549271583557129\n",
      "4.548985481262207\n",
      "4.548730373382568\n",
      "4.548504829406738\n",
      "4.548305988311768\n",
      "loss_g: 4.548305988311768, loss_f: -0.044536396861076355\n",
      "epoch : 26\n",
      "4.56540584564209\n",
      "4.565134048461914\n",
      "4.564896583557129\n",
      "4.564680576324463\n",
      "4.564486503601074\n",
      "loss_g: 4.564486503601074, loss_f: -0.025432635098695755\n",
      "epoch : 27\n",
      "4.580193519592285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m) :\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch :\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch)\n\u001b[0;32m---> 11\u001b[0m     mean_loss_f, mean_loss_g, prev_param_f, prev_param_g \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_makkuva_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mICNNf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mICNNf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mICNNg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mICNNg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_param_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_param_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_param_g\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_param_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_z_f\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minit_z_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_z_g\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minit_z_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_freq_g\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_freq_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularize_f\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularize_g\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_proximal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#mean_loss_f, mean_loss_g = train_makkuva_epoch(ICNNf, ICNNg, None, None, dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2, lr=0.0001, train_freq_g=10, train_freq_f=1, gaussian_transport=False)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     loss_f\u001b[38;5;241m.\u001b[39mappend(mean_loss_f)\n",
      "File \u001b[0;32m~/Desktop/CondOT/CondOT-1/train_makkuva.py:114\u001b[0m, in \u001b[0;36mtrain_makkuva_epoch\u001b[0;34m(ICNNf, ICNNg, prev_param_f, prev_param_g, dataloader, init_z_f, init_z_g, lr, train_freq_g, train_freq_f, regularize_g, regularize_f, lambda_proximal)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m#loss_g = ICNNf(grad_g, c, init_z_f) - torch.sum(y_trans * grad_g, dim=-1, keepdim=True)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#loss_g = (ICNNf(grad_g, c, z_0_g)+old_ICNNf(grad_g, c, z_0_g))/2 - torch.sum(y * grad_g, dim=-1, keepdim=True)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m loss_g \u001b[38;5;241m=\u001b[39m loss_g\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m--> 114\u001b[0m \u001b[43mloss_g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    116\u001b[0m simulated_parameters \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/Desktop/CondOT/CondOT-1/python_env_condot/lib64/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CondOT/CondOT-1/python_env_condot/lib64/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=501, shuffle=True)\n",
    "\n",
    "loss_f = list()\n",
    "loss_g = list()\n",
    "\n",
    "prev_param_f = [param.clone().detach() for param in ICNNf.parameters()]\n",
    "prev_param_g = [param.clone().detach() for param in ICNNg.parameters()]\n",
    "\n",
    "for epoch in range(1, 101) :\n",
    "    print('epoch :', epoch)\n",
    "    mean_loss_f, mean_loss_g, prev_param_f, prev_param_g = train_makkuva_epoch(ICNNf=ICNNf, ICNNg=ICNNg, prev_param_f=prev_param_f, prev_param_g=prev_param_g, dataloader = dataloader, init_z_f = init_z_f, init_z_g = init_z_g, lr=0.001, train_freq_g=5, train_freq_f=1, regularize_f = True, regularize_g = True, lambda_proximal = 0.0001)\n",
    "    #mean_loss_f, mean_loss_g = train_makkuva_epoch(ICNNf, ICNNg, None, None, dataloader, init_z_f = lambda x: (1/2) * torch.norm(x, dim=-1, keepdim=True)**2, init_z_g = lambda x: (1/2) * torch.norm(-x, dim=-1, keepdim=True)**2, lr=0.0001, train_freq_g=10, train_freq_f=1, gaussian_transport=False)\n",
    "\n",
    "    loss_f.append(mean_loss_f)\n",
    "    loss_g.append(mean_loss_g)\n",
    "\n",
    "    filename_pth_f = filepath_pth_f + str(epoch) + '.pth'\n",
    "    filename_pth_g = filepath_pth_g + str(epoch) + '.pth'\n",
    "    torch.save(ICNNf.state_dict(), filename_pth_f)\n",
    "    torch.save(ICNNg.state_dict(), filename_pth_g)\n",
    "\n",
    "    filename_plt_f = filepath_plt_f + str(epoch) + '.png'\n",
    "    filename_plt_g = filepath_plt_g + str(epoch) + '.png'\n",
    "    plot_transport(dataset, test, ICNNf, ICNNg, init_z_f=init_z_f, init_z_g=init_z_g, filename_f = filename_plt_f, filename_g = filename_plt_g, n_points=n_points)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_test = [12, 13, 14, 15, 16, 17, 18, 19]\n",
    "for epoch in [19]:\n",
    "    print('epoch :', epoch)\n",
    "\n",
    "    filename_pth_f = filepath_pth_f + str(epoch) + '.pth'\n",
    "    filename_pth_g = filepath_pth_g + str(epoch) + '.pth'\n",
    "\n",
    "    state_dict_init_f = torch.load(filename_pth_f)\n",
    "    state_dict_init_g = torch.load(filename_pth_f)\n",
    "\n",
    "    ICNNf.load_state_dict(state_dict_init_f)\n",
    "    ICNNg.load_state_dict(state_dict_init_g)\n",
    "\n",
    "    for test in l_test :\n",
    "        filename_plt_f = filepath_plt_f + 'test_' + str(test) + '_'  +str(epoch) + '.png'\n",
    "        filename_plt_g = filepath_plt_g + 'test_' + str(test) + '_' + str(epoch) + '.png'\n",
    "        plot_transport(dataset, test, ICNNf, ICNNg, init_z_f=init_z_f, init_z_g=init_z_g, filename_f = filename_plt_f, filename_g = filename_plt_g, n_points=n_points)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename_plt_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in loss_f:\n",
    "    print(ele)\n",
    "\n",
    "# print('stop')\n",
    "\n",
    "# for ele in loss_g:\n",
    "#     print(ele)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
